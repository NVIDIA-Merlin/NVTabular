{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Scaling Criteo: ETL with NVTabular\n",
    "\n",
    "## Overview\n",
    "\n",
    "NVTabular is a feature engineering and preprocessing library for tabular data designed to quickly and easily manipulate terabyte scale datasets used to train deep learning based recommender systems. It provides a high level abstraction to simplify code and accelerates computation on the GPU using the RAPIDS cuDF library.<br><br>\n",
    "\n",
    "**In this notebook, we will show how to scale NVTabular to multi-GPUs and multiple nodes.** Prerequisite is to be familiar with NVTabular and its API. You can read more NVTabular and its API in our [Getting Started with Movielens notebooks](https://github.com/NVIDIA/NVTabular/tree/main/examples/getting-started-movielens).<br><br>\n",
    "\n",
    "The full [Criteo 1TB Click Logs dataset](https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/) contains ~1.3 TB of uncompressed click logs containing over four billion samples spanning 24 days. In our benchmarks, we are able to preprocess and engineer features in **13.8min with 1x NVIDIA A100 GPU and 1.9min with 8x NVIDIA A100 GPUs**. This is a **speed-up of 100x-10000x** in comparison to different CPU versions, You can read more in our [blog](https://developer.nvidia.com/blog/announcing-the-nvtabular-open-beta-with-multi-gpu-support-and-new-data-loaders/).\n",
    "\n",
    "Our pipeline will be representative with most common preprocessing transformation for deep learning recommender models.\n",
    "\n",
    "* Categorical input features are `Categorified` to be continuous integers (0, ..., |C|) for the embedding layers\n",
    "* Missing values of continuous input features are filled with 0. Afterwards the continuous features are clipped and normalized.\n",
    "\n",
    "### Learning objectives\n",
    "In this notebook, we learn how to to scale ETLs with NVTabular\n",
    "\n",
    "- Learn to use larger than GPU/host memory datasets\n",
    "- Use multi-GPU or multi node for ETL\n",
    "- Apply common deep learning ETL workflow\n",
    "\n",
    "### Multi-GPU and multi-node scaling\n",
    "\n",
    "NVTabular is built on top off [RAPIDS.AI cuDF](https://github.com/rapidsai/cudf/), [dask_cudf](https://docs.rapids.ai/api/cudf/stable/dask-cudf.html) and [dask](https://dask.org/).<br><br>\n",
    "**Dask** is a task-based library for parallel scheduling and execution. Although it is certainly possible to use the task-scheduling machinery directly to implement customized parallel workflows (we do it in NVTabular), most users only interact with Dask through a Dask Collection API. The most popular \"collection\" API's include:\n",
    "\n",
    "* Dask DataFrame: Dask-based version of the Pandas DataFrame/Series API. Note that dask_cudf is just a wrapper around this collection module (dask.dataframe).\n",
    "* Dask Array: Dask-based version of the NumPy array API\n",
    "* Dask Bag: Similar to a Dask-based version of PyToolz or a Pythonic version of PySpark RDD\n",
    "\n",
    "For example, Dask DataFrame provides a convenient API for decomposing large Pandas (or cuDF) DataFrame/Series objects into a collection of DataFrame partitions.\n",
    "\n",
    "<img src=\"./imgs/dask-dataframe.svg\" width=\"20%\">\n",
    "\n",
    "We use **dask_cudf** to process large datasets as a collection of cuDF dataframes instead of Pandas. CuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data.\n",
    "<br><br>\n",
    "**Dask enables easily to schedule tasks for multiple workers: multi-GPU or multi-node. We just need to initialize a Dask cluster (`LocalCUDACluster`) and NVTabular will use the cluster to execture the workflow.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL with NVTabular\n",
    "Here we'll show how to use NVTabular first as a preprocessing library to prepare the [Criteo 1TB Click Logs dataset](https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/) dataset. The following notebooks can use the output to train a deep learning model.\n",
    "\n",
    "### Data Prep\n",
    "The previous notebook [01-Download-Convert](./01-Download-Convert.ipynb) converted the tsv data published by Criteo into the parquet format that our accelerated readers prefer. Accelerating these pipelines on new hardware like GPUs may require us to make new choices about the representations we use to store that data, and parquet represents a strong alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "# External Dependencies\n",
    "import numpy as np\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "# NVTabular\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import Categorify, Clip, FillMissing, Normalize, AddMetadata\n",
    "from nvtabular.utils import _pynvml_mem_size, device_mem_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our data is ready, we'll define some high level parameters to describe where our data is and what it \"looks like\" at a high level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# define some information about where to get our data\n",
    "BASE_DIR = os.environ.get(\"BASE_DIR\", \"/raid/data/criteo\")\n",
    "INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", BASE_DIR + \"/converted/criteo\")\n",
    "dask_workdir = os.path.join(BASE_DIR, \"test_dask/workdir\")\n",
    "OUTPUT_DATA_DIR = os.environ.get(\"OUTPUT_DATA_DIR\", BASE_DIR + \"/test_dask/output\")\n",
    "stats_path = os.path.join(BASE_DIR, \"test_dask/stats\")\n",
    "\n",
    "# Make sure we have a clean worker space for Dask\n",
    "if os.path.isdir(dask_workdir):\n",
    "    shutil.rmtree(dask_workdir)\n",
    "os.makedirs(dask_workdir)\n",
    "\n",
    "# Make sure we have a clean stats space for Dask\n",
    "if os.path.isdir(stats_path):\n",
    "    shutil.rmtree(stats_path)\n",
    "os.mkdir(stats_path)\n",
    "\n",
    "# Make sure we have a clean output path\n",
    "if os.path.isdir(OUTPUT_DATA_DIR):\n",
    "    shutil.rmtree(OUTPUT_DATA_DIR)\n",
    "os.mkdir(OUTPUT_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the last day as validation dataset and the remaining days as training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"day_{}.parquet\"\n",
    "num_days = len(\n",
    "    [i for i in os.listdir(INPUT_DATA_DIR) if re.match(fname.format(\"[0-9]{1,2}\"), i) is not None]\n",
    ")\n",
    "train_paths = [os.path.join(INPUT_DATA_DIR, fname.format(day)) for day in range(num_days - 1)]\n",
    "valid_paths = [\n",
    "    os.path.join(INPUT_DATA_DIR, fname.format(day)) for day in range(num_days - 1, num_days)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/raid/benny/04_criteo/converted/criteo/day_0.parquet']\n",
      "['/raid/benny/04_criteo/converted/criteo/day_1.parquet']\n"
     ]
    }
   ],
   "source": [
    "print(train_paths)\n",
    "print(valid_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy a Distributed-Dask Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we configure and deploy a Dask Cluster. Please, [read this document](https://github.com/NVIDIA/NVTabular/blob/d419a4da29cf372f1547edc536729b0733560a44/bench/examples/MultiGPUBench.md) to know how to set the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.8/site-packages/distributed-2021.7.1-py3.8.egg/distributed/node.py:160: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 34057 instead\n",
      "  warnings.warn(\n",
      "distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div style=\"\n",
       "                    width: 24px;\n",
       "                    height: 24px;\n",
       "                    background-color: #e1e1e1;\n",
       "                    border: 3px solid #9D9D9D;\n",
       "                    border-radius: 5px;\n",
       "                    position: absolute;\"> </div>\n",
       "                <div style=\"margin-left: 48px;\">\n",
       "                    <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "                    <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-152af06f-5cea-11ec-8b24-54ab3a8c9e18</p>\n",
       "                    <table style=\"width: 100%; text-align: left;\">\n",
       "                    \n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "                    <td style=\"text-align: left;\"><strong>Cluster type:</strong> LocalCUDACluster</td>\n",
       "                </tr>\n",
       "                \n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard: </strong>\n",
       "                        <a href=\"http://127.0.0.1:34057/status\">http://127.0.0.1:34057/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\"></td>\n",
       "                </tr>\n",
       "                \n",
       "                    </table>\n",
       "                    \n",
       "                <details>\n",
       "                <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "                \n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "                <div style=\"\n",
       "                    width: 24px;\n",
       "                    height: 24px;\n",
       "                    background-color: #e1e1e1;\n",
       "                    border: 3px solid #9D9D9D;\n",
       "                    border-radius: 5px;\n",
       "                    position: absolute;\"> </div>\n",
       "                <div style=\"margin-left: 48px;\">\n",
       "                    <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">LocalCUDACluster</h3>\n",
       "                    <p style=\"color: #9D9D9D; margin-bottom: 0px;\">ebb5ffa8</p>\n",
       "                    <table style=\"width: 100%; text-align: left;\">\n",
       "                    \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\"><strong>Status:</strong> running</td>\n",
       "                <td style=\"text-align: left;\"><strong>Using processes:</strong> True</td>\n",
       "            </tr>\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:34057/status\">http://127.0.0.1:34057/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"><strong>Workers:</strong> 1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong>\n",
       "                    1\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong>\n",
       "                    0.98 TiB\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "                    </table>\n",
       "                    <details>\n",
       "                    <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Scheduler Info</h3></summary>\n",
       "                    \n",
       "        <div style=\"\">\n",
       "            \n",
       "            <div>\n",
       "                <div style=\"\n",
       "                    width: 24px;\n",
       "                    height: 24px;\n",
       "                    background-color: #FFF7E5;\n",
       "                    border: 3px solid #FF6132;\n",
       "                    border-radius: 5px;\n",
       "                    position: absolute;\"> </div>\n",
       "                <div style=\"margin-left: 48px;\">\n",
       "                    <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "                    <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-05c73c3d-45c7-4dc9-9884-d068296172c9</p>\n",
       "                    <table style=\"width: 100%; text-align: left;\">\n",
       "                        <tr>\n",
       "                            <td style=\"text-align: left;\"><strong>Comm:</strong> tcp://127.0.0.1:35397</td>\n",
       "                            <td style=\"text-align: left;\"><strong>Workers:</strong> 1</td>\n",
       "                        </tr>\n",
       "                        <tr>\n",
       "                            <td style=\"text-align: left;\">\n",
       "                                <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:34057/status\">http://127.0.0.1:34057/status</a>\n",
       "                            </td>\n",
       "                            <td style=\"text-align: left;\">\n",
       "                                <strong>Total threads:</strong>\n",
       "                                1\n",
       "                            </td>\n",
       "                        </tr>\n",
       "                        <tr>\n",
       "                            <td style=\"text-align: left;\">\n",
       "                                <strong>Started:</strong>\n",
       "                                Just now\n",
       "                            </td>\n",
       "                            <td style=\"text-align: left;\">\n",
       "                                <strong>Total memory:</strong>\n",
       "                                0.98 TiB\n",
       "                            </td>\n",
       "                        </tr>\n",
       "                    </table>\n",
       "                </div>\n",
       "            </div>\n",
       "        \n",
       "            <details style=\"margin-left: 48px;\">\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Workers</h3></summary>\n",
       "            \n",
       "            <div style=\"margin-bottom: 20px;\">\n",
       "                <div style=\"width: 24px;\n",
       "                            height: 24px;\n",
       "                            background-color: #DBF5FF;\n",
       "                            border: 3px solid #4CC9FF;\n",
       "                            border-radius: 5px;\n",
       "                            position: absolute;\"> </div>\n",
       "                <div style=\"margin-left: 48px;\">\n",
       "                <details>\n",
       "                    <summary>\n",
       "                        <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 1</h4>\n",
       "                    </summary>\n",
       "                    <table style=\"width: 100%; text-align: left;\">\n",
       "                        <tr>\n",
       "                            <td style=\"text-align: left;\"><strong>Comm: </strong> tcp://127.0.0.1:43039</td>\n",
       "                            <td style=\"text-align: left;\"><strong>Total threads: </strong> 1</td>\n",
       "                        </tr>\n",
       "                        <tr>\n",
       "                            <td style=\"text-align: left;\">\n",
       "                                <strong>Dashboard: </strong>\n",
       "                                <a href=\"http://127.0.0.1:32795/status\">http://127.0.0.1:32795/status</a>\n",
       "                            </td>\n",
       "                            <td style=\"text-align: left;\">\n",
       "                                <strong>Memory: </strong>\n",
       "                                0.98 TiB\n",
       "                            </td>\n",
       "                        </tr>\n",
       "                        <tr>\n",
       "                            <td style=\"text-align: left;\"><strong>Nanny: </strong> tcp://127.0.0.1:36051</td>\n",
       "                            <td style=\"text-align: left;\"></td>\n",
       "                        </tr>\n",
       "                        <tr>\n",
       "                            <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                                <strong>Local directory: </strong>\n",
       "                                /raid/benny/04_criteo/test_dask/workdir/dask-worker-space/worker-dvdj11v9\n",
       "                            </td>\n",
       "                        </tr>\n",
       "                        \n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>GPU: </strong>Tesla V100-SXM2-32GB\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>GPU memory: </strong>\n",
       "                        31.75 GiB\n",
       "                    </td>\n",
       "                </tr>\n",
       "                \n",
       "                        \n",
       "                    </table>\n",
       "                </details>\n",
       "                </div>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        </div>\n",
       "        \n",
       "                    </details>\n",
       "                </div>\n",
       "            </div>\n",
       "        \n",
       "                </details>\n",
       "                \n",
       "                </div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:35397' processes=1 threads=1, memory=0.98 TiB>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dask dashboard\n",
    "dashboard_port = \"8787\"\n",
    "\n",
    "# Deploy a Single-Machine Multi-GPU Cluster\n",
    "protocol = \"tcp\"  # \"tcp\" or \"ucx\"\n",
    "NUM_GPUS = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "visible_devices = \",\".join([str(n) for n in NUM_GPUS])  # Delect devices to place workers\n",
    "device_limit_frac = 0.7  # Spill GPU-Worker memory to host at this limit.\n",
    "device_pool_frac = 0.8\n",
    "part_mem_frac = 0.15\n",
    "\n",
    "# Use total device size to calculate args.device_limit_frac\n",
    "device_size = device_mem_size(kind=\"total\")\n",
    "device_limit = int(device_limit_frac * device_size)\n",
    "device_pool_size = int(device_pool_frac * device_size)\n",
    "part_size = int(part_mem_frac * device_size)\n",
    "\n",
    "# Check if any device memory is already occupied\n",
    "for dev in visible_devices.split(\",\"):\n",
    "    fmem = _pynvml_mem_size(kind=\"free\", index=int(dev))\n",
    "    used = (device_size - fmem) / 1e9\n",
    "    if used > 1.0:\n",
    "        warnings.warn(f\"BEWARE - {used} GB is already occupied on device {int(dev)}!\")\n",
    "\n",
    "cluster = None  # (Optional) Specify existing scheduler port\n",
    "if cluster is None:\n",
    "    cluster = LocalCUDACluster(\n",
    "        protocol=protocol,\n",
    "        n_workers=len(visible_devices.split(\",\")),\n",
    "        CUDA_VISIBLE_DEVICES=visible_devices,\n",
    "        device_memory_limit=device_limit,\n",
    "        local_directory=dask_workdir,\n",
    "        dashboard_address=\":\" + dashboard_port,\n",
    "        rmm_pool_size=(device_pool_size // 256) * 256,\n",
    "    )\n",
    "\n",
    "# Create the distributed client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. We initialized our Dask cluster and NVTabular will execute the workflow on multiple GPUs. Similar, we could define a cluster with multiple nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our Preprocessing Pipeline\n",
    "At this point, our data still isn't in a form that's ideal for consumption by neural networks. The most pressing issues are missing values and the fact that our categorical variables are still represented by random, discrete identifiers, and need to be transformed into contiguous indices that can be leveraged by a learned embedding. Less pressing, but still important for learning dynamics, are the distributions of our continuous variables, which are distributed across multiple orders of magnitude and are uncentered (i.e. E[x] != 0).\n",
    "\n",
    "We can fix these issues in a conscise and GPU-accelerated manner with an NVTabular `Workflow`. We explained the NVTabular API in [Getting Started with Movielens notebooks](https://github.com/NVIDIA/NVTabular/tree/main/examples/getting-started-movielens) and hope you are familiar with the syntax.\n",
    "\n",
    "#### Frequency Thresholding\n",
    "One interesting thing worth pointing out is that we're using _frequency thresholding_ in our `Categorify` op. This handy functionality will map all categories which occur in the dataset with some threshold level of infrequency (which we've set here to be 15 occurrences throughout the dataset) to the _same_ index, keeping the model from overfitting to sparse signals.\n",
    "\n",
    "#### Adding Metadata\n",
    "Recently, we added support to tag columns with the `AddMetadata` operator. Using tags enables us to automate workflows and following training processes by standardizing the dataset schema.\n",
    "<br><br>\n",
    "In many real world use cases, the feature engineering pipelines are defined for a group of features. For example, we apply normalization to all continuous features. Similarly, NVTabular defines workflows on a list of feature names. We can add the `AddMetadata` operator to define our dataset schema.<br><br>\n",
    "\n",
    "```python\n",
    "AddMetadata(tags=None, properties=None)\n",
    "```\n",
    "\n",
    "NVTabular will write a schema file, when we store the dataset to disk with `workflow.transform(train_dataset).to_parquet`. In later steps, we can load the schema file and select columns by tags.<br><br>\n",
    "We will add the tags `target`, `cat` and `cont`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# define our dataset schema\n",
    "CONTINUOUS_COLUMNS = [\"I\" + str(x) for x in range(1, 14)]\n",
    "CATEGORICAL_COLUMNS = [\"C\" + str(x) for x in range(1, 27)]\n",
    "LABEL_COLUMNS = [\"label\"]\n",
    "COLUMNS = CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS + LABEL_COLUMNS\n",
    "\n",
    "num_buckets = 10000000\n",
    "categorify_op = Categorify(out_path=stats_path, max_size=num_buckets)\n",
    "cat_features = CATEGORICAL_COLUMNS >> categorify_op >> AddMetadata(tags=[\"cat\"])\n",
    "cont_features = (\n",
    "    CONTINUOUS_COLUMNS\n",
    "    >> FillMissing()\n",
    "    >> Clip(min_value=0)\n",
    "    >> Normalize()\n",
    "    >> AddMetadata(tags=[\"cont\"])\n",
    ")\n",
    "label_feature = LABEL_COLUMNS >> AddMetadata(tags=[\"label\"])\n",
    "\n",
    "features = cat_features + cont_features + label_feature\n",
    "\n",
    "workflow = nvt.Workflow(features, client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instantiate dataset iterators to loop through our dataset (which we couldn't fit into GPU memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = nvt.Dataset(train_paths, engine=\"parquet\", part_size=part_size)\n",
    "valid_dataset = nvt.Dataset(valid_paths, engine=\"parquet\", part_size=part_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run them through our workflows to collect statistics on the train set, then transform and save to parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "output_train_dir = os.path.join(OUTPUT_DATA_DIR, \"train/\")\n",
    "output_valid_dir = os.path.join(OUTPUT_DATA_DIR, \"valid/\")\n",
    "! mkdir -p $output_train_dir\n",
    "! mkdir -p $output_valid_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, let's time it to see how long it takes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.62 s, sys: 322 ms, total: 1.94 s\n",
      "Wall time: 21.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<nvtabular.workflow.workflow.Workflow at 0x7fac16fef970>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "workflow.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to enforce that the output datasets have the required HugeCTR data types. We define a dictionary with column names and data types. We give it as an argument when we transform and store our dataset to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dtypes = {}\n",
    "\n",
    "for col in CATEGORICAL_COLUMNS:\n",
    "    dict_dtypes[col] = np.int64\n",
    "\n",
    "for col in CONTINUOUS_COLUMNS:\n",
    "    dict_dtypes[col] = np.float32\n",
    "\n",
    "for col in LABEL_COLUMNS:\n",
    "    dict_dtypes[col] = np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 543 ms, sys: 252 ms, total: 796 ms\n",
      "Wall time: 24.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Add \"write_hugectr_keyset=True\" to \"to_parquet\" if using this ETL Notebook for training with HugeCTR\n",
    "workflow.transform(train_dataset).to_parquet(\n",
    "    output_files=len(NUM_GPUS),\n",
    "    output_path=output_train_dir,\n",
    "    shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "    dtypes=dict_dtypes,\n",
    "    cats=CATEGORICAL_COLUMNS,\n",
    "    conts=CONTINUOUS_COLUMNS,\n",
    "    labels=LABEL_COLUMNS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 494 ms, sys: 213 ms, total: 707 ms\n",
      "Wall time: 21.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Add \"write_hugectr_keyset=True\" to \"to_parquet\" if using this ETL Notebook for training with HugeCTR\n",
    "workflow.transform(valid_dataset).to_parquet(\n",
    "    output_path=output_valid_dir,\n",
    "    dtypes=dict_dtypes,\n",
    "    cats=CATEGORICAL_COLUMNS,\n",
    "    conts=CONTINUOUS_COLUMNS,\n",
    "    labels=LABEL_COLUMNS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebooks, we will train a deep learning model. Our training pipeline requires information about the data schema to define the neural network architecture. We will save the NVTabular workflow to disk so that we can restore it in the next notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.save(os.path.join(OUTPUT_DATA_DIR, \"workflow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing Schema File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look on the schema file. In the output foldes, NVTabular created a `schema.pbtxt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"C1\"\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"C1\"\n",
      "    min: 0\n",
      "    max: 9999999\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"cat\"\n",
      "    tag: \"categorical\"\n",
      "    extra_metadata {\n",
      "      type_url: \"type.googleapis.com/google.protobuf.Struct\"\n",
      "      value: \"\\n\\021\\n\\013num_buckets\\022\\002\\010\\000\\n\\033\\n\\016freq_threshold\\022\\t\\021\\000\\000\\000\\000\\000\\000\\000\\000\\n\\025\\n\\010max_size\\022\\t\\021\\000\\000\\000\\000\\320\\022cA\\n\\030\\n\\013start_index\\022\\t\\021\\000\\000\\000\\000\\000\\000\\000\\000\\nP\\n\\010cat_path\\022D\\032B/raid/benny/04_criteo/test_dask/stats/categories/unique.C1.parquet\\nG\\n\\017embedding_sizes\\0224*2\\n\\030\\n\\013cardinality\\022\\t\\021\\000\\000\\000\\340\\317\\022cA\\n\\026\\n\\tdimension\\022\\t\\021\\000\\000\\000\\000\\000\\000\\200@\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"C2\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"head -n 20 \" + output_train_dir + \"/schema.pbtxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the schema from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = nvt.Schema().load(output_train_dir + \"/schema.pbtxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select a column by name and get its tags and properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'C1', 'tags': ['cat', <Tags.CATEGORICAL: 'categorical'>], 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 10000000.0, 'start_index': 0.0, 'cat_path': '/raid/benny/04_criteo/test_dask/stats/categories/unique.C1.parquet', 'embedding_sizes': {'cardinality': 9999999.0, 'dimension': 512.0}, 'domain': {'min': 0, 'max': 9999999}}, 'dtype': <class 'int'>, '_is_list': False}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.select_by_name(\"C1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select columns by tags. We will select the columns by our defined tags `label`, `cat` and `cont`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.name for x in schema.select_by_tag(\"label\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C1',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4',\n",
       " 'C5',\n",
       " 'C6',\n",
       " 'C7',\n",
       " 'C8',\n",
       " 'C9',\n",
       " 'C10',\n",
       " 'C11',\n",
       " 'C12',\n",
       " 'C13',\n",
       " 'C14',\n",
       " 'C15',\n",
       " 'C16',\n",
       " 'C17',\n",
       " 'C18',\n",
       " 'C19',\n",
       " 'C20',\n",
       " 'C21',\n",
       " 'C22',\n",
       " 'C23',\n",
       " 'C24',\n",
       " 'C25',\n",
       " 'C26']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.name for x in schema.select_by_tag(\"cat\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I1',\n",
       " 'I2',\n",
       " 'I3',\n",
       " 'I4',\n",
       " 'I5',\n",
       " 'I6',\n",
       " 'I7',\n",
       " 'I8',\n",
       " 'I9',\n",
       " 'I10',\n",
       " 'I11',\n",
       " 'I12',\n",
       " 'I13']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.name for x in schema.select_by_tag(\"cont\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'label', 'tags': ['label'], 'properties': {}, 'dtype': <class 'int'>, '_is_list': False}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.select_by_name(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added manually tags to our data schema. NVTabular can infer column types, as well, and adds automatically tags to the schema file. For example, the output data type of Normalize is continuous and the output data type of Categorify is categorical.<br><br>\n",
    "We could use the pre-defined NVTabular tags, as well. However, we added them manually to demonstrate how to add custom tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C1',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4',\n",
       " 'C5',\n",
       " 'C6',\n",
       " 'C7',\n",
       " 'C8',\n",
       " 'C9',\n",
       " 'C10',\n",
       " 'C11',\n",
       " 'C12',\n",
       " 'C13',\n",
       " 'C14',\n",
       " 'C15',\n",
       " 'C16',\n",
       " 'C17',\n",
       " 'C18',\n",
       " 'C19',\n",
       " 'C20',\n",
       " 'C21',\n",
       " 'C22',\n",
       " 'C23',\n",
       " 'C24',\n",
       " 'C25',\n",
       " 'C26']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.name for x in schema.select_by_tag(nvt.graph.tags.Tags.CATEGORICAL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I1',\n",
       " 'I2',\n",
       " 'I3',\n",
       " 'I4',\n",
       " 'I5',\n",
       " 'I6',\n",
       " 'I7',\n",
       " 'I8',\n",
       " 'I9',\n",
       " 'I10',\n",
       " 'I11',\n",
       " 'I12',\n",
       " 'I13']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.name for x in schema.select_by_tag(nvt.graph.tags.Tags.CONTINUOUS)]"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "mimetype": "text/x-python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
