{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Criteo: Triton Inference with HugeCTR\n",
    "\n",
    "## Overview\n",
    "\n",
    "The last step is to deploy the ETL workflow and saved model to production. In the production setting, we want to transform the input data as during training (ETL). We need to apply the same mean/std for continuous features and use the same categorical mapping to convert the categories to continuous integer before we use the deep learning model for a prediction. Therefore, we deploy the NVTabular workflow with the HugeCTR model as an ensemble model to Triton Inference. The ensemble model garantuees that the same transformation are applied to the raw inputs.\n",
    "\n",
    "<img src='./imgs/triton-hugectr.png' width=\"25%\">\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this notebook, we learn how to deploy our models to production:\n",
    "\n",
    "- Use **NVTabular** to generate config and model files for Triton Inference Server\n",
    "- Deploy an ensemble of NVTabular workflow and HugeCTR model\n",
    "- Send example request to Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with Triton and HugeCTR\n",
    "\n",
    "First, we need to generate the Triton Inference Server configurations and save the models in the correct format. In the previous notebooks [02-ETL-with-NVTabular](./02-ETL-with-NVTabular.ipynb) and [03-Training-with-HugeCTR](./03-Training-with-HugeCTR.ipynb) we saved the NVTabular workflow and HugeCTR model to disk. We will load them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Ensemble Model for Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training terminates, we can see that two `.model` files are generated. We need to move them inside a temporary folder, like `criteo_hugectr/1`. Let's create these folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move our saved `.model` files inside 1 folder. We use only the last snapshot after `9600` iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"mv *9600.model ./criteo_hugectr/1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save our models to be deployed at the inference stage. To do so we will use export_hugectr_ensemble method below. With this method, we can generate the config.pbtxt files automatically for each model. In doing so, we should also create a hugectr_params dictionary, and define the parameters like where the amazonreview.json file will be read, slots which corresponds to number of categorical features, `embedding_vector_size`, `max_nnz`, and `n_outputs` which is number of outputs.<br><br>\n",
    "The script below creates an ensemble triton server model where\n",
    "- workflow is the the nvtabular workflow used in preprocessing,\n",
    "- hugectr_model_path is the HugeCTR model that should be served. \n",
    "- This path includes the .model files.name is the base name of the various triton models\n",
    "- output_path is the path where is model will be saved to.\n",
    "- cats are the categorical column names\n",
    "- conts are the continuous column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the NVTabular workflow first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvtabular as nvt\n",
    "\n",
    "BASE_DIR = os.environ.get(\"BASE_DIR\", \"/raid/data/criteo\")\n",
    "input_path = os.path.join(BASE_DIR, \"test_dask/output\")\n",
    "workflow = nvt.Workflow.load(os.path.join(input_path, \"workflow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clear the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"rm -rf /model/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvtabular.inference.triton import export_hugectr_ensemble\n",
    "\n",
    "hugectr_params = dict()\n",
    "hugectr_params[\"config\"] = \"/model/criteo/1/criteo.json\"\n",
    "hugectr_params[\"slots\"] = 26\n",
    "hugectr_params[\"max_nnz\"] = 1\n",
    "hugectr_params[\"embedding_vector_size\"] = 128\n",
    "hugectr_params[\"n_outputs\"] = 1\n",
    "export_hugectr_ensemble(\n",
    "    workflow=workflow,\n",
    "    hugectr_model_path=\"./criteo_hugectr/1/\",\n",
    "    hugectr_params=hugectr_params,\n",
    "    name=\"criteo\",\n",
    "    output_path=\"/model/\",\n",
    "    label_columns=[\"label\"],\n",
    "    cats=[\"C\" + str(x) for x in range(1, 27)],\n",
    "    conts=[\"I\" + str(x) for x in range(1, 14)],\n",
    "    max_batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the generated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/model\u001b[00m\r\n",
      "├── \u001b[01;34mcriteo\u001b[00m\r\n",
      "│   ├── \u001b[01;34m1\u001b[00m\r\n",
      "│   │   ├── 0_opt_sparse_9600.model\r\n",
      "│   │   ├── \u001b[01;34m0_sparse_9600.model\u001b[00m\r\n",
      "│   │   │   ├── emb_vector\r\n",
      "│   │   │   ├── key\r\n",
      "│   │   │   └── slot_id\r\n",
      "│   │   ├── _dense_9600.model\r\n",
      "│   │   ├── _opt_dense_9600.model\r\n",
      "│   │   └── criteo.json\r\n",
      "│   └── config.pbtxt\r\n",
      "├── \u001b[01;34mcriteo_ens\u001b[00m\r\n",
      "│   ├── \u001b[01;34m1\u001b[00m\r\n",
      "│   └── config.pbtxt\r\n",
      "└── \u001b[01;34mcriteo_nvt\u001b[00m\r\n",
      "    ├── \u001b[01;34m1\u001b[00m\r\n",
      "    │   ├── model.py\r\n",
      "    │   └── \u001b[01;34mworkflow\u001b[00m\r\n",
      "    │       ├── \u001b[01;34mcategories\u001b[00m\r\n",
      "    │       │   ├── unique.C1.parquet\r\n",
      "    │       │   ├── unique.C10.parquet\r\n",
      "    │       │   ├── unique.C11.parquet\r\n",
      "    │       │   ├── unique.C12.parquet\r\n",
      "    │       │   ├── unique.C13.parquet\r\n",
      "    │       │   ├── unique.C14.parquet\r\n",
      "    │       │   ├── unique.C15.parquet\r\n",
      "    │       │   ├── unique.C16.parquet\r\n",
      "    │       │   ├── unique.C17.parquet\r\n",
      "    │       │   ├── unique.C18.parquet\r\n",
      "    │       │   ├── unique.C19.parquet\r\n",
      "    │       │   ├── unique.C2.parquet\r\n",
      "    │       │   ├── unique.C20.parquet\r\n",
      "    │       │   ├── unique.C21.parquet\r\n",
      "    │       │   ├── unique.C22.parquet\r\n",
      "    │       │   ├── unique.C23.parquet\r\n",
      "    │       │   ├── unique.C24.parquet\r\n",
      "    │       │   ├── unique.C25.parquet\r\n",
      "    │       │   ├── unique.C26.parquet\r\n",
      "    │       │   ├── unique.C3.parquet\r\n",
      "    │       │   ├── unique.C4.parquet\r\n",
      "    │       │   ├── unique.C5.parquet\r\n",
      "    │       │   ├── unique.C6.parquet\r\n",
      "    │       │   ├── unique.C7.parquet\r\n",
      "    │       │   ├── unique.C8.parquet\r\n",
      "    │       │   └── unique.C9.parquet\r\n",
      "    │       ├── column_types.json\r\n",
      "    │       ├── metadata.json\r\n",
      "    │       └── workflow.pkl\r\n",
      "    └── config.pbtxt\r\n",
      "\r\n",
      "9 directories, 40 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree /model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to write a configuration file with the stored model weights and model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /model/ps.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile '/model/ps.json'\n",
    "# fmt: off\n",
    "{\n",
    "    \"supportlonglong\": true,\n",
    "    \"models\": [\n",
    "        {\n",
    "            \"model\": \"criteo\",\n",
    "            \"sparse_files\": [\"/model/criteo/1/0_sparse_9600.model\"],\n",
    "            \"dense_file\": \"/model/criteo/1/_dense_9600.model\",\n",
    "            \"network_file\": \"/model/criteo/1/criteo.json\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Ensemble Model with Triton Inference Server\n",
    "\n",
    "We have only saved the models for Triton Inference Server. We started Triton Inference Server in explicit mode, meaning that we need to send a request that Triton will load the ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We connect to the Triton Inference Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tritonhttpclient/__init__.py:31: DeprecationWarning: The package `tritonhttpclient` is deprecated and will be removed in a future version. Please use instead `tritonclient.http`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tritonhttpclient\n",
    "\n",
    "try:\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We deactivate warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check if the server is alive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.is_server_live()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the available models in the repositories:\n",
    "- criteo_ens: Ensemble \n",
    "- criteo_nvt: NVTabular \n",
    "- criteo: HugeCTR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '93'}>\n",
      "bytearray(b'[{\"name\":\".ipynb_checkpoints\"},{\"name\":\"criteo\"},{\"name\":\"criteo_ens\"},{\"name\":\"criteo_nvt\"}]')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': '.ipynb_checkpoints'},\n",
       " {'name': 'criteo'},\n",
       " {'name': 'criteo_ens'},\n",
       " {'name': 'criteo_nvt'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.get_model_repository_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the models individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/models/criteo_nvt/load, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'criteo_nvt'\n",
      "CPU times: user 4.21 ms, sys: 258 µs, total: 4.47 ms\n",
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "triton_client.load_model(model_name=\"criteo_nvt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/models/criteo/load, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'criteo'\n",
      "CPU times: user 1.8 ms, sys: 3.01 ms, total: 4.81 ms\n",
      "Wall time: 32.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "triton_client.load_model(model_name=\"criteo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/models/criteo_ens/load, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'criteo_ens'\n",
      "CPU times: user 4.7 ms, sys: 0 ns, total: 4.7 ms\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "triton_client.load_model(model_name=\"criteo_ens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Request to Triton Inference Server\n",
    "\n",
    "Now, the models are loaded and we can create a sample request. We read an example **raw batch** for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     I1   I2    I3    I4    I5  I6  I7  I8  I9  I10  ...        C17  \\\n",
      "0     5  110  <NA>    16  <NA>   1   0  14   7    1  ... -771205462   \n",
      "1    32    3     5  <NA>     1   0   0  61   5    0  ... -771205462   \n",
      "2  <NA>  233     1   146     1   0   0  99   7    0  ... -771205462   \n",
      "\n",
      "          C18         C19         C20         C21        C22        C23  \\\n",
      "0 -1206449222 -1793932789 -1014091992   351689309  632402057 -675152885   \n",
      "1 -1578429167 -1793932789   -20981661 -1556988767 -924717482  391309800   \n",
      "2  1653545869 -1793932789 -1014091992   351689309  632402057 -675152885   \n",
      "\n",
      "          C24         C25         C26  \n",
      "0  2091868316   809724924  -317696227  \n",
      "1  1966410890 -1726799382 -1218975401  \n",
      "2   883538181   -10139646  -317696227  \n",
      "\n",
      "[3 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "import cudf\n",
    "\n",
    "# read in the workflow (to get input/output schema to call triton with)\n",
    "batch_path = os.path.join(BASE_DIR, \"converted/criteo\")\n",
    "batch = cudf.read_parquet(os.path.join(batch_path, \"*.parquet\"), num_rows=3)\n",
    "batch = batch[[x for x in batch.columns if x != \"label\"]]\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the batch for inference by using correct column names and data types. We use the same datatypes as defined in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I1     int32\n",
       "I2     int32\n",
       "I3     int32\n",
       "I4     int32\n",
       "I5     int32\n",
       "I6     int32\n",
       "I7     int32\n",
       "I8     int32\n",
       "I9     int32\n",
       "I10    int32\n",
       "I11    int32\n",
       "I12    int32\n",
       "I13    int32\n",
       "C1     int32\n",
       "C2     int32\n",
       "C3     int32\n",
       "C4     int32\n",
       "C5     int32\n",
       "C6     int32\n",
       "C7     int32\n",
       "C8     int32\n",
       "C9     int32\n",
       "C10    int32\n",
       "C11    int32\n",
       "C12    int32\n",
       "C13    int32\n",
       "C14    int32\n",
       "C15    int32\n",
       "C16    int32\n",
       "C17    int32\n",
       "C18    int32\n",
       "C19    int32\n",
       "C20    int32\n",
       "C21    int32\n",
       "C22    int32\n",
       "C23    int32\n",
       "C24    int32\n",
       "C25    int32\n",
       "C26    int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import np_to_triton_dtype\n",
    "\n",
    "inputs = []\n",
    "\n",
    "col_names = list(batch.columns)\n",
    "col_dtypes = [np.int32] * len(col_names)\n",
    "\n",
    "for i, col in enumerate(batch.columns):\n",
    "    d = batch[col].values_host.astype(col_dtypes[i])\n",
    "    d = d.reshape(len(d), 1)\n",
    "    inputs.append(httpclient.InferInput(col_names[i], d.shape, np_to_triton_dtype(col_dtypes[i])))\n",
    "    inputs[i].set_data_from_numpy(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We send the request to the triton server and collect the last output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/models/criteo_ens/infer, headers {'Inference-Header-Content-Length': 3383}\n",
      "b'{\"id\":\"1\",\"inputs\":[{\"name\":\"I1\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"I2\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"I3\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"I4\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"I5\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"I6\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"I7\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"I8\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"I9\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"I10\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"I11\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"I12\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"I13\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C1\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C2\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C3\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C4\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C5\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C6\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C7\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C8\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C9\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C10\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C11\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C12\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C13\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C14\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C15\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C16\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C17\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C18\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C19\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C20\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C21\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C22\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C23\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C24\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C25\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}},{\"name\":\"C26\",\"shape\":[3,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":12}}],\"outputs\":[{\"name\":\"OUTPUT0\",\"parameters\":{\"binary_data\":true}}]}\\x05\\x00\\x00\\x00 \\x00\\x00\\x00\\x00\\x00\\x00\\x00n\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\xe9\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x92\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x0e\\x00\\x00\\x00=\\x00\\x00\\x00c\\x00\\x00\\x00\\x07\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x07\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x002\\x01\\x00\\x00U\\x0c\\x00\\x00\\x1d\\x0c\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x01\\x00\\x00\\x00y\\rwb\\x8d\\xfd\\xf3\\xe5y\\rwbX]\\x1f\\xe2\\xa6\\xff\\xaa\\xa0\\x03B\\x98\\xad/D\\xea\\xaf\\xd5\\x15\\xaao\\r\\xc6\\xbeb\\xcf\\x7f\\\\\\x94!4\\x8a\\xda\\xeeIl8H\\'\\xb08#\\x9f\\xd6<M\\x06U\\xe7\\xcbm\\xcdo\\xcbm\\xcdo\\xcbm\\xcdo!\\xaa\\x805\\x81\\xed\\x16\\xabb\\xeb\\xf5\\xb5\\x03\\x89\\x80()lBC\\x8b\\xcc\\xf2\\xd1\\xa6\\xdf\\xdeFT\\xe1\\xf5\\x1d\\x1f\\x82N.\\xc1}\\x02.\\xa9\\xc0\\xe9}\\xc1}\\x02.1B|\\x0cd\\xdcRf1B|\\x0c\\x1f\\x1d\\x98\\x95\\'N\\xeb\\x99\\x84aq\\x12\\xb7\\xff\\xc5\\x00\\xb7\\xff\\xc5\\x00\\xb7\\xff\\xc5\\x007\\xe5N\\xbe7\\xe5N\\xbe7\\xe5N\\xbe\\xcct\\x0b\\x8a\\x99\\xfe\\xbb\\xf3\\x0b\\r\\x0f\\xf7\\xfa>\\xdcL\\xfa>\\xdcL\\xfa>\\xdcL\\xaaV\\x08\\xd2\\xaaV\\x08\\xd2\\xaaV\\x08\\xd2\\xba\\x0b\\x17\\xb8\\x11\\x15\\xeb\\xa1\\x8d\\x1b\\x8fb\\x0b\\xc2\\x12\\x95\\x0b\\xc2\\x12\\x95\\x0b\\xc2\\x12\\x95(/\\x8e\\xc3c\\xd8\\xbf\\xfe(/\\x8e\\xc3]Z\\xf6\\x14\\xa1<2\\xa3]Z\\xf6\\x14\\x89\\xb0\\xb1%V\\xee\\xe1\\xc8\\x89\\xb0\\xb1%\\x0b\\xfc\\xc1\\xd7\\xe8\\xe9R\\x17\\x0b\\xfc\\xc1\\xd7\\x9c`\\xaf|\\x8a\\x0c5u\\x05\\xb9\\xa94\\xfckC0\\xea!\\x13\\x99\\x02He\\xff\\x1dW\\x10\\xedW\\xe9W\\xb7\\x1dW\\x10\\xed'\n",
      "<HTTPSocketPoolResponse status=400 headers={'content-length': '122', 'content-type': 'text/plain'}>\n"
     ]
    },
    {
     "ename": "InferenceServerException",
     "evalue": "in ensemble 'criteo_ens', Failed to process the request(s), message: The stub process has exited unexpectedly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInferenceServerException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2961/3517835948.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# This InferenceServerClient object is what we'll be using to talk to Triton.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# make the request with tritonclient.http.InferInput object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriton_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"criteo_ens\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predicted sigmoid result:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OUTPUT0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tritonclient/http/__init__.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, model_name, inputs, model_version, outputs, request_id, sequence_id, sequence_start, sequence_end, priority, timeout, headers, query_params, request_compression_algorithm, response_compression_algorithm)\u001b[0m\n\u001b[1;32m   1254\u001b[0m                               \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                               query_params=query_params)\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0m_raise_if_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mInferResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tritonclient/http/__init__.py\u001b[0m in \u001b[0;36m_raise_if_error\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInferenceServerException\u001b[0m: in ensemble 'criteo_ens', Failed to process the request(s), message: The stub process has exited unexpectedly."
     ]
    }
   ],
   "source": [
    "# placeholder variables for the output\n",
    "outputs = [httpclient.InferRequestedOutput(\"OUTPUT0\")]\n",
    "\n",
    "# build a client to connect to our server.\n",
    "# This InferenceServerClient object is what we'll be using to talk to Triton.\n",
    "# make the request with tritonclient.http.InferInput object\n",
    "response = triton_client.infer(\"criteo_ens\", inputs, request_id=\"1\", outputs=outputs)\n",
    "\n",
    "print(\"predicted sigmoid result:\\n\", response.as_numpy(\"OUTPUT0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unload the model. We need to unload each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/models/criteo_ens/unload, headers None\n",
      "{\"parameters\":{\"unload_dependents\":false}}\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'criteo_ens'\n",
      "POST /v2/repository/models/criteo_nvt/unload, headers None\n",
      "{\"parameters\":{\"unload_dependents\":false}}\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'criteo_nvt'\n",
      "POST /v2/repository/models/criteo/unload, headers None\n",
      "{\"parameters\":{\"unload_dependents\":false}}\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'criteo'\n"
     ]
    }
   ],
   "source": [
    "triton_client.unload_model(model_name=\"criteo_ens\")\n",
    "triton_client.unload_model(model_name=\"criteo_nvt\")\n",
    "triton_client.unload_model(model_name=\"criteo\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
