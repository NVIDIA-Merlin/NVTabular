{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Criteo: Multi-GPU Training with TensorFlow\n",
    "\n",
    "## Overview\n",
    "\n",
    "We observed that TensorFlow training pipelines can be slow as the dataloader is a bottleneck. The native dataloader in TensorFlow randomly sample each item from the dataset, which is very slow. The window dataloader in TensorFlow is not much faster. In our experiments, we are able to speed-up existing TensorFlow pipelines by 9x using a highly optimized dataloader.<br><br>\n",
    "\n",
    "We have already discussed the NVTabular dataloader for TensorFlow in more detail in our [Getting Started with Movielens notebooks](https://github.com/NVIDIA/NVTabular/tree/main/examples/getting-started-movielens). We provided an example to [train a TensorFlow model for the criteo dataset with a single GPU](./03-Training-with-TF.ipynb) and an [examples for multi-GPU training on Movielens](../multi-gpu-movielens/).<br><br>\n",
    "\n",
    "This notebook is another example to train a TensorFlow model with NVTabular data loaders using multiple GPUs. Our multi-GPU examples use [Horovod](https://github.com/horovod/horovod) for distributed training.\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this notebook, we learn how to:\n",
    "\n",
    "- Use **NVTabular dataloader** with TensorFlow Keras model\n",
    "- Use **[Horovod](https://github.com/horovod/horovod)** to train with multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVTabular dataloader for TensorFlow\n",
    "\n",
    "We’ve identified that the dataloader is one bottleneck in deep learning recommender systems when training pipelines with TensorFlow. The dataloader cannot prepare the next batch fast enough and therefore, the GPU is not fully utilized. \n",
    "\n",
    "We developed a highly customized tabular dataloader for accelerating existing pipelines in TensorFlow. In our experiments, we see a speed-up by 9x of the same training workflow with NVTabular dataloader. NVTabular dataloader’s features are:\n",
    "\n",
    "- removing bottleneck of item-by-item dataloading\n",
    "- enabling larger than memory dataset by streaming from disk\n",
    "- reading data directly into GPU memory and remove CPU-GPU communication\n",
    "- preparing batch asynchronously in GPU to avoid CPU-GPU communication\n",
    "- supporting commonly used .parquet format\n",
    "- easy integration into existing TensorFlow pipelines by using similar API - works with tf.keras models\n",
    "- **supporting multi-GPU training with Horovod**\n",
    "\n",
    "More information in our [blogpost](https://medium.com/nvidia-merlin/training-deep-learning-based-recommender-systems-9x-faster-with-tensorflow-cc5a2572ea49)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you are not familiar with NVTabular data loaders, we recommend you to review the basics in our starting notebooks [Movielens](../getting-started-movielens/03-Training-with-TF.ipynb) and [Criteo](03-Training-with-TF.ipynb).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horovod is a framework, which enables distributed training for TensorFlow, Keras, PyTorch and Apache MXNet models. It will launch the distributed training from the command line and requires a Python file. We will transform the [Single-GPU example of Criteo](03-Training-with-TF.ipynb) into a Python file and will launch Horovod.\n",
    "<br><br>\n",
    "**If you are not familiar with NVTabular+Horovod, we recommend you to review our [basic examples for distributed training](../multi-gpu-movielens/). They explain the required modifications.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DIR = os.environ.get(\"BASE_DIR\", \"/raid/data/criteo\")\n",
    "OUTPUT_DATA_DIR = os.environ.get(\"OUTPUT_DATA_DIR\", BASE_DIR + \"/test_dask/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile './tf_trainer.py'\n",
    "\n",
    "# External dependencies\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import cupy\n",
    "\n",
    "# we can control how much memory to give tensorflow with this environment variable\n",
    "# IMPORTANT: make sure you do this before you initialize TF's runtime, otherwise\n",
    "# TF will have claimed all free GPU memory\n",
    "os.environ[\"TF_MEMORY_ALLOCATION\"] = \"0.4\"  # fraction of free memory\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.framework_utils.tensorflow import layers\n",
    "from nvtabular.loader.tensorflow import KerasSequenceLoader\n",
    "\n",
    "import tensorflow as tf\n",
    "import horovod.tensorflow as hvd\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Process some integers.\")\n",
    "parser.add_argument(\"--dir_in\", default=None, help=\"Input directory\")\n",
    "parser.add_argument(\"--batch_size\", default=None, help=\"batch size\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "BASE_DIR = args.dir_in\n",
    "BATCH_SIZE = int(args.batch_size)\n",
    "\n",
    "schema = nvt.Schema().load(os.path.join(BASE_DIR + \"train/schema.pbtxt\"))\n",
    "\n",
    "CONTINUOUS_COLUMNS = [x.name for x in schema.select_by_tag(nvt.graph.tags.Tags.CONTINUOUS)]\n",
    "CATEGORICAL_COLUMNS = [x.name for x in schema.select_by_tag(nvt.graph.tags.Tags.CATEGORICAL)]\n",
    "LABEL_COLUMNS = [x.name for x in schema.select_by_tag(\"label\")]\n",
    "\n",
    "TRAIN_PATHS = sorted(\n",
    "    glob.glob(os.path.join(BASE_DIR, \"train/*.parquet\"))\n",
    ")  # Output from ETL-with-NVTabular\n",
    "VALID_PATHS = sorted(\n",
    "    glob.glob(os.path.join(BASE_DIR, \"valid/*.parquet\"))\n",
    ")  # Output from ETL-with-NVTabular\n",
    "hvd.init()\n",
    "\n",
    "# Seed with system randomness (or a static seed)\n",
    "cupy.random.seed(None)\n",
    "\n",
    "\n",
    "def seed_fn():\n",
    "    \"\"\"\n",
    "    Generate consistent dataloader shuffle seeds across workers\n",
    "\n",
    "    Reseeds each worker's dataloader each epoch to get fresh a shuffle\n",
    "    that's consistent across workers.\n",
    "    \"\"\"\n",
    "    min_int, max_int = tf.int32.limits\n",
    "    max_rand = max_int // hvd.size()\n",
    "\n",
    "    # Generate a seed fragment on each worker\n",
    "    seed_fragment = cupy.random.randint(0, max_rand).get()\n",
    "\n",
    "    # Aggregate seed fragments from all Horovod workers\n",
    "    seed_tensor = tf.constant(seed_fragment)\n",
    "    reduced_seed = hvd.allreduce(seed_tensor, name=\"shuffle_seed\", op=hvd.mpi_ops.Sum)\n",
    "\n",
    "    return reduced_seed % max_rand\n",
    "\n",
    "\n",
    "proc = nvt.Workflow.load(os.path.join(BASE_DIR, \"workflow/\"))\n",
    "EMBEDDING_TABLE_SHAPES = nvt.ops.get_embedding_sizes(proc)\n",
    "\n",
    "for key in EMBEDDING_TABLE_SHAPES.keys():\n",
    "    EMBEDDING_TABLE_SHAPES[key] = (\n",
    "        EMBEDDING_TABLE_SHAPES[key][0],\n",
    "        min(4, EMBEDDING_TABLE_SHAPES[key][1]),\n",
    "    )\n",
    "\n",
    "train_dataset_tf = KerasSequenceLoader(\n",
    "    TRAIN_PATHS,  # you could also use a glob pattern\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_names=LABEL_COLUMNS,\n",
    "    cat_names=CATEGORICAL_COLUMNS,\n",
    "    cont_names=NUMERIC_COLUMNS,\n",
    "    engine=\"parquet\",\n",
    "    shuffle=False,\n",
    "    buffer_size=0.06,  # how many batches to load at once\n",
    "    parts_per_chunk=1,\n",
    "    global_size=hvd.size(),\n",
    "    global_rank=hvd.rank(),\n",
    "    seed_fn=seed_fn,\n",
    ")\n",
    "\n",
    "valid_dataset_tf = KerasSequenceLoader(\n",
    "    VALID_PATHS,  # you could also use a glob pattern\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_names=LABEL_COLUMNS,\n",
    "    cat_names=CATEGORICAL_COLUMNS,\n",
    "    cont_names=NUMERIC_COLUMNS,\n",
    "    engine=\"parquet\",\n",
    "    shuffle=False,\n",
    "    buffer_size=0.06,  # how many batches to load at once\n",
    "    parts_per_chunk=1,\n",
    "    global_size=hvd.size(),\n",
    "    global_rank=hvd.rank(),\n",
    "    seed_fn=seed_fn,\n",
    ")\n",
    "\n",
    "inputs = {}  # tf.keras.Input placeholders for each feature to be used\n",
    "emb_layers = []  # output of all embedding layers, which will be concatenated\n",
    "num_layers = []  # output of numerical layers\n",
    "\n",
    "for col in CATEGORICAL_COLUMNS:\n",
    "    inputs[col] = tf.keras.Input(name=col, dtype=tf.int32, shape=(1,))\n",
    "\n",
    "for col in NUMERIC_COLUMNS:\n",
    "    inputs[col] = tf.keras.Input(name=col, dtype=tf.float32, shape=(1,))\n",
    "\n",
    "for col in CATEGORICAL_COLUMNS:\n",
    "    emb_layers.append(\n",
    "        tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_identity(\n",
    "                col, EMBEDDING_TABLE_SHAPES[col][0]\n",
    "            ),  # Input dimension (vocab size)\n",
    "            EMBEDDING_TABLE_SHAPES[col][1],  # Embedding output dimension\n",
    "        )\n",
    "    )\n",
    "\n",
    "for col in NUMERIC_COLUMNS:\n",
    "    num_layers.append(tf.feature_column.numeric_column(col))\n",
    "\n",
    "emb_layer = layers.DenseFeatures(emb_layers + num_layers)\n",
    "x_emb_output = emb_layer(inputs)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(x_emb_output)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"output\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "opt = tf.keras.optimizers.Adam(0.01 * hvd.size())\n",
    "opt = hvd.DistributedOptimizer(opt)\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.losses.BinaryCrossentropy(), optimizer=opt, metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    hvd.keras.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "    hvd.keras.callbacks.MetricAverageCallback(),\n",
    "]\n",
    "\n",
    "# Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "if hvd.rank() == 0:\n",
    "    callbacks.append(tf.keras.callbacks.ModelCheckpoint(\"./checkpoint-{epoch}.h5\"))\n",
    "    callbacks.append(tf.keras.callbacks.TensorBoard(\"./logs\"))\n",
    "\n",
    "verbose = 1 if hvd.rank() == 0 else 0\n",
    "\n",
    "model.fit(\n",
    "    train_dataset_tf,\n",
    "    validation_data=valid_dataset_tf,\n",
    "    callbacks=callbacks,\n",
    "    steps_per_epoch=6435 // hvd.size(),\n",
    "    epochs=2,\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need a small wrapper script to check environment variables set by the Horovod runner to see which rank we'll be assigned, in order to set CUDA_VISIBLE_DEVICES properly for each worker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%writefile './hvd_wrapper.sh'\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# Get local process ID from OpenMPI or alternatively from SLURM\n",
    "if [ -z \"${CUDA_VISIBLE_DEVICES:-}\" ]; then\n",
    "    if [ -n \"${OMPI_COMM_WORLD_LOCAL_RANK:-}\" ]; then\n",
    "        LOCAL_RANK=\"${OMPI_COMM_WORLD_LOCAL_RANK}\"\n",
    "    elif [ -n \"${SLURM_LOCALID:-}\" ]; then\n",
    "        LOCAL_RANK=\"${SLURM_LOCALID}\"\n",
    "    fi\n",
    "    export CUDA_VISIBLE_DEVICES=${LOCAL_RANK}\n",
    "fi\n",
    "\n",
    "exec \"$@\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!horovodrun -np 2 sh hvd_wrapper.sh python tf_trainer.py --dir_in $OUTPUT_DATA_DIR --batch_size 16384"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
