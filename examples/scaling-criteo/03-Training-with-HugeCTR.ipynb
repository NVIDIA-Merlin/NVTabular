{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Criteo: Training with HugeCTR\n",
    "\n",
    "## Overview\n",
    "\n",
    "HugeCTR is an open-source framework to accelerate the training of CTR estimation models on NVIDIA GPUs. It is written in CUDA C++ and highly exploits GPU-accelerated libraries such as cuBLAS, cuDNN, and NCCL.<br><br>\n",
    "HugeCTR offers multiple advantages to train deep learning recommender systems:\n",
    "\n",
    "1. **Speed**: HugeCTR is a highly efficient framework written C++. We experienced upto 10x speed up. HugeCTR on a NVIDIA DGX A100 system proved to be the fastest commercially available solution for training the architecture Deep Learning Recommender Model (DLRM) developed by Facebook.\n",
    "2. **Scale**: HugeCTR supports model parallel scaling. It distributes the large embedding tables over multiple GPUs or multiple nodes. \n",
    "3. **Easy-to-use**: Easy-to-use Python API similar to Keras. Examples for popular deep learning recommender systems architectures (Wide&Deep, DLRM, DCN, DeepFM) are available.\n",
    "\n",
    "HugeCTR is able to train recommender system models with larger-than-memory embedding tables by leveraging a parameter server. \n",
    "\n",
    "You can find more information about HugeCTR [here](https://github.com/NVIDIA/HugeCTR).\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this notebook, we learn how to to use HugeCTR for training recommender system models\n",
    "\n",
    "- Use **HugeCTR** to define a recommender system model\n",
    "- Train Facebook's [Deep Learning Recommendation Model](https://arxiv.org/pdf/1906.00091.pdf) with HugeCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with HugeCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As HugeCTR optimizes the training in CUDA++, we need to define the training pipeline and model architecture and execute it via the commandline. We will use the Python API, which is similar to Keras models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not familiar with HugeCTR's Python API and parameters, you can read more in its GitHub repository:\n",
    "- [HugeCTR User Guide](https://github.com/NVIDIA/HugeCTR/blob/master/docs/hugectr_user_guide.md)\n",
    "- [HugeCTR Python API](https://github.com/NVIDIA/HugeCTR/blob/master/docs/python_interface.md)\n",
    "- [HugeCTR example architectures](https://github.com/NVIDIA/HugeCTR/tree/master/samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write the code to a `./model.py` file and execute it. It will create snapshot, which we will use for inference in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  valid  workflow\r\n"
     ]
    }
   ],
   "source": [
    "!ls /raid/data/criteo/test_dask/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './model.py'\n",
    "\n",
    "import hugectr\n",
    "from mpi4py import MPI  # noqa\n",
    "\n",
    "# HugeCTR\n",
    "solver = hugectr.CreateSolver(\n",
    "    vvgpu=[[0]],\n",
    "    max_eval_batches=100,\n",
    "    batchsize_eval=2720,\n",
    "    batchsize=2720,    \n",
    "    i64_input_key=True,\n",
    "    use_mixed_precision=False,\n",
    "    repeat_dataset=True,\n",
    ")\n",
    "optimizer = hugectr.CreateOptimizer(\n",
    "    optimizer_type=hugectr.Optimizer_t.SGD\n",
    ")\n",
    "reader = hugectr.DataReaderParams(\n",
    "    data_reader_type = hugectr.DataReaderType_t.Parquet,\n",
    "    source=[\"/raid/data/criteo/test_dask/output/train/_file_list.txt\"],\n",
    "    eval_source=\"/raid/data/criteo/test_dask/output/train/_file_list.txt\",\n",
    "    check_type=hugectr.Check_t.Non,\n",
    "    slot_size_array=[\n",
    "            10000000,\n",
    "            10000000,\n",
    "            3014529,\n",
    "            400781,\n",
    "            11,\n",
    "            2209,\n",
    "            11869,\n",
    "            148,\n",
    "            4,\n",
    "            977,\n",
    "            15,\n",
    "            38713,\n",
    "            10000000,\n",
    "            10000000,\n",
    "            10000000,\n",
    "            584616,\n",
    "            12883,\n",
    "            109,\n",
    "            37,\n",
    "            17177,\n",
    "            7425,\n",
    "            20266,\n",
    "            4,\n",
    "            7085,\n",
    "            1535,\n",
    "            64,\n",
    "        ]\n",
    ")\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.add(\n",
    "    hugectr.Input(\n",
    "        label_dim=1,\n",
    "        label_name=\"label\",\n",
    "        dense_dim=13,\n",
    "        dense_name=\"dense\",\n",
    "        data_reader_sparse_param_array=[\n",
    "            hugectr.DataReaderSparseParam(\"data1\", 26, 1, 26)\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.SparseEmbedding(\n",
    "        embedding_type=hugectr.Embedding_t.LocalizedSlotSparseEmbeddingHash,\n",
    "        workspace_size_per_gpu_in_mb=6000,\n",
    "        embedding_vec_size=128,\n",
    "        combiner=\"sum\",\n",
    "        sparse_embedding_name=\"sparse_embedding1\",\n",
    "        bottom_name=\"data1\",\n",
    "        optimizer = optimizer\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"dense\"],\n",
    "        top_names=[\"fc1\"],\n",
    "        num_output=512,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc1\"], top_names=[\"relu1\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu1\"],\n",
    "        top_names=[\"fc2\"],\n",
    "        num_output=256,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc2\"], top_names=[\"relu2\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu2\"],\n",
    "        top_names=[\"fc3\"],\n",
    "        num_output=128,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc3\"], top_names=[\"relu3\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.Interaction,\n",
    "        bottom_names=[\"relu3\", \"sparse_embedding1\"],\n",
    "        top_names=[\"interaction1\"],\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"interaction1\"],\n",
    "        top_names=[\"fc4\"],\n",
    "        num_output=1024,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc4\"], top_names=[\"relu4\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu4\"],\n",
    "        top_names=[\"fc5\"],\n",
    "        num_output=1024,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc5\"], top_names=[\"relu5\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu5\"],\n",
    "        top_names=[\"fc6\"],\n",
    "        num_output=512,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc6\"], top_names=[\"relu6\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu6\"],\n",
    "        top_names=[\"fc7\"],\n",
    "        num_output=256,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc7\"], top_names=[\"relu7\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu7\"],\n",
    "        top_names=[\"fc8\"],\n",
    "        num_output=1,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "        bottom_names=[\"fc8\", \"label\"],\n",
    "        top_names=[\"loss\"],\n",
    "    )\n",
    ")\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(\n",
    "    max_iter=10000,\n",
    "    eval_interval=3200,\n",
    "    display=1000,\n",
    "    snapshot=3200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[21d15h27m37s][HUGECTR][INFO]: Global seed is 2629093357\n",
      "[21d15h27m40s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 0: Tesla V100-SXM2-32GB\n",
      "[21d15h27m40s][HUGECTR][INFO]: num of DataReader workers: 1\n",
      "[21d15h27m40s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=12288000\n",
      "[21d15h27m40s][HUGECTR][INFO]: All2All Warmup Start\n",
      "[21d15h27m40s][HUGECTR][INFO]: All2All Warmup End\n",
      "===================================================Model Compile===================================================\n",
      "[21d15h27m55s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[21d15h27m55s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1                         \n",
      "(None, 1)                               (None, 13)                              \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "LocalizedSlotSparseEmbeddingHash        data1                         sparse_embedding1             (None, 26, 128)               \n",
      "InnerProduct                            dense                         fc1                           (None, 512)                   \n",
      "ReLU                                    fc1                           relu1                         (None, 512)                   \n",
      "InnerProduct                            relu1                         fc2                           (None, 256)                   \n",
      "ReLU                                    fc2                           relu2                         (None, 256)                   \n",
      "InnerProduct                            relu2                         fc3                           (None, 128)                   \n",
      "ReLU                                    fc3                           relu3                         (None, 128)                   \n",
      "Interaction                             relu3,sparse_embedding1       interaction1                  (None, 480)                   \n",
      "InnerProduct                            interaction1                  fc4                           (None, 1024)                  \n",
      "ReLU                                    fc4                           relu4                         (None, 1024)                  \n",
      "InnerProduct                            relu4                         fc5                           (None, 1024)                  \n",
      "ReLU                                    fc5                           relu5                         (None, 1024)                  \n",
      "InnerProduct                            relu5                         fc6                           (None, 512)                   \n",
      "ReLU                                    fc6                           relu6                         (None, 512)                   \n",
      "InnerProduct                            relu6                         fc7                           (None, 256)                   \n",
      "ReLU                                    fc7                           relu7                         (None, 256)                   \n",
      "InnerProduct                            relu7                         fc8                           (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  fc8,label                     loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[21d15h27m55s][HUGECTR][INFO]: Use non-epoch mode with number of iterations: 10000\n",
      "[21d15h27m55s][HUGECTR][INFO]: Training batchsize: 2720, evaluation batchsize: 2720\n",
      "[21d15h27m55s][HUGECTR][INFO]: Evaluation interval: 3200, snapshot interval: 3200\n",
      "[21d15h27m55s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[21d15h27m55s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[21d15h27m55s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[21d15h27m55s][HUGECTR][INFO]: Training source file: /raid/data/criteo2/test_dask/output/train/_file_list.txt\n",
      "[21d15h27m55s][HUGECTR][INFO]: Evaluation source file: /raid/data/criteo2/test_dask/output/train/_file_list.txt\n",
      "[21d15h28m10s][HUGECTR][INFO]: Iter: 1000 Time(1000 iters): 5.290034s Loss: 0.136275 lr:0.001000\n",
      "[21d15h28m60s][HUGECTR][INFO]: Iter: 2000 Time(1000 iters): 5.317352s Loss: 0.139316 lr:0.001000\n",
      "[21d15h28m11s][HUGECTR][INFO]: Iter: 3000 Time(1000 iters): 5.324982s Loss: 0.152132 lr:0.001000\n",
      "[21d15h28m13s][HUGECTR][INFO]: Evaluation, AUC: 0.570704\n",
      "[21d15h28m13s][HUGECTR][INFO]: Eval Time for 100 iters: 0.283780s\n",
      "[21d15h28m14s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[21d15h28m14s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[21d15h28m17s][HUGECTR][INFO]: Done\n",
      "[21d15h28m17s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[21d15h28m17s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[21d15h28m17s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[21d15h28m17s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[21d15h28m17s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[21d15h28m21s][HUGECTR][INFO]: Iter: 4000 Time(1000 iters): 10.223675s Loss: 0.138752 lr:0.001000\n",
      "[21d15h28m27s][HUGECTR][INFO]: Iter: 5000 Time(1000 iters): 5.327017s Loss: 0.122543 lr:0.001000\n",
      "[21d15h28m32s][HUGECTR][INFO]: Iter: 6000 Time(1000 iters): 5.324773s Loss: 0.134638 lr:0.001000\n",
      "[21d15h28m35s][HUGECTR][INFO]: Evaluation, AUC: 0.632230\n",
      "[21d15h28m35s][HUGECTR][INFO]: Eval Time for 100 iters: 0.345534s\n",
      "[21d15h28m36s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[21d15h28m36s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[21d15h28m37s][HUGECTR][INFO]: Done\n",
      "[21d15h28m37s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[21d15h28m37s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[21d15h28m37s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[21d15h28m37s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[21d15h28m37s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[21d15h28m40s][HUGECTR][INFO]: Iter: 7000 Time(1000 iters): 8.335501s Loss: 0.162951 lr:0.001000\n",
      "[21d15h28m46s][HUGECTR][INFO]: Iter: 8000 Time(1000 iters): 5.319817s Loss: 0.136564 lr:0.001000\n",
      "[21d15h28m51s][HUGECTR][INFO]: Iter: 9000 Time(1000 iters): 5.332460s Loss: 0.148672 lr:0.001000\n",
      "[21d15h28m55s][HUGECTR][INFO]: Evaluation, AUC: 0.646564\n",
      "[21d15h28m55s][HUGECTR][INFO]: Eval Time for 100 iters: 0.347289s\n",
      "[21d15h28m57s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[21d15h28m57s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[21d15h28m58s][HUGECTR][INFO]: Done\n",
      "[21d15h28m58s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[21d15h28m58s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[21d15h28m58s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[21d15h28m58s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[21d15h28m58s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n"
     ]
    }
   ],
   "source": [
    "!python model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained the model and created snapshots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
