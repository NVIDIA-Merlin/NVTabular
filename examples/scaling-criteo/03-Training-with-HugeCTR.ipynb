{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ef79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03acfd49",
   "metadata": {},
   "source": [
    "# Scaling Criteo: Training with HugeCTR\n",
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b1895e",
   "metadata": {},
   "source": [
    "HugeCTR is an open-source framework to accelerate the training of CTR estimation models on NVIDIA GPUs. It is written in CUDA C++ and highly exploits GPU-accelerated libraries such as cuBLAS, cuDNN, and NCCL.\n",
    "\n",
    "HugeCTR offers multiple advantages to train deep learning recommender systems:\n",
    "\n",
    "Speed: HugeCTR is a highly efficient framework written C++. We experienced upto 10x speed up. HugeCTR on a NVIDIA DGX A100 system proved to be the fastest commercially available solution for training the architecture Deep Learning Recommender Model (DLRM) developed by Facebook.\n",
    "Scale: HugeCTR supports model parallel scaling. It distributes the large embedding tables over multiple GPUs or multiple nodes.\n",
    "Easy-to-use: Easy-to-use Python API similar to Keras. Examples for popular deep learning recommender systems architectures (Wide&Deep, DLRM, DCN, DeepFM) are available.\n",
    "HugeCTR is able to train recommender system models with larger-than-memory embedding tables by leveraging a parameter server.\n",
    "\n",
    "You can find more information about HugeCTR [here](https://github.com/NVIDIA/HugeCTR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28524714",
   "metadata": {},
   "source": [
    "### Learning Objectives \n",
    "In this notebook, we learn how to to use HugeCTR for training recommender system models\n",
    "\n",
    "* Use HugeCTR to define a recommender system model\n",
    "* Train Facebook's [Deep Learning Recommendation Model](https://arxiv.org/pdf/1906.00091.pdf) with HugeCTR\n",
    "* Train popular [Deep & Cross Network](https://arxiv.org/pdf/1708.05123.pdf) with HugeCTR\n",
    "* Train Google's [Wide & Deep Network](https://arxiv.org/pdf/1606.07792.pdf) with HugeCTR\n",
    "* Train [DeepFM: A Factorization-Machine based Neural Network ](https://arxiv.org/pdf/1703.04247.pdf) with HugeCTR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a533ab",
   "metadata": {},
   "source": [
    "## Training with HugeCTR\n",
    "As HugeCTR optimizes the training in CUDA++, we need to define the training pipeline and model architecture and execute it via the commandline. We will use the Python API, which is similar to Keras models.\n",
    "\n",
    "If you are not familiar with HugeCTR's Python API and parameters, you can read more in its GitHub repository:\n",
    "* [HugeCTR User Guide](https://github.com/NVIDIA/HugeCTR/blob/master/docs/hugectr_user_guide.md)\n",
    "* [HugeCTR Python API](https://github.com/NVIDIA/HugeCTR/blob/master/docs/python_interface.md)\n",
    "* [HugeCTR example architectures](https://github.com/NVIDIA/HugeCTR/tree/master/samples)\n",
    "\n",
    "#### NOTE: \n",
    "* In this example with Criteo Dataset, only DLRM architecture will be  used for further Inference purposes. Rest of the architectures are explored to give an insight on the AUC parameter with [SGD](https://arxiv.org/pdf/2003.10409.pdf) as an optimizer. Feel free to witness the difference with other optimizers as well like [ADAM](https://arxiv.org/pdf/1412.6980.pdf). \n",
    "\n",
    "* If you're training with Docker Container from NGC, make sure you use the below command for training purpose to avoid warnings and exceptions. \n",
    "```\n",
    "docker run --runtime=nvidia --rm -it --cap-add SYS_NICE -v /your/host/dir:/your/container/dir -w /your/container/dir -it -u $(id -u):$(id -g) nvcr.io/nvidia/merlin/merlin-training:0.6\n",
    "```\n",
    "An example could be:\n",
    "```\n",
    "docker run --gpus=all --rm -it --cap-add SYS_NICE -v /home/nvidia/user:/workspace/user -p 8000:8888 d9cb4e6936ea\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445222f8",
   "metadata": {},
   "source": [
    "### 1. DLRM Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6933e",
   "metadata": {},
   "source": [
    "We will write the code to a ./model.py file and execute it. It will create snapshot, which we will use for inference in the next notebook. The below cell executes the DLRM architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b160b766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './model.py'\n",
    "import hugectr\n",
    "from mpi4py import MPI  # noqa\n",
    "\n",
    "# HugeCTR\n",
    "solver = hugectr.CreateSolver(\n",
    "    vvgpu=[[0]],\n",
    "    max_eval_batches=100,\n",
    "    batchsize_eval=2720,\n",
    "    batchsize=2720,\n",
    "    i64_input_key=True,\n",
    "    use_mixed_precision=False,\n",
    "    repeat_dataset=True,\n",
    ")\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type=hugectr.Optimizer_t.SGD)\n",
    "\n",
    "\n",
    "reader = hugectr.DataReaderParams(\n",
    "    data_reader_type=hugectr.DataReaderType_t.Parquet,\n",
    "    source=[\"/raid/data/criteo/test_dask/output/train/_file_list.txt\"],\n",
    "    eval_source=\"/raid/data/criteo/test_dask/output/valid/_file_list.txt\",\n",
    "    check_type=hugectr.Check_t.Non,\n",
    "    slot_size_array=[\n",
    "        10000000,\n",
    "        10000000,\n",
    "        3014529,\n",
    "        400781,\n",
    "        11,\n",
    "        2209,\n",
    "        11869,\n",
    "        148,\n",
    "        4,\n",
    "        977,\n",
    "        15,\n",
    "        38713,\n",
    "        10000000,\n",
    "        10000000,\n",
    "        10000000,\n",
    "        584616,\n",
    "        12883,\n",
    "        109,\n",
    "        37,\n",
    "        17177,\n",
    "        7425,\n",
    "        20266,\n",
    "        4,\n",
    "        7085,\n",
    "        1535,\n",
    "        64,\n",
    "    ],\n",
    ")\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.add(\n",
    "    hugectr.Input(\n",
    "        label_dim=1,\n",
    "        label_name=\"label\",\n",
    "        dense_dim=13,\n",
    "        dense_name=\"dense\",\n",
    "        data_reader_sparse_param_array=[hugectr.DataReaderSparseParam(\"data1\", 1, False, 26)],\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.SparseEmbedding(\n",
    "        embedding_type=hugectr.Embedding_t.LocalizedSlotSparseEmbeddingHash,\n",
    "        workspace_size_per_gpu_in_mb=6000,\n",
    "        embedding_vec_size=128,\n",
    "        combiner=\"sum\",\n",
    "        sparse_embedding_name=\"sparse_embedding1\",\n",
    "        bottom_name=\"data1\",\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"dense\"],\n",
    "        top_names=[\"fc1\"],\n",
    "        num_output=512,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc1\"], top_names=[\"relu1\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu1\"],\n",
    "        top_names=[\"fc2\"],\n",
    "        num_output=256,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc2\"], top_names=[\"relu2\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu2\"],\n",
    "        top_names=[\"fc3\"],\n",
    "        num_output=128,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc3\"], top_names=[\"relu3\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.Interaction,\n",
    "        bottom_names=[\"relu3\", \"sparse_embedding1\"],\n",
    "        top_names=[\"interaction1\"],\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"interaction1\"],\n",
    "        top_names=[\"fc4\"],\n",
    "        num_output=1024,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc4\"], top_names=[\"relu4\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu4\"],\n",
    "        top_names=[\"fc5\"],\n",
    "        num_output=1024,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc5\"], top_names=[\"relu5\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu5\"],\n",
    "        top_names=[\"fc6\"],\n",
    "        num_output=512,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc6\"], top_names=[\"relu6\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu6\"],\n",
    "        top_names=[\"fc7\"],\n",
    "        num_output=256,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc7\"], top_names=[\"relu7\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu7\"],\n",
    "        top_names=[\"fc8\"],\n",
    "        num_output=1,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "        bottom_names=[\"fc8\", \"label\"],\n",
    "        top_names=[\"loss\"],\n",
    "    )\n",
    ")\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(max_iter=10000, eval_interval=3200, display=1000, snapshot=3200)\n",
    "model.graph_to_json(graph_config_file=\"./criteo_hugectr/1/criteo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a69a57cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[28d10h30m09s][HUGECTR][INFO]: Global seed is 2223000078\n",
      "[28d10h30m09s][HUGECTR][INFO]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[28d10h30m10s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "[28d10h30m10s][HUGECTR][INFO]: Start all2all warmup\n",
      "[28d10h30m10s][HUGECTR][INFO]: End all2all warmup\n",
      "[28d10h30m10s][HUGECTR][INFO]: Using All-reduce algorithm OneShot\n",
      "Device 0: Quadro RTX 8000\n",
      "[28d10h30m10s][HUGECTR][INFO]: num of DataReader workers: 1\n",
      "[28d10h30m10s][HUGECTR][INFO]: Vocabulary size: 54120457\n",
      "[28d10h30m10s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=12288000\n",
      "[28d10h30m10s][HUGECTR][INFO]: All2All Warmup Start\n",
      "[28d10h30m10s][HUGECTR][INFO]: All2All Warmup End\n",
      "===================================================Model Compile===================================================\n",
      "[28d10h30m26s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[28d10h30m26s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[28d10h30m26s][HUGECTR][INFO]: Starting AUC NCCL warm-up\n",
      "[28d10h30m26s][HUGECTR][INFO]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1                         \n",
      "(None, 1)                               (None, 13)                              \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "LocalizedSlotSparseEmbeddingHash        data1                         sparse_embedding1             (None, 26, 128)               \n",
      "InnerProduct                            dense                         fc1                           (None, 512)                   \n",
      "ReLU                                    fc1                           relu1                         (None, 512)                   \n",
      "InnerProduct                            relu1                         fc2                           (None, 256)                   \n",
      "ReLU                                    fc2                           relu2                         (None, 256)                   \n",
      "InnerProduct                            relu2                         fc3                           (None, 128)                   \n",
      "ReLU                                    fc3                           relu3                         (None, 128)                   \n",
      "Interaction                             relu3,sparse_embedding1       interaction1                  (None, 480)                   \n",
      "InnerProduct                            interaction1                  fc4                           (None, 1024)                  \n",
      "ReLU                                    fc4                           relu4                         (None, 1024)                  \n",
      "InnerProduct                            relu4                         fc5                           (None, 1024)                  \n",
      "ReLU                                    fc5                           relu5                         (None, 1024)                  \n",
      "InnerProduct                            relu5                         fc6                           (None, 512)                   \n",
      "ReLU                                    fc6                           relu6                         (None, 512)                   \n",
      "InnerProduct                            relu6                         fc7                           (None, 256)                   \n",
      "ReLU                                    fc7                           relu7                         (None, 256)                   \n",
      "InnerProduct                            relu7                         fc8                           (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  fc8,label                     loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[28d10h30m26s][HUGECTR][INFO]: Use non-epoch mode with number of iterations: 10000\n",
      "[28d10h30m26s][HUGECTR][INFO]: Training batchsize: 2720, evaluation batchsize: 2720\n",
      "[28d10h30m26s][HUGECTR][INFO]: Evaluation interval: 3200, snapshot interval: 3200\n",
      "[28d10h30m26s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[28d10h30m26s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[28d10h30m26s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[28d10h30m26s][HUGECTR][INFO]: Training source file: /workspace/aryan/test_dask/output/train/_file_list.txt\n",
      "[28d10h30m26s][HUGECTR][INFO]: Evaluation source file: /workspace/aryan/test_dask/output/valid/_file_list.txt\n",
      "[28d10h30m32s][HUGECTR][INFO]: Iter: 1000 Time(1000 iters): 6.034442s Loss: 0.172563 lr:0.001000\n",
      "[28d10h30m39s][HUGECTR][INFO]: Iter: 2000 Time(1000 iters): 6.271048s Loss: 0.147328 lr:0.001000\n",
      "[28d10h30m45s][HUGECTR][INFO]: Iter: 3000 Time(1000 iters): 6.543730s Loss: 0.160528 lr:0.001000\n",
      "[28d10h30m47s][HUGECTR][INFO]: Evaluation, AUC: 0.563052\n",
      "[28d10h30m47s][HUGECTR][INFO]: Eval Time for 100 iters: 0.227760s\n",
      "[28d10h30m47s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[28d10h30m47s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[28d10h30m48s][HUGECTR][INFO]: Done\n",
      "[28d10h30m49s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[28d10h30m49s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[28d10h30m49s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[28d10h30m49s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[28d10h30m49s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[28d10h30m53s][HUGECTR][INFO]: Iter: 4000 Time(1000 iters): 8.227036s Loss: 0.183154 lr:0.001000\n",
      "[28d10h31m00s][HUGECTR][INFO]: Iter: 5000 Time(1000 iters): 6.299841s Loss: 0.141766 lr:0.001000\n",
      "[28d10h31m60s][HUGECTR][INFO]: Iter: 6000 Time(1000 iters): 6.413942s Loss: 0.144913 lr:0.001000\n",
      "[28d10h31m90s][HUGECTR][INFO]: Evaluation, AUC: 0.613813\n",
      "[28d10h31m90s][HUGECTR][INFO]: Eval Time for 100 iters: 0.260254s\n",
      "[28d10h31m10s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[28d10h31m10s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[28d10h31m11s][HUGECTR][INFO]: Done\n",
      "[28d10h31m12s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[28d10h31m12s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[28d10h31m12s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[28d10h31m12s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[28d10h31m12s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[28d10h31m16s][HUGECTR][INFO]: Iter: 7000 Time(1000 iters): 9.366976s Loss: 0.164099 lr:0.001000\n",
      "[28d10h31m22s][HUGECTR][INFO]: Iter: 8000 Time(1000 iters): 6.489785s Loss: 0.153941 lr:0.001000\n",
      "[28d10h31m29s][HUGECTR][INFO]: Iter: 9000 Time(1000 iters): 6.547301s Loss: 0.144348 lr:0.001000\n",
      "[28d10h31m33s][HUGECTR][INFO]: Evaluation, AUC: 0.634325\n",
      "[28d10h31m33s][HUGECTR][INFO]: Eval Time for 100 iters: 0.243787s\n",
      "[28d10h31m34s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[28d10h31m34s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[28d10h31m35s][HUGECTR][INFO]: Done\n",
      "[28d10h31m36s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[28d10h31m36s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[28d10h31m36s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[28d10h31m36s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[28d10h31m36s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "Finish 10000 iterations with batchsize: 2720 in 72.22s\n",
      "[28d10h31m39s][HUGECTR][INFO]: Save the model graph to ./criteo_hugectr/1/criteo.json, successful\n"
     ]
    }
   ],
   "source": [
    "!python model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec003051",
   "metadata": {},
   "source": [
    "### 2. Deep and Cross Network\n",
    "\n",
    "We will write the code to a ./model_dcn.py file and execute it.  The below cell executes the Deep & Cross architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ffd782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model_dcn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './model_dcn.py'\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "solver = hugectr.CreateSolver(\n",
    "    vvgpu=[[0]],\n",
    "    max_eval_batches=100,\n",
    "    batchsize_eval=2720,\n",
    "    batchsize=2720,\n",
    "    i64_input_key=True,\n",
    "    use_mixed_precision=False,\n",
    "    repeat_dataset=True,\n",
    ")\n",
    "\n",
    "\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type=hugectr.Optimizer_t.SGD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reader = hugectr.DataReaderParams(\n",
    "    data_reader_type=hugectr.DataReaderType_t.Parquet,\n",
    "    source=[\"/raid/data/criteo/test_dask/output/train/_file_list.txt\"],\n",
    "    eval_source=\"/raid/data/criteo/test_dask/output/valid/_file_list.txt\",\n",
    "    check_type=hugectr.Check_t.Sum,\n",
    "\n",
    ")\n",
    "\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"data1\", 2, False, 26)]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.LocalizedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 89,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"data1\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"], top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Slice,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"slice11\", \"slice12\"],\n",
    "                            ranges=[(0,429),(0,429)]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.MultiCross,\n",
    "                            bottom_names = [\"slice11\"],\n",
    "                            top_names = [\"multicross1\"],\n",
    "                            num_layers=6))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"slice12\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"dropout2\", \"multicross1\"],\n",
    "                            top_names = [\"concat2\"]))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"fc3\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "\n",
    "model.compile()\n",
    "model.summary()\n",
    "\n",
    "model.fit(max_iter=1400, eval_interval=200, display=200, snapshot=1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "befd2e08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[28d10h31m40s][HUGECTR][INFO]: Global seed is 1036292643\n",
      "[28d10h31m40s][HUGECTR][INFO]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[28d10h31m42s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "[28d10h31m42s][HUGECTR][INFO]: Start all2all warmup\n",
      "[28d10h31m42s][HUGECTR][INFO]: End all2all warmup\n",
      "[28d10h31m42s][HUGECTR][INFO]: Using All-reduce algorithm OneShot\n",
      "Device 0: Quadro RTX 8000\n",
      "[28d10h31m42s][HUGECTR][INFO]: num of DataReader workers: 1\n",
      "[28d10h31m42s][HUGECTR][INFO]: Vocabulary size: 0\n",
      "[28d10h31m42s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=1458176\n",
      "[28d10h31m42s][HUGECTR][INFO]: All2All Warmup Start\n",
      "[28d10h31m42s][HUGECTR][INFO]: All2All Warmup End\n",
      "===================================================Model Compile===================================================\n",
      "[28d10h31m53s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[28d10h31m53s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[28d10h31m53s][HUGECTR][INFO]: Starting AUC NCCL warm-up\n",
      "[28d10h31m53s][HUGECTR][INFO]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1                         \n",
      "(None, 1)                               (None, 13)                              \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "LocalizedSlotSparseEmbeddingHash        data1                         sparse_embedding1             (None, 26, 16)                \n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "Concat                                  reshape1,dense                concat1                       (None, 429)                   \n",
      "Slice                                   concat1                       slice11,slice12                                             \n",
      "MultiCross                              slice11                       multicross1                   (None, 429)                   \n",
      "InnerProduct                            slice12                       fc1                           (None, 1024)                  \n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "Concat                                  dropout2,multicross1          concat2                       (None, 1453)                  \n",
      "InnerProduct                            concat2                       fc3                           (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  fc3,label                     loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[28d10h31m53s][HUGECTR][INFO]: Use non-epoch mode with number of iterations: 1400\n",
      "[28d10h31m53s][HUGECTR][INFO]: Training batchsize: 2720, evaluation batchsize: 2720\n",
      "[28d10h31m53s][HUGECTR][INFO]: Evaluation interval: 200, snapshot interval: 1000000\n",
      "[28d10h31m53s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[28d10h31m53s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[28d10h31m53s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[28d10h31m53s][HUGECTR][INFO]: Training source file: /workspace/aryan/test_dask/output/train/_file_list.txt\n",
      "[28d10h31m53s][HUGECTR][INFO]: Evaluation source file: /workspace/aryan/test_dask/output/valid/_file_list.txt\n",
      "[28d10h31m54s][HUGECTR][INFO]: Iter: 200 Time(200 iters): 1.071357s Loss: 0.369730 lr:0.001000\n",
      "[28d10h31m54s][HUGECTR][INFO]: Evaluation, AUC: 0.512687\n",
      "[28d10h31m54s][HUGECTR][INFO]: Eval Time for 100 iters: 0.174378s\n",
      "[28d10h31m55s][HUGECTR][INFO]: Iter: 400 Time(200 iters): 1.192470s Loss: 0.277967 lr:0.001000\n",
      "[28d10h31m55s][HUGECTR][INFO]: Evaluation, AUC: 0.522005\n",
      "[28d10h31m55s][HUGECTR][INFO]: Eval Time for 100 iters: 0.191102s\n",
      "[28d10h31m56s][HUGECTR][INFO]: Iter: 600 Time(200 iters): 1.204503s Loss: 0.228461 lr:0.001000\n",
      "[28d10h31m57s][HUGECTR][INFO]: Evaluation, AUC: 0.518284\n",
      "[28d10h31m57s][HUGECTR][INFO]: Eval Time for 100 iters: 0.173636s\n",
      "[28d10h31m58s][HUGECTR][INFO]: Iter: 800 Time(200 iters): 1.184951s Loss: 0.204394 lr:0.001000\n",
      "[28d10h31m58s][HUGECTR][INFO]: Evaluation, AUC: 0.523243\n",
      "[28d10h31m58s][HUGECTR][INFO]: Eval Time for 100 iters: 0.191345s\n",
      "[28d10h31m59s][HUGECTR][INFO]: Iter: 1000 Time(200 iters): 1.205533s Loss: 0.194926 lr:0.001000\n",
      "[28d10h31m59s][HUGECTR][INFO]: Evaluation, AUC: 0.530464\n",
      "[28d10h31m59s][HUGECTR][INFO]: Eval Time for 100 iters: 0.173781s\n",
      "[28d10h32m00s][HUGECTR][INFO]: Iter: 1200 Time(200 iters): 1.191101s Loss: 0.180838 lr:0.001000\n",
      "[28d10h32m00s][HUGECTR][INFO]: Evaluation, AUC: 0.535546\n",
      "[28d10h32m00s][HUGECTR][INFO]: Eval Time for 100 iters: 0.189153s\n",
      "Finish 1400 iterations with batchsize: 2720 in 8.29s\n"
     ]
    }
   ],
   "source": [
    "!python model_dcn.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b67c8",
   "metadata": {},
   "source": [
    "### 3. Wide & Deep Network\n",
    "We will write the code to a ./model_wdl.py file and execute it. The below cell executes the Wide & Deep architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "399ebbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model_wdl.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './model_wdl.py'\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "solver = hugectr.CreateSolver(\n",
    "    vvgpu=[[0]],\n",
    "    max_eval_batches=100,\n",
    "    batchsize_eval=2720,\n",
    "    batchsize=2720,\n",
    "    i64_input_key=True,\n",
    "    use_mixed_precision=False,\n",
    "    repeat_dataset=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# optimizer = hugectr.CreateOptimizer(optimizer_type=hugectr.Optimizer_t.SGD)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam,\n",
    "                                    update_type = hugectr.Update_t.Global,\n",
    "                                    beta1 = 0.9,\n",
    "                                    beta2 = 0.999,\n",
    "                                    epsilon = 0.0000001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reader = hugectr.DataReaderParams(\n",
    "    data_reader_type=hugectr.DataReaderType_t.Parquet,\n",
    "    source=[\"/raid/data/criteo/test_dask/output/train/_file_list.txt\"],\n",
    "    eval_source=\"/raid/data/criteo/test_dask/output/valid/_file_list.txt\",\n",
    "    check_type=hugectr.Check_t.Sum,\n",
    ")\n",
    "\n",
    "\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"wide_data\", 30, True, 1),\n",
    "                        hugectr.DataReaderSparseParam(\"deep_data\", 2, False, 26)]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.LocalizedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 23,\n",
    "                            embedding_vec_size = 1,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding2\",\n",
    "                            bottom_name = \"wide_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.LocalizedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 358,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"deep_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding2\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"],\n",
    "                            top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Add,\n",
    "                            bottom_names = [\"fc3\", \"reshape2\"],\n",
    "                            top_names = [\"add1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"add1\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(max_iter=10000, eval_interval=3200, display=1000, snapshot=3200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8e61e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[28d10h32m03s][HUGECTR][INFO]: Global seed is 4063477066\n",
      "[28d10h32m03s][HUGECTR][INFO]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[28d10h32m04s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "[28d10h32m04s][HUGECTR][INFO]: Start all2all warmup\n",
      "[28d10h32m04s][HUGECTR][INFO]: End all2all warmup\n",
      "[28d10h32m04s][HUGECTR][INFO]: Using All-reduce algorithm OneShot\n",
      "Device 0: Quadro RTX 8000\n",
      "[28d10h32m04s][HUGECTR][INFO]: num of DataReader workers: 1\n",
      "[28d10h32m04s][HUGECTR][INFO]: Vocabulary size: 0\n",
      "[28d10h32m04s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=6029312\n",
      "[28d10h32m04s][HUGECTR][INFO]: All2All Warmup Start\n",
      "[28d10h32m04s][HUGECTR][INFO]: All2All Warmup End\n",
      "[28d10h32m04s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=5865472\n",
      "[28d10h32m04s][HUGECTR][INFO]: All2All Warmup Start\n",
      "[28d10h32m04s][HUGECTR][INFO]: All2All Warmup End\n",
      "===================================================Model Compile===================================================\n",
      "[28d10h32m15s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[28d10h32m15s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[28d10h32m15s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[28d10h32m15s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[28d10h32m15s][HUGECTR][INFO]: Starting AUC NCCL warm-up\n",
      "[28d10h32m15s][HUGECTR][INFO]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(None, 1)                               (None, 13)                              \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "LocalizedSlotSparseEmbeddingHash        wide_data                     sparse_embedding2             (None, 1, 1)                  \n",
      "LocalizedSlotSparseEmbeddingHash        deep_data                     sparse_embedding1             (None, 26, 16)                \n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "Reshape                                 sparse_embedding2             reshape2                      (None, 1)                     \n",
      "Concat                                  reshape1,dense                concat1                       (None, 429)                   \n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "InnerProduct                            dropout2                      fc3                           (None, 1)                     \n",
      "Add                                     fc3,reshape2                  add1                          (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  add1,label                    loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[28d10h32m15s][HUGECTR][INFO]: Use non-epoch mode with number of iterations: 10000\n",
      "[28d10h32m15s][HUGECTR][INFO]: Training batchsize: 2720, evaluation batchsize: 2720\n",
      "[28d10h32m15s][HUGECTR][INFO]: Evaluation interval: 3200, snapshot interval: 3200\n",
      "[28d10h32m15s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[28d10h32m15s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[28d10h32m15s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[28d10h32m15s][HUGECTR][INFO]: Training source file: /workspace/aryan/test_dask/output/train/_file_list.txt\n",
      "[28d10h32m15s][HUGECTR][INFO]: Evaluation source file: /workspace/aryan/test_dask/output/valid/_file_list.txt\n",
      "[28d10h32m27s][HUGECTR][INFO]: Iter: 1000 Time(1000 iters): 12.313599s Loss: 0.158861 lr:0.001000\n",
      "[28d10h32m39s][HUGECTR][INFO]: Iter: 2000 Time(1000 iters): 11.993367s Loss: 0.129315 lr:0.001000\n",
      "[28d10h32m51s][HUGECTR][INFO]: Iter: 3000 Time(1000 iters): 11.997043s Loss: 0.145514 lr:0.001000\n",
      "[28d10h32m54s][HUGECTR][INFO]: Evaluation, AUC: 0.764010\n",
      "[28d10h32m54s][HUGECTR][INFO]: Eval Time for 100 iters: 0.121214s\n",
      "[28d10h32m54s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[28d10h32m54s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[28d10h32m54s][HUGECTR][INFO]: Done\n",
      "[28d10h32m54s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[28d10h32m54s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[28d10h32m54s][HUGECTR][INFO]: Done\n",
      "[28d10h32m54s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[28d10h32m54s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[28d10h32m54s][HUGECTR][INFO]: Done\n",
      "[28d10h32m54s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[28d10h32m54s][HUGECTR][INFO]: Done\n",
      "[28d10h32m54s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[28d10h32m55s][HUGECTR][INFO]: Done\n",
      "[28d10h32m55s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[28d10h32m55s][HUGECTR][INFO]: Done\n",
      "[28d10h32m57s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[28d10h32m57s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[28d10h32m57s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[28d10h32m57s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[28d10h33m60s][HUGECTR][INFO]: Iter: 4000 Time(1000 iters): 14.807089s Loss: 0.156088 lr:0.001000\n",
      "[28d10h33m18s][HUGECTR][INFO]: Iter: 5000 Time(1000 iters): 11.958671s Loss: 0.126493 lr:0.001000\n",
      "[28d10h33m30s][HUGECTR][INFO]: Iter: 6000 Time(1000 iters): 11.977989s Loss: 0.128313 lr:0.001000\n",
      "[28d10h33m35s][HUGECTR][INFO]: Evaluation, AUC: 0.756964\n",
      "[28d10h33m35s][HUGECTR][INFO]: Eval Time for 100 iters: 0.139813s\n",
      "[28d10h33m35s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[28d10h33m35s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[28d10h33m35s][HUGECTR][INFO]: Done\n",
      "[28d10h33m35s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[28d10h33m35s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[28d10h33m35s][HUGECTR][INFO]: Done\n",
      "[28d10h33m35s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[28d10h33m35s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[28d10h33m35s][HUGECTR][INFO]: Done\n",
      "[28d10h33m35s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[28d10h33m35s][HUGECTR][INFO]: Done\n",
      "[28d10h33m36s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[28d10h33m36s][HUGECTR][INFO]: Done\n",
      "[28d10h33m36s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[28d10h33m36s][HUGECTR][INFO]: Done\n",
      "[28d10h33m38s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[28d10h33m38s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[28d10h33m38s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[28d10h33m38s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[28d10h33m45s][HUGECTR][INFO]: Iter: 7000 Time(1000 iters): 15.134684s Loss: 0.143932 lr:0.001000\n",
      "[28d10h33m57s][HUGECTR][INFO]: Iter: 8000 Time(1000 iters): 11.987772s Loss: 0.133735 lr:0.001000\n",
      "[28d10h34m90s][HUGECTR][INFO]: Iter: 9000 Time(1000 iters): 12.002207s Loss: 0.132955 lr:0.001000\n",
      "[28d10h34m17s][HUGECTR][INFO]: Evaluation, AUC: 0.762748\n",
      "[28d10h34m17s][HUGECTR][INFO]: Eval Time for 100 iters: 0.122008s\n",
      "[28d10h34m17s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[28d10h34m17s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[28d10h34m17s][HUGECTR][INFO]: Done\n",
      "[28d10h34m17s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[28d10h34m17s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[28d10h34m17s][HUGECTR][INFO]: Done\n",
      "[28d10h34m17s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[28d10h34m17s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[28d10h34m17s][HUGECTR][INFO]: Done\n",
      "[28d10h34m17s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[28d10h34m17s][HUGECTR][INFO]: Done\n",
      "[28d10h34m17s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[28d10h34m18s][HUGECTR][INFO]: Done\n",
      "[28d10h34m18s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[28d10h34m18s][HUGECTR][INFO]: Done\n",
      "[28d10h34m20s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[28d10h34m20s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[28d10h34m20s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[28d10h34m20s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "Finish 10000 iterations with batchsize: 2720 in 129.49s\n"
     ]
    }
   ],
   "source": [
    "!python model_wdl.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b591af74",
   "metadata": {},
   "source": [
    "#### Notice the difference in the AUC score just by fine-tuning the Optimizer. The highest AUC score achieved is: 0.764 with Wide & Deep Network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf79afa",
   "metadata": {},
   "source": [
    "### 4. DeepFM: Factorization Machine based Neural Network\n",
    "We will write the code to a ./model_deepfm.py file and execute it. The below cell executes the DeepFM architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af63d4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model_deepfm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './model_deepfm.py'\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "\n",
    "solver = hugectr.CreateSolver(\n",
    "    vvgpu=[[0]],\n",
    "    max_eval_batches=100,\n",
    "    batchsize_eval=2720,\n",
    "    batchsize=2720,\n",
    "    i64_input_key=True,\n",
    "    use_mixed_precision=False,\n",
    "    repeat_dataset=True,\n",
    ")\n",
    "\n",
    "\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type=hugectr.Optimizer_t.SGD)\n",
    "\n",
    "\n",
    "reader = hugectr.DataReaderParams(\n",
    "    data_reader_type=hugectr.DataReaderType_t.Parquet,\n",
    "    source=[\"/raid/data/criteo/test_dask/output/train/_file_list.txt\"],\n",
    "    eval_source=\"/raid/data/criteo/test_dask/output/valid/_file_list.txt\",\n",
    "    check_type=hugectr.Check_t.Sum,\n",
    ")\n",
    "\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"data1\", 2, False, 26)]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 61,\n",
    "                            embedding_vec_size = 11,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"data1\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=11))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Slice,\n",
    "                            bottom_names = [\"reshape1\"],\n",
    "                            top_names = [\"slice11\", \"slice12\"],\n",
    "                            ranges=[(0,10),(10,11)]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"slice11\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=260))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"slice12\"],\n",
    "                            top_names = [\"reshape3\"],\n",
    "                            leading_dim=26))                            \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Slice,\n",
    "                            bottom_names = [\"dense\"],\n",
    "                            top_names = [\"slice21\", \"slice22\"],\n",
    "                            ranges=[(0,13),(0,13)]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.WeightMultiply,\n",
    "                            bottom_names = [\"slice21\"],\n",
    "                            top_names = [\"weight_multiply1\"],\n",
    "                            weight_dims= [13,10]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.WeightMultiply,\n",
    "                            bottom_names = [\"slice22\"],\n",
    "                            top_names = [\"weight_multiply2\"],\n",
    "                            weight_dims= [13,1]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape2\",\"weight_multiply1\"],\n",
    "                            top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Slice,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"slice31\", \"slice32\"],\n",
    "                            ranges=[(0,390),(0,390)]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"slice31\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=400))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=400))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=400))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc3\"],\n",
    "                            top_names = [\"relu3\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu3\"],\n",
    "                            top_names = [\"dropout3\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout3\"],\n",
    "                            top_names = [\"fc4\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.FmOrder2,\n",
    "                            bottom_names = [\"slice32\"],\n",
    "                            top_names = [\"fmorder2\"],\n",
    "                            out_dim=10))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReduceSum,\n",
    "                            bottom_names = [\"fmorder2\"],\n",
    "                            top_names = [\"reducesum1\"],\n",
    "                            axis=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape3\",\"weight_multiply2\"],\n",
    "                            top_names = [\"concat2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReduceSum,\n",
    "                            bottom_names = [\"concat2\"],\n",
    "                            top_names = [\"reducesum2\"],\n",
    "                            axis=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Add,\n",
    "                            bottom_names = [\"fc4\", \"reducesum1\", \"reducesum2\"],\n",
    "                            top_names = [\"add\"]))                                                                                                        \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"add\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(max_iter = 1200, display = 200, eval_interval = 600, snapshot = 1000000, snapshot_prefix = \"deepfm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f275b670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[28d10h34m26s][HUGECTR][INFO]: Global seed is 554547979\n",
      "[28d10h34m26s][HUGECTR][INFO]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[28d10h34m27s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "[28d10h34m27s][HUGECTR][INFO]: Start all2all warmup\n",
      "[28d10h34m27s][HUGECTR][INFO]: End all2all warmup\n",
      "[28d10h34m27s][HUGECTR][INFO]: Using All-reduce algorithm OneShot\n",
      "Device 0: Quadro RTX 8000\n",
      "[28d10h34m27s][HUGECTR][INFO]: num of DataReader workers: 1\n",
      "[28d10h34m27s][HUGECTR][INFO]: Vocabulary size: 0\n",
      "[28d10h34m27s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=1453707\n",
      "===================================================Model Compile===================================================\n",
      "[28d10h34m33s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[28d10h34m33s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[28d10h34m33s][HUGECTR][INFO]: Starting AUC NCCL warm-up\n",
      "[28d10h34m33s][HUGECTR][INFO]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1                         \n",
      "(None, 1)                               (None, 13)                              \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      data1                         sparse_embedding1             (None, 26, 11)                \n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 11)                    \n",
      "Slice                                   reshape1                      slice11,slice12                                             \n",
      "Reshape                                 slice11                       reshape2                      (None, 260)                   \n",
      "Reshape                                 slice12                       reshape3                      (None, 26)                    \n",
      "Slice                                   dense                         slice21,slice22                                             \n",
      "WeightMultiply                          slice21                       weight_multiply1              (None, 130)                   \n",
      "WeightMultiply                          slice22                       weight_multiply2              (None, 13)                    \n",
      "Concat                                  reshape2,weight_multiply1     concat1                       (None, 390)                   \n",
      "Slice                                   concat1                       slice31,slice32                                             \n",
      "InnerProduct                            slice31                       fc1                           (None, 400)                   \n",
      "ReLU                                    fc1                           relu1                         (None, 400)                   \n",
      "Dropout                                 relu1                         dropout1                      (None, 400)                   \n",
      "InnerProduct                            dropout1                      fc2                           (None, 400)                   \n",
      "ReLU                                    fc2                           relu2                         (None, 400)                   \n",
      "Dropout                                 relu2                         dropout2                      (None, 400)                   \n",
      "InnerProduct                            dropout2                      fc3                           (None, 400)                   \n",
      "ReLU                                    fc3                           relu3                         (None, 400)                   \n",
      "Dropout                                 relu3                         dropout3                      (None, 400)                   \n",
      "InnerProduct                            dropout3                      fc4                           (None, 1)                     \n",
      "FmOrder2                                slice32                       fmorder2                      (None, 10)                    \n",
      "ReduceSum                               fmorder2                      reducesum1                    (None, 1)                     \n",
      "Concat                                  reshape3,weight_multiply2     concat2                       (None, 39)                    \n",
      "ReduceSum                               concat2                       reducesum2                    (None, 1)                     \n",
      "Add                                     fc4,reducesum1,reducesum2     add                           (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  add,label                     loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[28d10h34m33s][HUGECTR][INFO]: Use non-epoch mode with number of iterations: 1200\n",
      "[28d10h34m33s][HUGECTR][INFO]: Training batchsize: 2720, evaluation batchsize: 2720\n",
      "[28d10h34m33s][HUGECTR][INFO]: Evaluation interval: 600, snapshot interval: 1000000\n",
      "[28d10h34m33s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[28d10h34m33s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[28d10h34m33s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[28d10h34m33s][HUGECTR][INFO]: Training source file: /workspace/aryan/test_dask/output/train/_file_list.txt\n",
      "[28d10h34m33s][HUGECTR][INFO]: Evaluation source file: /workspace/aryan/test_dask/output/valid/_file_list.txt\n",
      "[28d10h34m33s][HUGECTR][INFO]: Iter: 200 Time(200 iters): 0.604364s Loss: 0.201735 lr:0.001000\n",
      "[28d10h34m34s][HUGECTR][INFO]: Iter: 400 Time(200 iters): 0.533356s Loss: 0.189705 lr:0.001000\n",
      "[28d10h34m34s][HUGECTR][INFO]: Iter: 600 Time(200 iters): 0.541807s Loss: 0.182874 lr:0.001000\n",
      "[28d10h34m34s][HUGECTR][INFO]: Evaluation, AUC: 0.509014\n",
      "[28d10h34m34s][HUGECTR][INFO]: Eval Time for 100 iters: 0.092454s\n",
      "[28d10h34m35s][HUGECTR][INFO]: Iter: 800 Time(200 iters): 0.636804s Loss: 0.173919 lr:0.001000\n",
      "[28d10h34m36s][HUGECTR][INFO]: Iter: 1000 Time(200 iters): 0.534398s Loss: 0.184201 lr:0.001000\n",
      "Finish 1200 iterations with batchsize: 2720 in 3.41s\n"
     ]
    }
   ],
   "source": [
    "!python model_deepfm.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
