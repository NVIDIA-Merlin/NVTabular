{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Criteo: Training with HugeCTR\n",
    "\n",
    "## Overview\n",
    "\n",
    "HugeCTR is an open-source framework to accelerate the training of CTR estimation models on NVIDIA GPUs. It is written in CUDA C++ and highly exploits GPU-accelerated libraries such as cuBLAS, cuDNN, and NCCL.<br><br>\n",
    "HugeCTR offers multiple advantages to train deep learning recommender systems:\n",
    "\n",
    "1. **Speed**: HugeCTR is a highly efficient framework written C++. We experienced up to 10x speed up. HugeCTR on a NVIDIA DGX A100 system proved to be the fastest commercially available solution for training the architecture Deep Learning Recommender Model (DLRM) developed by Facebook.\n",
    "2. **Scale**: HugeCTR supports model parallel scaling. It distributes the large embedding tables over multiple GPUs or multiple nodes. \n",
    "3. **Easy-to-use**: Easy-to-use Python API similar to Keras. Examples for popular deep learning recommender systems architectures (Wide&Deep, DLRM, DCN, DeepFM) are available.\n",
    "\n",
    "HugeCTR is able to train recommender system models with larger-than-memory embedding tables by leveraging a parameter server. \n",
    "\n",
    "You can find more information about HugeCTR [here](https://github.com/NVIDIA/HugeCTR).\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this notebook, we learn how to to use HugeCTR for training recommender system models\n",
    "\n",
    "- Use **HugeCTR** to define a recommender system model\n",
    "- Train Facebook's [Deep Learning Recommendation Model](https://arxiv.org/pdf/1906.00091.pdf) with HugeCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with HugeCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As HugeCTR optimizes the training in CUDA++, we need to define the training pipeline and model architecture and execute it via the commandline. We will use the Python API, which is similar to Keras models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not familiar with HugeCTR's Python API and parameters, you can read more in its GitHub repository:\n",
    "- [HugeCTR User Guide](https://github.com/NVIDIA/HugeCTR/blob/master/docs/hugectr_user_guide.md)\n",
    "- [HugeCTR Python API](https://github.com/NVIDIA/HugeCTR/blob/master/docs/python_interface.md)\n",
    "- [HugeCTR example architectures](https://github.com/NVIDIA/HugeCTR/tree/master/samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write the code to a `./model.py` file and execute it. It will create snapshot, which we will use for inference in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /raid/data/criteo/test_dask/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system(\"rm -rf ./criteo_hugectr/\")\n",
    "os.system(\"mkdir -p ./criteo_hugectr/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.environ.get(\"BASE_DIR\", \"/raid/data/criteo\")\n",
    "OUTPUT_DATA_DIR = os.environ.get(\"OUTPUT_DATA_DIR\", BASE_DIR + \"/test_dask/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `graph_to_json` to convert the model to a JSON configuration, required for the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, \"w\") as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate model.py\n",
    "\n",
    "import hugectr\n",
    "from mpi4py import MPI  # noqa\n",
    "\n",
    "# HugeCTR\n",
    "solver = hugectr.CreateSolver(\n",
    "    vvgpu=[[0]],\n",
    "    max_eval_batches=100,\n",
    "    batchsize_eval=2720,\n",
    "    batchsize=2720,\n",
    "    i64_input_key=True,\n",
    "    use_mixed_precision=False,\n",
    "    repeat_dataset=True,\n",
    ")\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type=hugectr.Optimizer_t.SGD)\n",
    "reader = hugectr.DataReaderParams(\n",
    "    data_reader_type=hugectr.DataReaderType_t.Parquet,\n",
    "    source=[\"{OUTPUT_DATA_DIR}/train/_file_list.txt\"],\n",
    "    eval_source=\"{OUTPUT_DATA_DIR}/valid/_file_list.txt\",\n",
    "    check_type=hugectr.Check_t.Non,\n",
    "    slot_size_array=[\n",
    "        10000000,\n",
    "        10000000,\n",
    "        3014529,\n",
    "        400781,\n",
    "        11,\n",
    "        2209,\n",
    "        11869,\n",
    "        148,\n",
    "        4,\n",
    "        977,\n",
    "        15,\n",
    "        38713,\n",
    "        10000000,\n",
    "        10000000,\n",
    "        10000000,\n",
    "        584616,\n",
    "        12883,\n",
    "        109,\n",
    "        37,\n",
    "        17177,\n",
    "        7425,\n",
    "        20266,\n",
    "        4,\n",
    "        7085,\n",
    "        1535,\n",
    "        64,\n",
    "    ],\n",
    ")\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.add(\n",
    "    hugectr.Input(\n",
    "        label_dim=1,\n",
    "        label_name=\"label\",\n",
    "        dense_dim=13,\n",
    "        dense_name=\"dense\",\n",
    "        data_reader_sparse_param_array=[hugectr.DataReaderSparseParam(\"data1\", 1, False, 26)],\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.SparseEmbedding(\n",
    "        embedding_type=hugectr.Embedding_t.LocalizedSlotSparseEmbeddingHash,\n",
    "        workspace_size_per_gpu_in_mb=6000,\n",
    "        embedding_vec_size=128,\n",
    "        combiner=\"sum\",\n",
    "        sparse_embedding_name=\"sparse_embedding1\",\n",
    "        bottom_name=\"data1\",\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"dense\"],\n",
    "        top_names=[\"fc1\"],\n",
    "        num_output=512,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc1\"], top_names=[\"relu1\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu1\"],\n",
    "        top_names=[\"fc2\"],\n",
    "        num_output=256,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc2\"], top_names=[\"relu2\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu2\"],\n",
    "        top_names=[\"fc3\"],\n",
    "        num_output=128,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc3\"], top_names=[\"relu3\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.Interaction,\n",
    "        bottom_names=[\"relu3\", \"sparse_embedding1\"],\n",
    "        top_names=[\"interaction1\"],\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"interaction1\"],\n",
    "        top_names=[\"fc4\"],\n",
    "        num_output=1024,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc4\"], top_names=[\"relu4\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu4\"],\n",
    "        top_names=[\"fc5\"],\n",
    "        num_output=1024,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc5\"], top_names=[\"relu5\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu5\"],\n",
    "        top_names=[\"fc6\"],\n",
    "        num_output=512,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc6\"], top_names=[\"relu6\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu6\"],\n",
    "        top_names=[\"fc7\"],\n",
    "        num_output=256,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc7\"], top_names=[\"relu7\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu7\"],\n",
    "        top_names=[\"fc8\"],\n",
    "        num_output=1,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "        bottom_names=[\"fc8\", \"label\"],\n",
    "        top_names=[\"loss\"],\n",
    "    )\n",
    ")\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(max_iter=10000, eval_interval=3200, display=1000, snapshot=3200)\n",
    "model.graph_to_json(graph_config_file=\"./criteo_hugectr/1/criteo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained the model and created snapshots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
