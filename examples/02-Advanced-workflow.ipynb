{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb28e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77464844",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Advanced workflow\n",
    "\n",
    "This notebook is created using the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow/tags) container. \n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook builds on what is covered in [01-Getting-started NB](01-Getting-started.ipynb). In this notebook we will take a closer look at running more complex `Operators` as well as defining an operator of our own.\n",
    "\n",
    "We will also examine the functionality of the `Schema` -- this can be very useful for learning more about our data and instructing which columns we would like to use and how.\n",
    "\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "- Leveraging more complex Merlin NVTabular Operators\n",
    "- Splitting our data in chunks to limit memory footprint\n",
    "- Understanding the role of the schema and modifying it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5598ae",
   "metadata": {},
   "source": [
    "## Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60653f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 01:52:33.690029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-22 01:52:33.690468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-22 01:52:33.690602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from merlin.datasets.entertainment import get_movielens\n",
    "\n",
    "input_path = os.environ.get(\"INPUT_DATA_DIR\", os.path.expanduser(\"~/merlin-framework/movielens/\"))\n",
    "get_movielens(variant=\"ml-1m\", path=input_path); #noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d4c12",
   "metadata": {},
   "source": [
    "## Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0c3cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "\n",
    "train = cudf.read_parquet(f'{input_path}/ml-1m/train.parquet')\n",
    "valid = cudf.read_parquet(f'{input_path}/ml-1m/train.parquet')\n",
    "\n",
    "movies = cudf.read_parquet(f'{input_path}/ml-1m/movies_converted.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b83f0f2",
   "metadata": {},
   "source": [
    "From the provided `train` and `validation` sets we extract `userId`, `movieId` and `rating`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf108aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>467288</th>\n",
       "      <td>2880</td>\n",
       "      <td>208</td>\n",
       "      <td>2</td>\n",
       "      <td>989099150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376263</th>\n",
       "      <td>2195</td>\n",
       "      <td>2054</td>\n",
       "      <td>2</td>\n",
       "      <td>974606701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558754</th>\n",
       "      <td>3431</td>\n",
       "      <td>1408</td>\n",
       "      <td>4</td>\n",
       "      <td>967337733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54069</th>\n",
       "      <td>352</td>\n",
       "      <td>2951</td>\n",
       "      <td>5</td>\n",
       "      <td>976324605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537899</th>\n",
       "      <td>3312</td>\n",
       "      <td>2105</td>\n",
       "      <td>3</td>\n",
       "      <td>967956531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating  timestamp\n",
       "467288    2880      208       2  989099150\n",
       "376263    2195     2054       2  974606701\n",
       "558754    3431     1408       4  967337733\n",
       "54069      352     2951       5  976324605\n",
       "537899    3312     2105       3  967956531"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ffafa0",
   "metadata": {},
   "source": [
    "We will also use the metadata contained in `movies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86a8d694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>[Animation, Children's, Comedy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>[Adventure, Children's, Fantasy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>[Comedy, Romance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>[Comedy, Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>[Comedy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                             genres  \n",
       "0   [Animation, Children's, Comedy]  \n",
       "1  [Adventure, Children's, Fantasy]  \n",
       "2                 [Comedy, Romance]  \n",
       "3                   [Comedy, Drama]  \n",
       "4                          [Comedy]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d75cb24",
   "metadata": {},
   "source": [
    "Let's create `Merlin Dataset`s that we will be able to run through our workflow (a workflow defines the operations we would like performed on our data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0f72d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<merlin.io.dataset.Dataset at 0x7f4573a96c40>,\n",
       " <merlin.io.dataset.Dataset at 0x7f4573a96d60>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nvtabular as nvt\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "train_ds = nvt.Dataset(train, npartitions=2)\n",
    "valid_ds = nvt.Dataset(valid)\n",
    "\n",
    "train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf6eda0",
   "metadata": {},
   "source": [
    "The constructor `nvt.Dataset(...)` accepts an important parameter: `npartitions`. We can leverage it to specify into how many chunks we would like our data to be split. Our workflow will process data in chunks and by increasing the number of partitions we can limit the memory footprint.\n",
    "\n",
    "We have to remember though to shuffle the data so that all relevant records reside in a single partition. Otherwise, depending on the operations we chose to perform, the results of our workflow might be incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b09971ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<merlin.io.dataset.Dataset at 0x7f4573a9a1f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.shuffle_by_keys('userId')\n",
    "valid_ds.shuffle_by_keys('userId')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298ee6fb",
   "metadata": {},
   "source": [
    "We have now shuffled the records so that all examples for the same `userId` reside in the same partition.\n",
    "\n",
    "The number of partitions hasn't changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86d80fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.npartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee5c7c2",
   "metadata": {},
   "source": [
    "## Defining the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad73fc70",
   "metadata": {},
   "source": [
    "### Joining an external dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f849efd",
   "metadata": {},
   "source": [
    "We want to make use of information contained in the `movies` DataFrame.\n",
    "\n",
    "Let us join it onto our data, making sure we only select the `genres` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f5ad3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['movieId'] >> nvt.ops.JoinExternal(movies, on='movieId', columns_ext=['movieId', 'genres'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0956648e",
   "metadata": {},
   "source": [
    "The `genres` information is represented as a list of strings. In order for us to be able to use it for training an ML model we need to encode the strings as categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "174360dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [Animation, Children's, Comedy]\n",
       "1    [Adventure, Children's, Fantasy]\n",
       "2                   [Comedy, Romance]\n",
       "3                     [Comedy, Drama]\n",
       "4                            [Comedy]\n",
       "Name: genres, dtype: list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies['genres'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0242c3a0",
   "metadata": {},
   "source": [
    "### Encoding a list of strings as categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab23a9",
   "metadata": {},
   "source": [
    "We leverage the `Categorify` operator to encode our lists of strings.\n",
    "\n",
    "Additionally, let's treat the categories that appear infrequently as unknowns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45562fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = genres >> nvt.ops.Categorify(freq_threshold=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1004cf",
   "metadata": {},
   "source": [
    "### Running a custom preprocessing step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf87e58",
   "metadata": {},
   "source": [
    "Let's look at an example of how we can run a custom preprocessing step on our data.\n",
    "\n",
    "Here we will define our own function that will transform the `rating` column to `binary_rating`. We will output a value of False for any `rating` below 4 and a value of True for any rating of `4` or `5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20fd4f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_to_binary(col):\n",
    "    return col > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96b2806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_rating = ['rating'] >> nvt.ops.LambdaOp(rating_to_binary) >> nvt.ops.Rename(name='binary_rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666439ad",
   "metadata": {},
   "source": [
    "### Tagging columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357ca53e",
   "metadata": {},
   "source": [
    "Let us also tag our columns. This will help streamline model training and serving down the road, should we opt to do so using other components of the Merlin Framework.\n",
    "\n",
    "Running NVTabular operators on our data automatically tags the output columns. For instance, the `Categorify` operator we used above will tag the output columns as `Tags.CATEGORICAL`. Still, some information that can be useful down the road needs to be added by hand. This is true for instance for target information. Let's add this information below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0748ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "userId = ['userId'] >> nvt.ops.Categorify() >> nvt.ops.AddTags(tags=[Tags.USER_ID, Tags.CATEGORICAL, Tags.USER])\n",
    "movieId = ['movieId'] >> nvt.ops.Categorify() >> nvt.ops.AddTags(tags=[Tags.ITEM_ID, Tags.CATEGORICAL, Tags.ITEM])\n",
    "binary_rating = binary_rating >> nvt.ops.AddTags(tags=[Tags.TARGET, Tags.BINARY_CLASSIFICATION])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce8958",
   "metadata": {},
   "source": [
    "## Applying the workflow to the train and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0790675d",
   "metadata": {},
   "source": [
    "We are now ready to create our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b7226fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = nvt.Workflow(userId + movieId + genres + binary_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ac346",
   "metadata": {},
   "source": [
    "Let us now fit the `workflow` to our train set and transform the train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4aef5e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>genres</th>\n",
       "      <th>binary_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1172</td>\n",
       "      <td>453</td>\n",
       "      <td>[3, 7]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1187</td>\n",
       "      <td>189</td>\n",
       "      <td>[7, 10, 1, 15, 5]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4133</td>\n",
       "      <td>210</td>\n",
       "      <td>[3, 6, 11]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>589</td>\n",
       "      <td>[3, 16]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>213</td>\n",
       "      <td>221</td>\n",
       "      <td>[3, 7, 15, 5]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId             genres  binary_rating\n",
       "0    1172      453             [3, 7]          False\n",
       "1    1187      189  [7, 10, 1, 15, 5]          False\n",
       "2    4133      210         [3, 6, 11]           True\n",
       "3      71      589            [3, 16]           True\n",
       "4     213      221      [3, 7, 15, 5]          False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transformed = workflow.fit_transform(train_ds)\n",
    "valid_transformed = workflow.transform(valid_ds)\n",
    "valid_transformed.compute().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd00418",
   "metadata": {},
   "source": [
    "Our data was processed as we expected.\n",
    "\n",
    "Let us look at the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "778956ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.start_index</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>userId</td>\n",
       "      <td>(Tags.USER_ID, Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.userId.parquet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6041.0</td>\n",
       "      <td>userId</td>\n",
       "      <td>6041.0</td>\n",
       "      <td>210.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>movieId</td>\n",
       "      <td>(Tags.ITEM_ID, Tags.CATEGORICAL, Tags.ITEM)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.movieId.parquet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1853.0</td>\n",
       "      <td>movieId</td>\n",
       "      <td>1853.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>genres</td>\n",
       "      <td>(Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.genres.parquet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>genres</td>\n",
       "      <td>19.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>binary_rating</td>\n",
       "      <td>(Tags.BINARY_CLASSIFICATION, Tags.TARGET)</td>\n",
       "      <td>bool</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'userId', 'tags': {<Tags.USER_ID: 'user_id'>, <Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.userId.parquet', 'domain': {'min': 0, 'max': 6041, 'name': 'userId'}, 'embedding_sizes': {'cardinality': 6041, 'dimension': 210}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'movieId', 'tags': {<Tags.ITEM_ID: 'item_id'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.movieId.parquet', 'domain': {'min': 0, 'max': 1853, 'name': 'movieId'}, 'embedding_sizes': {'cardinality': 1853, 'dimension': 108}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'genres', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 100, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.genres.parquet', 'domain': {'min': 0, 'max': 19, 'name': 'genres'}, 'embedding_sizes': {'cardinality': 19, 'dimension': 16}}, 'dtype': dtype('int64'), 'is_list': True, 'is_ragged': True}, {'name': 'binary_rating', 'tags': {<Tags.BINARY_CLASSIFICATION: 'binary_classification'>, <Tags.TARGET: 'target'>}, 'properties': {}, 'dtype': dtype('bool'), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transformed.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48029e4b",
   "metadata": {},
   "source": [
    "The schema contains a lot of useful information, such as the frequency threshold that was used to process a column or its cardinality.\n",
    "\n",
    "Beyond providing us with useful information, schema can be used to inform other components of the Merlin Framework on what information from our datasets we would like it to act on and in what way.\n",
    "\n",
    "This is what the `Tags.TARGET`, `Tags.ITEM_ID`, and `Tags.USER_ID` achieve. When we train our Deep Leranin, the model will be able to infer which columns it should use as a source of what type of information.\n",
    "\n",
    "We can also use the schema to remove information that we wouldn't want our model to use in training.\n",
    "\n",
    "Let us look at an example of that below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f16be0",
   "metadata": {},
   "source": [
    "## Training a DLRM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f84cec3",
   "metadata": {},
   "source": [
    "Let us train a DLRM Model.\n",
    "\n",
    "It is usually advantageous to train on the entiriety of the available data. Let us do so below -- we will utilize all the columns included in our schema file.\n",
    "\n",
    "We tell the model about the structure of our data by passing in the `Schema` (`train_transformed.schema` in the example below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fafc27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 01:52:35.209478: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-22 01:52:35.210401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-22 01:52:35.210575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-22 01:52:35.210709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-22 01:52:35.210961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-22 01:52:35.211101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-22 01:52:35.211235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-22 01:52:35.211351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 24576 MB memory:  -> device: 0, name: Quadro RTX 8000, pci bus id: 0000:08:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 13s 13ms/step - loss: 0.5650 - precision: 0.7182 - recall: 0.8094 - binary_accuracy: 0.7079 - auc: 0.7681 - regularization_loss: 0.0000e+00 - val_loss: 0.5401 - val_precision: 0.7393 - val_recall: 0.8148 - val_binary_accuracy: 0.7283 - val_auc: 0.7952 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.5425 - precision: 0.7361 - recall: 0.8107 - binary_accuracy: 0.7241 - auc: 0.7899 - regularization_loss: 0.0000e+00 - val_loss: 0.5277 - val_precision: 0.7458 - val_recall: 0.8142 - val_binary_accuracy: 0.7337 - val_auc: 0.8033 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.5344 - precision: 0.7402 - recall: 0.8154 - binary_accuracy: 0.7294 - auc: 0.7972 - regularization_loss: 0.0000e+00 - val_loss: 0.5216 - val_precision: 0.7449 - val_recall: 0.8291 - val_binary_accuracy: 0.7385 - val_auc: 0.8095 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.5269 - precision: 0.7444 - recall: 0.8187 - binary_accuracy: 0.7342 - auc: 0.8039 - regularization_loss: 0.0000e+00 - val_loss: 0.5113 - val_precision: 0.7497 - val_recall: 0.8353 - val_binary_accuracy: 0.7451 - val_auc: 0.8179 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.5187 - precision: 0.7488 - recall: 0.8226 - binary_accuracy: 0.7394 - auc: 0.8112 - regularization_loss: 0.0000e+00 - val_loss: 0.5046 - val_precision: 0.7579 - val_recall: 0.8310 - val_binary_accuracy: 0.7503 - val_auc: 0.8250 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 1/3\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.4963 - precision: 0.7609 - recall: 0.8316 - binary_accuracy: 0.7530 - auc: 0.8293 - regularization_loss: 0.0000e+00 - val_loss: 0.4855 - val_precision: 0.7676 - val_recall: 0.8390 - val_binary_accuracy: 0.7615 - val_auc: 0.8381 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.4863 - precision: 0.7676 - recall: 0.8346 - binary_accuracy: 0.7597 - auc: 0.8371 - regularization_loss: 0.0000e+00 - val_loss: 0.4755 - val_precision: 0.7770 - val_recall: 0.8357 - val_binary_accuracy: 0.7677 - val_auc: 0.8456 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.4776 - precision: 0.7718 - recall: 0.8388 - binary_accuracy: 0.7648 - auc: 0.8437 - regularization_loss: 0.0000e+00 - val_loss: 0.4657 - val_precision: 0.7783 - val_recall: 0.8481 - val_binary_accuracy: 0.7739 - val_auc: 0.8526 - val_regularization_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4550c0f670>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "import merlin.models.tf as mm\n",
    "\n",
    "model = mm.DLRMModel(\n",
    "    train_transformed.schema,\n",
    "    embedding_dim=64,\n",
    "    bottom_block=mm.MLPBlock([128, 64]),\n",
    "    top_block=mm.MLPBlock([128, 64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask('rating_binary')\n",
    ")\n",
    "\n",
    "opt = tensorflow.optimizers.Adam(learning_rate=5e-3)\n",
    "model.compile(optimizer=opt)\n",
    "model.fit(train_transformed, validation_data=valid_transformed, batch_size=1024, epochs=5)\n",
    "\n",
    "model.optimizer.learning_rate = 1e-3\n",
    "model.fit(train_transformed, validation_data=valid_transformed, batch_size=1024, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af903a8",
   "metadata": {},
   "source": [
    "Let us now retry the training without the `genres` information. Maybe we have reason to suspect this data has some issues and we would like to verify how the model performs without this information being passed in.\n",
    "\n",
    "We can achieve all this by modifying the passed in schema using the `without` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e8132d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 10s 11ms/step - loss: 0.5668 - precision_1: 0.7178 - recall_1: 0.8092 - binary_accuracy: 0.7075 - auc_1: 0.7666 - regularization_loss: 0.0000e+00 - val_loss: 0.5409 - val_precision_1: 0.7296 - val_recall_1: 0.8329 - val_binary_accuracy: 0.7265 - val_auc_1: 0.7927 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.5453 - precision_1: 0.7347 - recall_1: 0.8094 - binary_accuracy: 0.7224 - auc_1: 0.7878 - regularization_loss: 0.0000e+00 - val_loss: 0.5306 - val_precision_1: 0.7478 - val_recall_1: 0.8038 - val_binary_accuracy: 0.7314 - val_auc_1: 0.8009 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.5378 - precision_1: 0.7405 - recall_1: 0.8076 - binary_accuracy: 0.7267 - auc_1: 0.7944 - regularization_loss: 0.0000e+00 - val_loss: 0.5242 - val_precision_1: 0.7516 - val_recall_1: 0.8077 - val_binary_accuracy: 0.7360 - val_auc_1: 0.8064 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.5316 - precision_1: 0.7431 - recall_1: 0.8123 - binary_accuracy: 0.7307 - auc_1: 0.7998 - regularization_loss: 0.0000e+00 - val_loss: 0.5184 - val_precision_1: 0.7486 - val_recall_1: 0.8253 - val_binary_accuracy: 0.7402 - val_auc_1: 0.8119 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.5246 - precision_1: 0.7472 - recall_1: 0.8148 - binary_accuracy: 0.7351 - auc_1: 0.8059 - regularization_loss: 0.0000e+00 - val_loss: 0.5111 - val_precision_1: 0.7561 - val_recall_1: 0.8217 - val_binary_accuracy: 0.7451 - val_auc_1: 0.8179 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 1/3\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.5053 - precision_1: 0.7581 - recall_1: 0.8220 - binary_accuracy: 0.7469 - auc_1: 0.8218 - regularization_loss: 0.0000e+00 - val_loss: 0.4962 - val_precision_1: 0.7668 - val_recall_1: 0.8221 - val_binary_accuracy: 0.7541 - val_auc_1: 0.8293 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.4968 - precision_1: 0.7642 - recall_1: 0.8239 - binary_accuracy: 0.7526 - auc_1: 0.8285 - regularization_loss: 0.0000e+00 - val_loss: 0.4887 - val_precision_1: 0.7720 - val_recall_1: 0.8238 - val_binary_accuracy: 0.7589 - val_auc_1: 0.8353 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.4897 - precision_1: 0.7687 - recall_1: 0.8257 - binary_accuracy: 0.7570 - auc_1: 0.8340 - regularization_loss: 0.0000e+00 - val_loss: 0.4808 - val_precision_1: 0.7817 - val_recall_1: 0.8180 - val_binary_accuracy: 0.7641 - val_auc_1: 0.8412 - val_regularization_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f45450ca1f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "import merlin.models.tf as mm\n",
    "\n",
    "model = mm.DLRMModel(\n",
    "    train_transformed.schema.without('genres'),   # <--- this is where we make the change\n",
    "    embedding_dim=64,\n",
    "    bottom_block=mm.MLPBlock([128, 64]),\n",
    "    top_block=mm.MLPBlock([128, 64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask('rating_binary')\n",
    ")\n",
    "\n",
    "opt = tensorflow.optimizers.Adam(learning_rate=5e-3)\n",
    "model.compile(optimizer=opt)\n",
    "model.fit(train_transformed, validation_data=valid_transformed, batch_size=1024, epochs=5)\n",
    "\n",
    "model.optimizer.learning_rate = 1e-3\n",
    "model.fit(train_transformed, validation_data=valid_transformed, batch_size=1024, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c9f35b",
   "metadata": {},
   "source": [
    "As it turns out, `genres` contained signal that was genuinely useful to the model as without it the performance on the validation set decreased!\n",
    "\n",
    "Using the `Schema` (combined with the ability of the Merlin Framework to infer the appropriate shape of the model based on our data) we can streamline the training and experimentation phase to a significant extent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "merlin": {
   "containers": [
    "nvcr.io/nvidia/merlin/merlin-tensorflow:latest"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
