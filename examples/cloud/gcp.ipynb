{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask\n",
    "from dask_cloudprovider.gcp import GCPCluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Platform - NVTabular Criteo Example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and Authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure dependencies are installed: `pip install gcsfs`, `pip install dask-cloudprovider[gcp]`\n",
    "    \n",
    "Configure credentials (`gcloud auth login`) and set project (`gcloud config set project <project>`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_config = {\n",
    "    # GCP config options\n",
    "    \"projectid\": \"merlin-295819\",\n",
    "    \"zone\": \"us-east1-c\",\n",
    "    \"machine_type\": \"n1-standard-8\",\n",
    "    \"gpu_type\": \"nvidia-tesla-v100\",\n",
    "    \"ngpus\": 1,\n",
    "    \"filesystem_size\": 10000,\n",
    "    \n",
    "    # RAPIDS config options\n",
    "    \"docker_image\": \"rapidsai/rapidsai:cuda11.0-runtime-ubuntu18.04-py3.8\",\n",
    "    \"worker_class\": \"dask_cuda.CUDAWorker\",\n",
    "    \n",
    "    # Dask/Python options\n",
    "    \"n_workers\": 1,\n",
    "    \"env_vars\": {\"EXTRA_PIP_PACKAGES\": \"gcsfs\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the cluster and wait until the 2 GPUs are ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching cluster with the following configuration: \n",
      "  Source Image: projects/ubuntu-os-cloud/global/images/ubuntu-minimal-1804-bionic-v20201014 \n",
      "  Docker Image: rapidsai/rapidsai:cuda11.0-runtime-ubuntu18.04-py3.8 \n",
      "  Machine Type: n1-standard-8 \n",
      "  Filesytsem Size: 10000 \n",
      "  N-GPU Type: 1 nvidia-tesla-v100\n",
      "  Zone: us-east1-c \n",
      "Creating scheduler instance\n",
      "dask-ad0b0aed-scheduler\n",
      "\tInternal IP: 10.142.0.10\n",
      "\tExternal IP: 35.196.218.184\n",
      "Waiting for scheduler to run\n",
      "Scheduler is running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/contextlib.py:88: UserWarning: Creating your cluster is taking a surprisingly long time. This is likely due to pending resources. Hang tight! \n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating worker instance\n",
      "dask-ad0b0aed-worker-d782119e\n",
      "\tInternal IP: 10.142.0.11\n",
      "\tExternal IP: 35.196.132.38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad214dab77540979129d2002ebc4e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>GCPCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster = GCPCluster(**cluster_config)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/albertoa/.local/lib/python3.6/site-packages/distributed/client.py:1129: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "+-------------+---------------+---------------+---------------+\n",
      "| Package     | client        | scheduler     | workers       |\n",
      "+-------------+---------------+---------------+---------------+\n",
      "| distributed | 2.30.0        | 2.30.1        | 2.30.1        |\n",
      "| numpy       | 1.18.3        | 1.19.4        | 1.19.4        |\n",
      "| python      | 3.6.9.final.0 | 3.8.6.final.0 | 3.8.6.final.0 |\n",
      "| tornado     | 6.0.4         | 6.1           | 6.1           |\n",
      "+-------------+---------------+---------------+---------------+\n",
      "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://35.196.218.184:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://35.196.218.184:8787/status' target='_blank'>http://35.196.218.184:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>1</li>\n",
       "  <li><b>Memory: </b>31.56 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.142.0.10:8786' processes=1 threads=1, memory=31.56 GB>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(cluster)\n",
    "client.wait_for_workers(1) # because one GPU workers per node\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running NVTabular Criteo Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dataset located in GCS\n",
    "input_path = \"gs://merlin-datasets/crit_int_pq/\"\n",
    "# Output data paths\n",
    "BASE_DIR = \"/raid/criteo/tests/\"\n",
    "dask_workdir = os.path.join(BASE_DIR, \"test_dask/workdir\")\n",
    "output_path = os.path.join(BASE_DIR, \"test_dask/output\")\n",
    "stats_path = os.path.join(BASE_DIR, \"test_dask/stats\")\n",
    "\n",
    "# number of days worth of data to use for training, the rest will be used for validation\n",
    "NUM_DAYS = 24\n",
    "NUM_TRAIN_DAYS = 23\n",
    "\n",
    "# define our dataset schema\n",
    "CONTINUOUS_COLUMNS = ['I' + str(x) for x in range(1,14)]\n",
    "CATEGORICAL_COLUMNS =  ['C' + str(x) for x in range(1,27)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "COLUMNS = CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS + LABEL_COLUMNS\n",
    "\n",
    "# Make sure we have a clean worker space for Dask\n",
    "if os.path.isdir(dask_workdir):\n",
    "    shutil.rmtree(dask_workdir)\n",
    "os.makedirs(dask_workdir)\n",
    "\n",
    "# Make sure we have a clean stats space for Dask\n",
    "if os.path.isdir(stats_path):\n",
    "    shutil.rmtree(stats_path)\n",
    "os.mkdir(stats_path)\n",
    "         \n",
    "# Make sure we have a clean output path\n",
    "if os.path.isdir(output_path):\n",
    "    shutil.rmtree(output_path)\n",
    "os.mkdir(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation files\n",
    "fname = 'day_{}.parquet'\n",
    "train_paths = [os.path.join(input_path, fname.format(day)) for day in range(NUM_TRAIN_DAYS)]\n",
    "valid_paths = [os.path.join(input_path, fname.format(day)) for day in range(NUM_TRAIN_DAYS, NUM_DAYS)]\n",
    "print(train_paths)\n",
    "print(valid_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and run Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Workflow\n",
    "proc = nvt.Workflow(\n",
    "    cat_names=CATEGORICAL_COLUMNS,\n",
    "    cont_names=CONTINUOUS_COLUMNS,\n",
    "    label_name=LABEL_COLUMNS,\n",
    "    client = client)\n",
    "\n",
    "# Apply conts and cats operators\n",
    "proc.add_cont_feature([ops.FillMissing(), ops.Clip(min_value=0), ops.LogOp()])\n",
    "proc.add_cat_preprocess(ops.Categorify(freq_threshold=15, out_path=stats_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data iterators\n",
    "train_dataset = nvt.Dataset(train_paths, engine='parquet', part_size=part_size)\n",
    "valid_dataset = nvt.Dataset(valid_paths, engine='parquet', part_size=part_size)\n",
    "# Create output dirs\n",
    "output_train_dir = os.path.join(output_path, 'train/')\n",
    "output_valid_dir = os.path.join(output_path, 'valid/')\n",
    "! mkdir -p $output_train_dir\n",
    "! mkdir -p $output_valid_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "proc.apply(train_dataset, shuffle=nvt.io.Shuffle.PER_PARTITION, output_path=output_train_dir, out_files_per_proc=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "proc.apply(train_dataset, shuffle=nvt.io.Shuffle.PER_PARTITION, output_path=output_train_dir, out_files_per_proc=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the client and cluster so resources in GCP are releasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
