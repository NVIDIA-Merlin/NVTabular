{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask\n",
    "from dask_cloudprovider.gcp import GCPCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "import boto3\n",
    "\n",
    "import os\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import Normalize,  Categorify,  LogOp, FillMissing, Clip\n",
    "\n",
    "from nvtabular.utils import get_rmm_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Web Service - NVTabular Criteo Example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and Authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure dependencies are installed: `pip install s3fs`, `pip install dask-cloudprovider[aws]`\n",
    "    \n",
    "Configure credentials (`awsconfigure`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_config = {\n",
    "    # AWS config options\n",
    "    \"region\": \"us-east-1\",\n",
    "    \"instance_type\": \"p3.2xlarge\",  # Has 1 V100s\n",
    "    \"filesystem_size\": 1000,\n",
    "    \n",
    "    # RAPIDS config options\n",
    "    \"docker_image\": \"nvcr.io/nvidia/nvtabular:0.3\",\n",
    "    \"worker_class\": \"dask_cuda.CUDAWorker\",\n",
    "    \n",
    "    # Dask/Python options\n",
    "    \"n_workers\": 1,\n",
    "    \"env_vars\": {\"EXTRA_PIP_PACKAGES\": \"s3fs\", \"EXTRA_CONDA_PACKAGES\": \"distributed\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the cluster and wait until the 2 GPUs are ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching cluster with the following configuration: \n",
      "  Source Image: projects/ubuntu-os-cloud/global/images/ubuntu-minimal-1804-bionic-v20201014 \n",
      "  Docker Image: nvcr.io/nvidia/nvtabular:0.3 \n",
      "  Machine Type: n1-standard-8 \n",
      "  Filesytsem Size: 10000 \n",
      "  N-GPU Type: 1 nvidia-tesla-v100\n",
      "  Zone: us-east1-c \n",
      "Creating scheduler instance\n",
      "dask-759aa49e-scheduler\n",
      "\tInternal IP: 10.142.0.14\n",
      "\tExternal IP: 104.196.12.26\n",
      "Waiting for scheduler to run\n"
     ]
    }
   ],
   "source": [
    "cluster = GCPCluster(**cluster_config)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client.wait_for_workers(1) # because one GPU workers per node\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running NVTabular Criteo Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_rmm_pool(client, pool_size):\n",
    "    # Initialize an RMM pool allocator.\n",
    "    # Note: RMM may require the pool size to be a multiple of 256.\n",
    "    pool_size = get_rmm_size(pool_size)\n",
    "    client.run(rmm.reinitialize, pool_allocator=True, initial_pool_size=pool_size)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "# Input dataset located in GCS\n",
    "input_path = \"s3://merlin-datasets/crit_int_pq/\"\n",
    "output_path = \"s3://merlin-datasets/output\"\n",
    "\n",
    "# Output data paths\n",
    "BASE_DIR = \"output\"\n",
    "dask_workdir = os.path.join(BASE_DIR, \"test_dask/workdir\")\n",
    "out_path = os.path.join(BASE_DIR, \"test_dask/output\")\n",
    "stats_path = os.path.join(BASE_DIR, \"test_dask/stats\")\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('merlin-datasets')\n",
    "\n",
    "# Make sure we have a clean worker space for Dask\n",
    "for key in bucket.list(prefix=dask_workdir):\n",
    "    key.delete()\n",
    "\n",
    "# Make sure we have a clean stats space for Dask\n",
    "for key in bucket.list(prefix=stats_path):\n",
    "    key.delete()\n",
    "\n",
    "# Make sure we have a clean output path\n",
    "for key in bucket.list(prefix=out_path):\n",
    "    key.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of days worth of data to use for training, the rest will be used for validation\n",
    "NUM_DAYS = 24\n",
    "NUM_TRAIN_DAYS = 23\n",
    "\n",
    "# define our dataset schema\n",
    "CONTINUOUS_COLUMNS = ['I' + str(x) for x in range(1,14)]\n",
    "CATEGORICAL_COLUMNS =  ['C' + str(x) for x in range(1,27)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "COLUMNS = CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS + LABEL_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation files\n",
    "fname = 'day_{}.parquet'\n",
    "train_paths = [os.path.join(input_path, fname.format(day)) for day in range(NUM_TRAIN_DAYS)]\n",
    "valid_paths = [os.path.join(input_path, fname.format(day)) for day in range(NUM_TRAIN_DAYS, NUM_DAYS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and run Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conts and cats operators\n",
    "cat_features = CATEGORICAL_COLUMNS >> Categorify(freq_threshold=15, out_path=stats_path)\n",
    "cont_features = CONTINUOUS_COLUMNS >> FillMissing() >> Clip(min_value=0) >> LogOp() >> Normalize()\n",
    "\n",
    "# Create Workflow\n",
    "workflow = nvt.Workflow(cat_features + cont_features + LABEL_COLUMNS, client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data iterators\n",
    "train_dataset = nvt.Dataset(train_paths, engine='parquet', part_size=part_size)\n",
    "valid_dataset = nvt.Dataset(valid_paths, engine='parquet', part_size=part_size)\n",
    "# Create output dirs\n",
    "output_train_dir = os.path.join(output_path, 'train/')\n",
    "output_valid_dir = os.path.join(output_path, 'valid/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "workflow.transform(train_dataset).to_parquet(output_path=output_train_dir,\n",
    "                                         shuffle=nvt.io.Shuffle.PER_PARTITION, \n",
    "                                         out_files_per_proc=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "workflow.transform(valid_dataset).to_parquet(output_path=output_valid_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the client and cluster so resources in GCP are releasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
