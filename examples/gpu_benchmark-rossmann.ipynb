{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GPU_id = 3\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(GPU_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time \n",
    "# from ds_itr.adamw import AdamW as AW\n",
    "\n",
    "import fastai\n",
    "from fastai import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.tabular import *\n",
    "from fastai.basic_data import DataBunch\n",
    "from fastai.tabular import TabularModel\n",
    "\n",
    "import cudf\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import Normalize, FillMissing, Categorify, Moments, Median, Encoder, LogOp, ZeroFill\n",
    "from nvtabular.batchloader import TensorItrDataset, FileItrDataset, DLCollator, DLDataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__, cudf.__version__, fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext snakeviz\n",
    "# load snakeviz if you want to run profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_cpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "datafolder = '/datasets/rossmann/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traindf = pd.read_csv(f'{datafolder}/train.csv', low_memory=False)\n",
    "# testdf = pd.read_csv(f'{datafolder}/test.csv', low_memory=False)\n",
    "# storedf = pd.read_csv(f'{datafolder}/store.csv', low_memory=False)\n",
    "# print(\"Preprocessing...\")\n",
    "\n",
    "categorical_cols = [\n",
    "    'Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'Week', 'Promo', 'StateHoliday', 'SchoolHoliday',\n",
    "    'StoreType', 'Assortment', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek',\n",
    "    'Promo2SinceYear', 'PromoInterval',\n",
    "]\n",
    "continuous_cols = [\n",
    "    'CompetitionDistance'\n",
    "]\n",
    "# traindf = traindf.drop(['Customers', 'Open'], axis=1)\n",
    "# testdf = testdf.drop(['Open', 'Id'], axis=1)\n",
    "\n",
    "# def modify_df(df):\n",
    "#     initlen = len(df)\n",
    "\n",
    "#     # Expand date into [Year, Month, Day, Week]\n",
    "#     df.Date = pd.to_datetime(df.Date)\n",
    "#     for attr in ['Year', 'Month', 'Day', 'Week']:\n",
    "#         df[attr] = getattr(pd.DatetimeIndex(df.Date), attr.lower())\n",
    "\n",
    "#     # Join store.csv and train/test.csv on Store\n",
    "#     df = pd.merge(df, storedf, on='Store')\n",
    "\n",
    "#     # Fix NaNs in *Since* columns\n",
    "#     defaults = {'CompetitionOpenSinceYear': 1900, 'CompetitionOpenSinceMonth': 1,\n",
    "#                 'Promo2SinceYear': 1900, 'Promo2SinceWeek': 1, 'CompetitionDistance': 0}\n",
    "#     df = df.fillna(defaults)\n",
    "\n",
    "#     # Ensure no rows lost on the join\n",
    "#     assert len(df) == initlen\n",
    "\n",
    "#     # Cast continuous columns to float\n",
    "#     df = df.astype({x: 'float32' for x in continuous_cols})\n",
    "\n",
    "#     # StateHoliday is a special case\n",
    "#     df.StateHoliday = df.StateHoliday.astype(str).replace(\"0\", \"d\")\n",
    "\n",
    "#     # Move the continuous columns to the end\n",
    "#     for column in continuous_cols:\n",
    "#         cd = df.pop(column)\n",
    "#         df[column] = cd\n",
    "#     return df\n",
    "\n",
    "# traindf = modify_df(traindf)\n",
    "# testdf = modify_df(testdf)\n",
    "\n",
    "# # Filter out 0 sales from train\n",
    "# traindf = traindf[traindf.Sales > 0]\n",
    "\n",
    "# # Index categorical columns across both train and test.\n",
    "# for col in categorical_cols:\n",
    "#     # Get the default type for the column\n",
    "#     default_type = traindf[col].dtype.type() if traindf[col].dtype.type() is not None else ''\n",
    "#     # Fix NaNs with the default value for the column's type.\n",
    "#     traindf[col] = traindf[col].fillna(default_type)\n",
    "#     testdf[col] = testdf[col].fillna(default_type)\n",
    "\n",
    "# # Move the sales column to the end\n",
    "# sales = traindf.pop('Sales')\n",
    "# traindf['Sales'] = sales\n",
    "\n",
    "# # Make a validation set from the equivalent period of the test set in the training set, a year before.\n",
    "# t0 = testdf.Date.min() - datetime.timedelta(365)\n",
    "# t1 = testdf.Date.max() - datetime.timedelta(365)\n",
    "# val_mask = (traindf.Date > t0) & (traindf.Date <= t1)\n",
    "# valdf, traindf = traindf[val_mask], traindf[~val_mask]\n",
    "\n",
    "# # Drop unnecessary columns\n",
    "# traindf = traindf.drop('Date', axis=1)\n",
    "# valdf = valdf.drop('Date', axis=1)\n",
    "# testdf = testdf.drop('Date', axis=1)\n",
    "\n",
    "# print(\"Saving to file...\")\n",
    "# traindf.to_csv(f'{datafolder}/train_fin.csv', index=False)\n",
    "# testdf.to_csv(f'{datafolder}/test_fin.csv', index=False)\n",
    "# valdf.to_csv(f'{datafolder}/val_fin.csv', index=False)\n",
    "\n",
    "# print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traindf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Dataset Gathering: Define files in the training and validation datasets. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = os.path.join(datafolder , 'train_fin.csv')\n",
    "valid_set = os.path.join(datafolder , 'val_fin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Store', 'DayOfWeek', 'Promo', 'StateHoliday', 'SchoolHoliday', 'Year',\n",
    "       'Month', 'Day', 'Week', 'StoreType', 'Assortment',\n",
    "       'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2',\n",
    "       'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval',\n",
    "       'CompetitionDistance', 'Sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Grab column information</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_names = [\n",
    "        'Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'Week', 'Promo', 'StateHoliday', 'SchoolHoliday',\n",
    "        'StoreType', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Assortment',\n",
    "        'Promo2SinceYear', 'PromoInterval',\n",
    "    ]\n",
    "cont_names = [\n",
    "        'CompetitionDistance'\n",
    "    ] \n",
    "cat_names = [name for name in cat_names if name in cols]\n",
    "cont_names = [name for name in cont_names if name in cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocessing:</h3> <p>Select operations to perform, create the Preprocessor object, create dataset iterator object and collect the stats on the training dataset</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "proc = nvt.Workflow(cat_names=cat_names, cont_names=cont_names, label_name=['Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.add_cont_feature(FillMissing())\n",
    "proc.add_cont_preprocess(Normalize())\n",
    "proc.add_cat_preprocess(Categorify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trains_itrs = nvt.dataset(train_set,names=cols, engine='csv')\n",
    "valid_itrs = nvt.dataset(valid_set, names=cols, engine='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_train = './jp_ross/train'\n",
    "output_path_valid = './jp_ross/valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "proc.apply(trains_itrs, apply_offline=True, record_stats=True, output_path=output_path_train, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "proc.apply(valid_itrs, apply_offline=True, record_stats=False, output_path=output_path_valid, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.create_final_cols()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Gather embeddings using statistics gathered in the Read phase.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [x[1] for x in proc.df_ops['Categorify'].get_emb_sz(proc.stats[\"categories\"], proc.columns_ctx[\"categorical\"]['base'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_set = [os.path.join(output_path_train, x) for x in os.listdir(output_path_train) if x.endswith(\"parquet\")]\n",
    "new_valid_set = [os.path.join(output_path_valid, x) for x in os.listdir(output_path_valid) if x.endswith(\"parquet\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_valids_itr = nvt.dataset(new_valid_set)\n",
    "new_trains_itr = nvt.dataset(new_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Create the file iterators using the FileItrDataset Class.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xy = proc.ds_to_tensors(new_trains_itr, apply_ops=False)\n",
    "val_xy = proc.ds_to_tensors(new_trains_itr, apply_ops=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t_sets = [train_xy[0], train_xy[1], train_xy[2]]\n",
    "v_sets = [val_xy[0], val_xy[1], val_xy[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_batch_sets = [TensorItrDataset(t_sets, batch_size=100000) for i in range(10)]\n",
    "v_batch_sets = [TensorItrDataset(v_sets, batch_size=100000) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t_chain = torch.utils.data.ChainDataset(t_batch_sets)\n",
    "v_chain = torch.utils.data.ChainDataset(v_batch_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Use the Deep Learning Collator to create a collate function to pass to the dataloader.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_col(batch):\n",
    "    batch = batch[0]\n",
    "    return batch[0], batch[1].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t_data = DLDataLoader(t_chain, collate_fn=gen_col, pin_memory=False, num_workers=0)\n",
    "v_data = DLDataLoader(v_chain, collate_fn=gen_col, pin_memory=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>After creating the Dataloaders you can leverage fastai framework to create Machine Learning models</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databunch = DataBunch(t_data, v_data, collate_fn=gen_col, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = TabularModel(emb_szs = embeddings, n_cont=len(cont_names), out_sz=1, layers=[1000,500])\n",
    "\n",
    "learn =  Learner(databunch, model)\n",
    "learn.loss_func = MSELossFlat()\n",
    "# learn.opt_func = AW\n",
    "learn.lr_find()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot(show_moms=True, suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2.75-2\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "learn.fit(epochs,learning_rate)\n",
    "t_final = time() - start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
