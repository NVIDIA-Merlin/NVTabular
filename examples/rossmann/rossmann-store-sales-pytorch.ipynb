{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# NVTabular demo on Rossmann data - PyTorch\n",
    "\n",
    "## Overview\n",
    "\n",
    "NVTabular is a feature engineering and preprocessing library for tabular data designed to quickly and easily manipulate terabyte scale datasets used to train deep learning based recommender systems.  It provides a high level abstraction to simplify code and accelerates computation on the GPU using the RAPIDS cuDF library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "\n",
    "In the previous notebooks ([rossmann-store-sales-preproc.ipynb](https://github.com/NVIDIA/NVTabular/blob/main/examples/rossmann/rossmann-store-sales-preproc.ipynb) and [rossmann-store-sales-feature-engineering.ipynb](https://github.com/NVIDIA/NVTabular/blob/main/examples/rossmann/rossmann-store-sales-feature-engineering.ipynb)), we downloaded, preprocessed and created features for the dataset. Now, we are ready to train our deep learning model on the dataset. In this notebook, we use **PyTorch** with the NVTabular data loader for PyTorch to accelereate the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import nvtabular as nvt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading NVTabular workflow\n",
    "This time, we only need to define our data directories. We can load the data schema from the NVTabular workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.environ.get(\"OUTPUT_DATA_DIR\", \"./data\")\n",
    "PREPROCESS_DIR = os.path.join(DATA_DIR, 'ross_pre')\n",
    "PREPROCESS_DIR_TRAIN = os.path.join(PREPROCESS_DIR, 'train')\n",
    "PREPROCESS_DIR_VALID = os.path.join(PREPROCESS_DIR, 'valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What files are available to train on in our directories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  valid\r\n"
     ]
    }
   ],
   "source": [
    "!ls $PREPROCESS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0b096162c7e54285961978c247e4aa27.parquet  _file_list.txt  _metadata.json\r\n",
      "1.2b7b8b6edf4d425e87f09d2a46d15f3b.parquet  _metadata\r\n"
     ]
    }
   ],
   "source": [
    "!ls $PREPROCESS_DIR_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_metadata  part.0.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls $PREPROCESS_DIR_VALID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a saved NVTabular workflow, we need to initalize a NVTabular workflow, first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, we can initialize it with an empty schema\n",
    "proc = nvt.Workflow(\n",
    "    cat_names=[],\n",
    "    cont_names=[],\n",
    "    label_name=[]\n",
    ")\n",
    "proc.load_stats(PREPROCESS_DIR + 'stats_and_workflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the categorical, continuous and label column names from the NVTabular workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = proc.columns_ctx['final']['cols']['categorical']\n",
    "CONTINUOUS_COLUMNS = proc.columns_ctx['final']['cols']['continuous']\n",
    "LABEL_COLUMNS = proc.columns_ctx['final']['cols']['label']\n",
    "\n",
    "COLUMNS = CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS + LABEL_COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the statistics for embedding tables of our neural network. The following shows the cardinality of each categorical variable along with its associated embedding size. Each entry is of the form `(cardinality, embedding_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Assortment': (4, 3),\n",
       " 'CompetitionMonthsOpen': (26, 10),\n",
       " 'CompetitionOpenSinceYear': (24, 9),\n",
       " 'Day': (32, 11),\n",
       " 'DayOfWeek': (8, 5),\n",
       " 'Events': (22, 9),\n",
       " 'Month': (13, 7),\n",
       " 'Promo2SinceYear': (9, 5),\n",
       " 'Promo2Weeks': (27, 10),\n",
       " 'PromoInterval': (4, 3),\n",
       " 'Promo_bw': (7, 5),\n",
       " 'Promo_fw': (7, 5),\n",
       " 'SchoolHoliday_bw': (9, 5),\n",
       " 'SchoolHoliday_fw': (9, 5),\n",
       " 'State': (13, 7),\n",
       " 'StateHoliday': (3, 3),\n",
       " 'StateHoliday_bw': (4, 3),\n",
       " 'StateHoliday_fw': (4, 3),\n",
       " 'Store': (1116, 16),\n",
       " 'StoreType': (5, 4),\n",
       " 'Week': (53, 15),\n",
       " 'Year': (4, 3)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_TABLE_SHAPES = nvt.ops.get_embedding_sizes(proc)\n",
    "EMBEDDING_TABLE_SHAPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, we can initialize it with an empty schema\n",
    "proc = nvt.Workflow(\n",
    "    cat_names=CATEGORICAL_COLUMNS,\n",
    "    cont_names=CONTINUOUS_COLUMNS,\n",
    "    label_name=LABEL_COLUMNS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.load_stats(PREPROCESS_DIR + 'stats_and_workflow')\n",
    "EMBEDDING_TABLE_SHAPES = nvt.ops.get_embedding_sizes(proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Network\n",
    "\n",
    "Now that our data is preprocessed and saved out, we can leverage `dataset`s to read through the preprocessed parquet files in an online fashion to train neural networks.\n",
    "\n",
    "We'll start by setting some universal hyperparameters for our model and optimizer. These settings will be the same across all of the frameworks that we explore in the different notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DROPOUT_RATE = 0.04\n",
    "DROPOUT_RATES = [0.001, 0.01]\n",
    "HIDDEN_DIMS = [1000, 500]\n",
    "BATCH_SIZE = 65536\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 25\n",
    "\n",
    "# TODO: Calculate on the fly rather than recalling from previous analysis.\n",
    "MAX_SALES_IN_TRAINING_SET = 38722.0\n",
    "MAX_LOG_SALES_PREDICTION = 1.2 * math.log(MAX_SALES_IN_TRAINING_SET + 1.0)\n",
    "\n",
    "TRAIN_PATHS = sorted(glob.glob(os.path.join(PREPROCESS_DIR_TRAIN, '*.parquet')))\n",
    "VALID_PATHS = sorted(glob.glob(os.path.join(PREPROCESS_DIR_VALID, '*.parquet')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch<a id=\"PyTorch\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Preparing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nvtabular.loader.torch import TorchAsyncItr, DLDataLoader\n",
    "from nvtabular.framework_utils.torch.models import Model\n",
    "from nvtabular.framework_utils.torch.utils import process_epoch\n",
    "\n",
    "# TensorItrDataset returns a single batch of x_cat, x_cont, y.\n",
    "collate_fn = lambda x: x\n",
    "\n",
    "train_dataset = TorchAsyncItr(nvt.Dataset(TRAIN_PATHS), batch_size=BATCH_SIZE, cats=CATEGORICAL_COLUMNS, conts=CONTINUOUS_COLUMNS, labels=LABEL_COLUMNS)\n",
    "train_loader = DLDataLoader(train_dataset, batch_size=None, collate_fn=collate_fn, pin_memory=False, num_workers=0)\n",
    "\n",
    "valid_dataset = TorchAsyncItr(nvt.Dataset(VALID_PATHS), batch_size=BATCH_SIZE, cats=CATEGORICAL_COLUMNS, conts=CONTINUOUS_COLUMNS, labels=LABEL_COLUMNS)\n",
    "valid_loader = DLDataLoader(valid_dataset, batch_size=None, collate_fn=collate_fn, pin_memory=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Defining a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    embedding_table_shapes=EMBEDDING_TABLE_SHAPES,\n",
    "    num_continuous=len(CONTINUOUS_COLUMNS),\n",
    "    emb_dropout=EMBEDDING_DROPOUT_RATE,\n",
    "    layer_hidden_dims=HIDDEN_DIMS,\n",
    "    layer_dropout_rates=DROPOUT_RATES,\n",
    "    max_output=MAX_LOG_SALES_PREDICTION\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe_func(y_pred, y):\n",
    "    \"Return y_pred and y to non-log space and compute RMSPE\"\n",
    "    y_pred, y = torch.exp(y_pred) - 1, torch.exp(y) - 1\n",
    "    pct_var = (y_pred - y) / y\n",
    "    return (pct_var**2).mean().pow(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 00. Train loss: 8.6104. Train RMSPE: inf. Valid loss: 3.9498. Valid RMSPE: 0.8429.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 01. Train loss: 4.0310. Train RMSPE: 0.8177. Valid loss: 3.1250. Valid RMSPE: 0.8055.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 02. Train loss: 2.6453. Train RMSPE: 0.7657. Valid loss: 1.7697. Valid RMSPE: 0.6968.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 03. Train loss: 1.3757. Train RMSPE: 0.6448. Valid loss: 0.7813. Valid RMSPE: 0.5389.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 04. Train loss: 0.4870. Train RMSPE: 0.4860. Valid loss: 0.2632. Valid RMSPE: 0.4807.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 05. Train loss: 0.2209. Train RMSPE: 0.5534. Valid loss: 0.2236. Valid RMSPE: 0.6519.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 06. Train loss: 0.1985. Train RMSPE: 0.6176. Valid loss: 0.2016. Valid RMSPE: 0.6156.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 07. Train loss: 0.1734. Train RMSPE: 0.5410. Valid loss: 0.1635. Valid RMSPE: 0.4993.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 08. Train loss: 0.1560. Train RMSPE: 0.4829. Valid loss: 0.1493. Valid RMSPE: 0.4653.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 09. Train loss: 0.1456. Train RMSPE: 0.4707. Valid loss: 0.1369. Valid RMSPE: 0.4417.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 10. Train loss: 0.1372. Train RMSPE: 0.4361. Valid loss: 0.1303. Valid RMSPE: 0.4419.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 11. Train loss: 0.1298. Train RMSPE: 0.4496. Valid loss: 0.1213. Valid RMSPE: 0.4180.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 12. Train loss: 0.1233. Train RMSPE: 0.4065. Valid loss: 0.1177. Valid RMSPE: 0.4155.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 13. Train loss: 0.1187. Train RMSPE: 0.4138. Valid loss: 0.1091. Valid RMSPE: 0.3908.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 14. Train loss: 0.1124. Train RMSPE: 0.3875. Valid loss: 0.1045. Valid RMSPE: 0.3869.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 15. Train loss: 0.1076. Train RMSPE: 0.3985. Valid loss: 0.1016. Valid RMSPE: 0.3816.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 16. Train loss: 0.1145. Train RMSPE: 0.4027. Valid loss: 0.0939. Valid RMSPE: 0.3496.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 17. Train loss: 0.1037. Train RMSPE: 0.3850. Valid loss: 0.0921. Valid RMSPE: 0.3484.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 18. Train loss: 0.0963. Train RMSPE: 0.3727. Valid loss: 0.0864. Valid RMSPE: 0.3416.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 19. Train loss: 0.0925. Train RMSPE: 0.3515. Valid loss: 0.0842. Valid RMSPE: 0.3430.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 20. Train loss: 0.0918. Train RMSPE: 0.3639. Valid loss: 0.0800. Valid RMSPE: 0.3272.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 21. Train loss: 0.0872. Train RMSPE: 0.3625. Valid loss: 0.0771. Valid RMSPE: 0.3114.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 22. Train loss: 0.0843. Train RMSPE: 0.3349. Valid loss: 0.0745. Valid RMSPE: 0.3121.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 23. Train loss: 0.0838. Train RMSPE: 0.3377. Valid loss: 0.0800. Valid RMSPE: 0.3297.\n",
      "Total batches: 12\n",
      "Total batches: 0\n",
      "Epoch 24. Train loss: 0.0867. Train RMSPE: 0.3467. Valid loss: 0.0716. Valid RMSPE: 0.3024.\n",
      "CPU times: user 55.2 s, sys: 6.18 s, total: 1min 1s\n",
      "Wall time: 51.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, y_pred, y = process_epoch(train_loader, model, train=True, optimizer=optimizer)\n",
    "    train_rmspe = rmspe_func(y_pred, y)\n",
    "    valid_loss, y_pred, y = process_epoch(valid_loader, model, train=False)\n",
    "    valid_rmspe = rmspe_func(y_pred, y)\n",
    "    print(f'Epoch {epoch:02d}. Train loss: {train_loss:.4f}. Train RMSPE: {train_rmspe:.4f}. Valid loss: {valid_loss:.4f}. Valid RMSPE: {valid_rmspe:.4f}.')"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
