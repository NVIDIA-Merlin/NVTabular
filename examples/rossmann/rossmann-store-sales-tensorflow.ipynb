{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# NVTabular demo on Rossmann data - TensorFlow\n",
    "\n",
    "## Overview\n",
    "\n",
    "NVTabular is a feature engineering and preprocessing library for tabular data designed to quickly and easily manipulate terabyte scale datasets used to train deep learning based recommender systems.  It provides a high level abstraction to simplify code and accelerates computation on the GPU using the RAPIDS cuDF library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "\n",
    "In the previous notebooks ([rossmann-store-sales-preproc.ipynb](https://github.com/NVIDIA/NVTabular/blob/main/examples/rossmann/rossmann-store-sales-preproc.ipynb) and [rossmann-store-sales-feature-engineering.ipynb](https://github.com/NVIDIA/NVTabular/blob/main/examples/rossmann/rossmann-store-sales-feature-engineering.ipynb)), we downloaded, preprocessed and created features for the dataset. Now, we are ready to train our deep learning model on the dataset. In this notebook, we use **TensorFlow** with the NVTabular data loader for TensorFlow to accelereate the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import nvtabular as nvt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading NVTabular workflow\n",
    "This time, we only need to define our data directories. We can load the data schema from the NVTabular workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.environ.get(\"OUTPUT_DATA_DIR\", \"./data\")\n",
    "PREPROCESS_DIR = os.path.join(DATA_DIR, 'ross_pre')\n",
    "PREPROCESS_DIR_TRAIN = os.path.join(PREPROCESS_DIR, 'train')\n",
    "PREPROCESS_DIR_VALID = os.path.join(PREPROCESS_DIR, 'valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What files are available to train on in our directories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  valid\r\n"
     ]
    }
   ],
   "source": [
    "!ls $PREPROCESS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0b096162c7e54285961978c247e4aa27.parquet  _file_list.txt  _metadata.json\r\n",
      "1.2b7b8b6edf4d425e87f09d2a46d15f3b.parquet  _metadata\r\n"
     ]
    }
   ],
   "source": [
    "!ls $PREPROCESS_DIR_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_metadata  part.0.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls $PREPROCESS_DIR_VALID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a saved NVTabular workflow, we need to initalize a NVTabular workflow, first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, we can initialize it with an empty schema\n",
    "proc = nvt.Workflow(\n",
    "    cat_names=[],\n",
    "    cont_names=[],\n",
    "    label_name=[]\n",
    ")\n",
    "proc.load_stats(PREPROCESS_DIR + 'stats_and_workflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the categorical, continuous and label column names from the NVTabular workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = proc.columns_ctx['final']['cols']['categorical']\n",
    "CONTINUOUS_COLUMNS = proc.columns_ctx['final']['cols']['continuous']\n",
    "LABEL_COLUMNS = proc.columns_ctx['final']['cols']['label']\n",
    "\n",
    "COLUMNS = CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS + LABEL_COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the statistics for embedding tables of our neural network. The following shows the cardinality of each categorical variable along with its associated embedding size. Each entry is of the form `(cardinality, embedding_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Assortment': (4, 3),\n",
       " 'CompetitionMonthsOpen': (26, 10),\n",
       " 'CompetitionOpenSinceYear': (24, 9),\n",
       " 'Day': (32, 11),\n",
       " 'DayOfWeek': (8, 5),\n",
       " 'Events': (22, 9),\n",
       " 'Month': (13, 7),\n",
       " 'Promo2SinceYear': (9, 5),\n",
       " 'Promo2Weeks': (27, 10),\n",
       " 'PromoInterval': (4, 3),\n",
       " 'Promo_bw': (7, 5),\n",
       " 'Promo_fw': (7, 5),\n",
       " 'SchoolHoliday_bw': (9, 5),\n",
       " 'SchoolHoliday_fw': (9, 5),\n",
       " 'State': (13, 7),\n",
       " 'StateHoliday': (3, 3),\n",
       " 'StateHoliday_bw': (4, 3),\n",
       " 'StateHoliday_fw': (4, 3),\n",
       " 'Store': (1116, 16),\n",
       " 'StoreType': (5, 4),\n",
       " 'Week': (53, 15),\n",
       " 'Year': (4, 3)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_TABLE_SHAPES = nvt.ops.get_embedding_sizes(proc)\n",
    "EMBEDDING_TABLE_SHAPES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Network\n",
    "\n",
    "Now that our data is preprocessed and saved out, we can leverage `dataset`s to read through the preprocessed parquet files in an online fashion to train neural networks.\n",
    "\n",
    "We'll start by setting some universal hyperparameters for our model and optimizer. These settings will be the same across all of the frameworks that we explore in the different notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested in contributing to NVTabular, feel free to take this challenge on and submit a pull request if successful. 12% RMSPE is achievable using the Novograd optimizer, but we know of no Novograd implementation for TensorFlow that supports sparse gradients, and so we are not including that solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DROPOUT_RATE = 0.04\n",
    "DROPOUT_RATES = [0.001, 0.01]\n",
    "HIDDEN_DIMS = [1000, 500]\n",
    "BATCH_SIZE = 65536\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 25\n",
    "\n",
    "# TODO: Calculate on the fly rather than recalling from previous analysis.\n",
    "MAX_SALES_IN_TRAINING_SET = 38722.0\n",
    "MAX_LOG_SALES_PREDICTION = 1.2 * math.log(MAX_SALES_IN_TRAINING_SET + 1.0)\n",
    "\n",
    "TRAIN_PATHS = sorted(glob.glob(os.path.join(PREPROCESS_DIR_TRAIN, '*.parquet')))\n",
    "VALID_PATHS = sorted(glob.glob(os.path.join(PREPROCESS_DIR_VALID, '*.parquet')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "<a id=\"TensorFlow\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow: Preparing Datasets\n",
    "\n",
    "`KerasSequenceLoader` wraps a lightweight iterator around a `dataset` object to handle chunking, shuffling, and application of any workflows (which can be applied online as a preprocessing step). For column names, can use either a list of string names or a list of TensorFlow `feature_columns` that will be used to feed the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# we can control how much memory to give tensorflow with this environment variable\n",
    "# IMPORTANT: make sure you do this before you initialize TF's runtime, otherwise\n",
    "# it's too late and TF will have claimed all free GPU memory\n",
    "os.environ['TF_MEMORY_ALLOCATION'] = \"8192\" # explicit MB\n",
    "os.environ['TF_MEMORY_ALLOCATION'] = \"0.5\" # fraction of free memory\n",
    "from nvtabular.loader.tensorflow import KerasSequenceLoader, KerasSequenceValidater\n",
    "\n",
    "# cheap wrapper to keep things some semblance of neat\n",
    "def make_categorical_embedding_column(name, dictionary_size, embedding_dim):\n",
    "    return tf.feature_column.embedding_column(\n",
    "        tf.feature_column.categorical_column_with_identity(name, dictionary_size),\n",
    "        embedding_dim\n",
    "    )\n",
    "\n",
    "# instantiate our columns\n",
    "categorical_columns = [\n",
    "    make_categorical_embedding_column(name, *EMBEDDING_TABLE_SHAPES[name]) for\n",
    "        name in CATEGORICAL_COLUMNS\n",
    "]\n",
    "continuous_columns = [\n",
    "    tf.feature_column.numeric_column(name, (1,)) for name in CONTINUOUS_COLUMNS\n",
    "]\n",
    "\n",
    "# feed them to our datasets\n",
    "train_dataset_tf = KerasSequenceLoader(\n",
    "    TRAIN_PATHS, # you could also use a glob pattern\n",
    "    feature_columns=categorical_columns+continuous_columns,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_names=LABEL_COLUMNS,\n",
    "    shuffle=True,\n",
    "    buffer_size=0.06 # amount of data, as a fraction of GPU memory, to load at once\n",
    ")\n",
    "\n",
    "valid_dataset_tf = KerasSequenceLoader(\n",
    "    VALID_PATHS, # you could also use a glob pattern\n",
    "    feature_columns=categorical_columns+continuous_columns,\n",
    "    batch_size=BATCH_SIZE*4,\n",
    "    label_names=LABEL_COLUMNS,\n",
    "    shuffle=False,\n",
    "    buffer_size=0.06 # amount of data, as a fraction of GPU memory, to load at once\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow: Defining a Model\n",
    "\n",
    "Using Keras, we can define the layers of our model and their parameters explicitly. Here, for the sake of consistency, we'll mimic fast.ai's [TabularModel](https://docs.fast.ai/tabular.learner.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseFeatures layer needs a dictionary of {feature_name: input}\n",
    "categorical_inputs = {}\n",
    "for column_name in CATEGORICAL_COLUMNS:\n",
    "    categorical_inputs[column_name] = tf.keras.Input(name=column_name, shape=(1,), dtype=tf.int64)\n",
    "categorical_embedding_layer = tf.keras.layers.DenseFeatures(categorical_columns)\n",
    "categorical_x = categorical_embedding_layer(categorical_inputs)\n",
    "categorical_x = tf.keras.layers.Dropout(EMBEDDING_DROPOUT_RATE)(categorical_x)\n",
    "\n",
    "# Just concatenating continuous, so can use a list\n",
    "continuous_inputs = []\n",
    "for column_name in CONTINUOUS_COLUMNS:\n",
    "    continuous_inputs.append(tf.keras.Input(name=column_name, shape=(1,), dtype=tf.float32))\n",
    "continuous_embedding_layer = tf.keras.layers.Concatenate(axis=1)\n",
    "continuous_x = continuous_embedding_layer(continuous_inputs)\n",
    "continuous_x = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.1)(continuous_x)\n",
    "\n",
    "# concatenate and build MLP\n",
    "x = tf.keras.layers.Concatenate(axis=1)([categorical_x, continuous_x])\n",
    "for dim, dropout_rate in zip(HIDDEN_DIMS, DROPOUT_RATES):\n",
    "    x = tf.keras.layers.Dense(dim, activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.1)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "x = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "# TODO: Initialize model weights to fix saturation issues.\n",
    "# For now, we'll just scale the output of our model directly before\n",
    "# hitting the sigmoid.\n",
    "x = 0.1 * x\n",
    "\n",
    "x = MAX_LOG_SALES_PREDICTION * tf.keras.activations.sigmoid(x)\n",
    "\n",
    "# combine all our inputs into a single list\n",
    "# (note that you can still use .fit, .predict, etc. on a dict\n",
    "# that maps input tensor names to input values)\n",
    "inputs = list(categorical_inputs.values()) + continuous_inputs\n",
    "tf_model = tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe_tf(y_true, y_pred):\n",
    "    # map back into \"true\" space by undoing transform\n",
    "    y_true = tf.exp(y_true) - 1\n",
    "    y_pred = tf.exp(y_pred) - 1\n",
    "\n",
    "    percent_error = (y_true - y_pred) / y_true\n",
    "    return tf.sqrt(tf.reduce_mean(percent_error**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "tf_model.compile(optimizer, 'mse', metrics=[rmspe_tf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 6.0261 - rmspe_tf: 0.8918 - val_loss: 6.7433 - val_rmspe_tf: 0.8898\n",
      "Epoch 2/25\n",
      "13/13 [==============================] - 3s 252ms/step - loss: 5.2557 - rmspe_tf: 0.8882 - val_loss: 6.1526 - val_rmspe_tf: 0.8884\n",
      "Epoch 3/25\n",
      "13/13 [==============================] - 4s 276ms/step - loss: 4.6124 - rmspe_tf: 0.8739 - val_loss: 5.7047 - val_rmspe_tf: 0.8730\n",
      "Epoch 4/25\n",
      "13/13 [==============================] - 3s 243ms/step - loss: 3.7993 - rmspe_tf: 0.8476 - val_loss: 5.1375 - val_rmspe_tf: 0.8461\n",
      "Epoch 5/25\n",
      "13/13 [==============================] - 3s 251ms/step - loss: 2.8152 - rmspe_tf: 0.8009 - val_loss: 4.4700 - val_rmspe_tf: 0.7983\n",
      "Epoch 6/25\n",
      "13/13 [==============================] - 3s 268ms/step - loss: 1.7985 - rmspe_tf: 0.7220 - val_loss: 3.8011 - val_rmspe_tf: 0.7183\n",
      "Epoch 7/25\n",
      "13/13 [==============================] - 3s 251ms/step - loss: 0.9462 - rmspe_tf: 0.6012 - val_loss: 3.2846 - val_rmspe_tf: 0.5937\n",
      "Epoch 8/25\n",
      "13/13 [==============================] - 3s 256ms/step - loss: 0.3964 - rmspe_tf: 0.4462 - val_loss: 2.9589 - val_rmspe_tf: 0.4399\n",
      "Epoch 9/25\n",
      "13/13 [==============================] - 4s 271ms/step - loss: 0.1393 - rmspe_tf: 0.3052 - val_loss: 2.8210 - val_rmspe_tf: 0.3033\n",
      "Epoch 10/25\n",
      "13/13 [==============================] - 3s 263ms/step - loss: 0.0619 - rmspe_tf: 0.2522 - val_loss: 2.8223 - val_rmspe_tf: 0.2534\n",
      "Epoch 11/25\n",
      "13/13 [==============================] - 4s 270ms/step - loss: 0.0455 - rmspe_tf: 0.2537 - val_loss: 2.8013 - val_rmspe_tf: 0.2529\n",
      "Epoch 12/25\n",
      "13/13 [==============================] - 4s 275ms/step - loss: 0.0419 - rmspe_tf: 0.2466 - val_loss: 2.8001 - val_rmspe_tf: 0.2454\n",
      "Epoch 13/25\n",
      "13/13 [==============================] - 3s 249ms/step - loss: 0.0400 - rmspe_tf: 0.2395 - val_loss: 2.8051 - val_rmspe_tf: 0.2394\n",
      "Epoch 14/25\n",
      "13/13 [==============================] - 3s 253ms/step - loss: 0.0379 - rmspe_tf: 0.2290 - val_loss: 2.8039 - val_rmspe_tf: 0.2293\n",
      "Epoch 15/25\n",
      "13/13 [==============================] - 3s 252ms/step - loss: 0.0367 - rmspe_tf: 0.2263 - val_loss: 2.7969 - val_rmspe_tf: 0.2259\n",
      "Epoch 16/25\n",
      "13/13 [==============================] - 3s 249ms/step - loss: 0.0357 - rmspe_tf: 0.2177 - val_loss: 2.7987 - val_rmspe_tf: 0.2179\n",
      "Epoch 17/25\n",
      "13/13 [==============================] - 3s 250ms/step - loss: 0.0348 - rmspe_tf: 0.2145 - val_loss: 2.7951 - val_rmspe_tf: 0.2142\n",
      "Epoch 18/25\n",
      "13/13 [==============================] - 3s 247ms/step - loss: 0.0340 - rmspe_tf: 0.2172 - val_loss: 2.7924 - val_rmspe_tf: 0.2166\n",
      "Epoch 19/25\n",
      "13/13 [==============================] - 3s 254ms/step - loss: 0.0334 - rmspe_tf: 0.2170 - val_loss: 2.7952 - val_rmspe_tf: 0.2163\n",
      "Epoch 20/25\n",
      "13/13 [==============================] - 3s 247ms/step - loss: 0.0329 - rmspe_tf: 0.2099 - val_loss: 2.8036 - val_rmspe_tf: 0.2106\n",
      "Epoch 21/25\n",
      "13/13 [==============================] - 3s 253ms/step - loss: 0.0323 - rmspe_tf: 0.2135 - val_loss: 2.7956 - val_rmspe_tf: 0.2128\n",
      "Epoch 22/25\n",
      "13/13 [==============================] - 3s 248ms/step - loss: 0.0316 - rmspe_tf: 0.2069 - val_loss: 2.7989 - val_rmspe_tf: 0.2073\n",
      "Epoch 23/25\n",
      "13/13 [==============================] - 3s 246ms/step - loss: 0.0304 - rmspe_tf: 0.2060 - val_loss: 2.7863 - val_rmspe_tf: 0.2051\n",
      "Epoch 24/25\n",
      "13/13 [==============================] - 3s 216ms/step - loss: 0.0296 - rmspe_tf: 0.1995 - val_loss: 2.7899 - val_rmspe_tf: 0.1992\n",
      "Epoch 25/25\n",
      "13/13 [==============================] - 3s 235ms/step - loss: 0.0290 - rmspe_tf: 0.1993 - val_loss: 2.7972 - val_rmspe_tf: 0.1997\n",
      "CPU times: user 3min 37s, sys: 23.3 s, total: 4min\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "tf_model.compile(optimizer, 'mse', metrics=[rmspe_tf])\n",
    "\n",
    "validation_callback = KerasSequenceValidater(valid_dataset_tf)\n",
    "history = tf_model.fit(\n",
    "    train_dataset_tf,\n",
    "    callbacks=[validation_callback],\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
