{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# NVTabular demo on Rossmann data\n",
    "\n",
    "## Overview\n",
    "\n",
    "NVTabular is a feature engineering and preprocessing library for tabular data designed to quickly and easily manipulate terabyte scale datasets used to train deep learning based recommender systems.  It provides a high level abstraction to simplify code and accelerates computation on the GPU using the RAPIDS cuDF library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "\n",
    "This notebook demonstrates the steps for carrying out data preprocessing, transformation and loading with NVTabular on the Kaggle Rossmann [dataset](https://www.kaggle.com/c/rossmann-store-sales/overview).  Rossmann operates over 3,000 drug stores in 7 European countries. Historical sales data for 1,115 Rossmann stores are provided. The task is to forecast the \"Sales\" column for the test set. \n",
    "\n",
    "The following example will illustrate how to use NVTabular to preprocess and load tabular data for training neural networks in both PyTorch and TensorFlow. We'll use a [dataset built by FastAI](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb) for solving the [Kaggle Rossmann Store Sales competition](https://www.kaggle.com/c/rossmann-store-sales). Some pandas preprocessing is required to build the appropriate feature set, so make sure to run [rossmann-store-sales-preproc.ipynb](./rossmann-store-sales-preproc.ipynb) first before going through this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/conda/envs/rapids/lib/python3.7/site-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_NVVM=/usr/local/cuda/nvvm/lib64/libnvvm.so.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n",
      "/conda/envs/rapids/lib/python3.7/site-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_LIBDEVICE=/usr/local/cuda/nvvm/libdevice/.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nvtabular as nvt\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Fix so that these warnings never arise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our dataset\n",
    "Let's start by defining some of the a priori information about our data, including its schema (what columns to use and what sorts of variables they represent), as well as the location of the files corresponding to some particular sampling from this schema. Note that throughout, I'll use UPPERCASE variables to represent this sort of a priori information that you might usually encode using commandline arguments or config files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.environ.get('DATA_DIR', './data')\n",
    "\n",
    "CATEGORICAL_COLUMNS = [\n",
    "    'Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
    "    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
    "    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
    "    'SchoolHoliday_fw', 'SchoolHoliday_bw'\n",
    "]\n",
    "\n",
    "CONTINUOUS_COLUMNS = [\n",
    "    'CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
    "   'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "   'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "   'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday'\n",
    "]\n",
    "LABEL_COLUMNS = ['Sales']\n",
    "\n",
    "COLUMNS = CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS + LABEL_COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What files are available to train on in our data directory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jp_ross  test.csv  train.csv  valid.csv\n"
     ]
    }
   ],
   "source": [
    "! ls $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train.csv` and `valid.csv` seem like good candidates, let's use those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = os.path.join(DATA_DIR, 'train.csv')\n",
    "VALID_PATH = os.path.join(DATA_DIR, 'valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflows and Preprocessing\n",
    "A `Workflow` is used to represent the chains of feature engineering and preprocessing operations performed on a dataset, and is instantiated with a description of the dataset's schema so that it can keep track of how columns transform with each operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that here, we want to perform a normalization transformation on the label\n",
    "# column. Since NVT doesn't support transforming label columns right now, we'll\n",
    "# pretend it's a regular continuous column during our feature engineering phase\n",
    "proc = nvt.Workflow(\n",
    "    cat_names=CATEGORICAL_COLUMNS,\n",
    "    cont_names=CONTINUOUS_COLUMNS,\n",
    "    label_name=LABEL_COLUMNS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ops\n",
    "We add operations to a `Workflow` by leveraging the `add_(cat|cont)_feature` and `add_(cat|cont)_preprocess` methods for categorical and continuous variables, respectively. When we're done adding ops, we call the `finalize` method to let the `Workflow` build  a representation of its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.add_cont_feature(nvt.ops.FillMissing())\n",
    "proc.add_cont_preprocess(nvt.ops.LogOp(columns=['Sales']))\n",
    "proc.add_cont_preprocess(nvt.ops.Normalize())\n",
    "proc.add_cat_preprocess(nvt.ops.Categorify())\n",
    "proc.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "In general, the `Op`s in our `Workflow` will require measurements of statistical properties of our data in order to be leveraged. For example, the `Normalize` op requires measurements of the dataset mean and standard deviation, and the `Categorify` op requires an accounting of all the categories a particular feature can manifest. However, we frequently need to measure these properties across datasets which are too large to fit into GPU memory (or CPU memory for that matter) at once.\n",
    "\n",
    "NVTabular solves this by providing the `dataset` object, an iterator over manageable chunks of sets of parquet or csv files that can we can use to compute statistics in an online fashion (and, later, to train neural networks in batches loaded from disk). The size of those chunks will be determined by the `gpu_memory_frac` kwarg, which will load chunks whose memory footprint is equal to that fraction of available GPU memory.\n",
    "\n",
    "Larger chunks will lead to shorter run times due to the parallel-processing power of GPUs, but will constrain your memory and possibly lead to disk caching by expensive operations, thereby lowering efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_MEMORY_FRAC = 0.2\n",
    "train_dataset = nvt.Dataset(TRAIN_PATH, gpu_memory_frac=GPU_MEMORY_FRAC, columns=COLUMNS)\n",
    "valid_dataset = nvt.Dataset(VALID_PATH, gpu_memory_frac=GPU_MEMORY_FRAC, columns=COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our datasets, we'll apply our `Workflow` to them and save the results out to parquet files for fast reading at train time. We'll also measure and record statistics on our training set using the `record_stats=True` kwarg so that our `Workflow` can use them at apply time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESS_DIR = os.path.join(DATA_DIR, 'jp_ross')\n",
    "PREPROCESS_DIR_TRAIN = os.path.join(PREPROCESS_DIR, 'train')\n",
    "PREPROCESS_DIR_VALID = os.path.join(PREPROCESS_DIR, 'valid')\n",
    "\n",
    "! rm -rf $PREPROCESS_DIR # remove previous trials\n",
    "! mkdir -p $PREPROCESS_DIR_TRAIN\n",
    "! mkdir -p $PREPROCESS_DIR_VALID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.apply(train_dataset, apply_offline=True, record_stats=True, output_path=PREPROCESS_DIR_TRAIN, shuffle=True, out_files_per_proc=2)\n",
    "proc.apply(valid_dataset, apply_offline=True, record_stats=False, output_path=PREPROCESS_DIR_VALID, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Describe `apply_offline` and `record_stats`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize columns\n",
    "The FastAI workflow will leverage the `Workflow.ds_to_tensors` method, which will map a dataset to its corresponding PyTorch tensors. In order to make sure it runs correctly, we'll call the `create_final_cols` method to let the `Workflow` know to build the output dataset schema, and then we'll be sure to remove instances of the label column that got added to that schema when we performed processing on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.create_final_cols()\n",
    "# using log op and normalize on sales column causes it to get added to\n",
    "# continuous columns_ctx, so we'll remove it here\n",
    "while True:\n",
    "    try:\n",
    "        proc.columns_ctx['final']['cols']['continuous'].remove(LABEL_COLUMNS[0])\n",
    "    except ValueError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Network\n",
    "\n",
    "Now that our data is preprocessed and saved out, we can leverage `dataset`s to read through the preprocessed parquet files in an online fashion to train neural networks.\n",
    "\n",
    "We'll start by setting some universal hyperparameters for our model and optimizer. These settings will be shared across all of the frameworks that we explore below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Improve RMSPE to come closer to SoTA (approx. 9% RMSPE on the public Kaggle leaderboard)\n",
    "- using every framework explored in this notebook\n",
    "- while maintaining a large batch size (65536)\n",
    "\n",
    "If you're interested in contributing to NVTabular, feel free to take this challenge on and submit a pull request if successful. 12% RMSPE is achievable using the Novograd optimizer, but we know of no Novograd implementation for TensorFlow that supports sparse gradients, and so we are not including that solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DROPOUT_RATE = 0.04\n",
    "DROPOUT_RATES = [0.001, 0.01]\n",
    "HIDDEN_DIMS = [1000, 500]\n",
    "BATCH_SIZE = 65536\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 25\n",
    "\n",
    "# TODO: Calculate on the fly rather than recalling from previous analysis.\n",
    "MAX_SALES_IN_TRAINING_SET = 38722.0\n",
    "MAX_LOG_SALES_PREDICTION = 1.2 * np.log(MAX_SALES_IN_TRAINING_SET + 1.0)\n",
    "\n",
    "# It's possible to use defaults defined within NVTabular.\n",
    "EMBEDDING_TABLE_SHAPES = {\n",
    "    column: shape for column, shape in\n",
    "        nvt.ops.get_embedding_sizes(proc).items()\n",
    "}\n",
    "\n",
    "# Here, however, we will use fast.ai's rule for embedding sizes.\n",
    "for col in EMBEDDING_TABLE_SHAPES:\n",
    "    EMBEDDING_TABLE_SHAPES[col] = (EMBEDDING_TABLE_SHAPES[col][0], min(600, round(1.6 * EMBEDDING_TABLE_SHAPES[col][0] ** 0.56)))\n",
    "\n",
    "TRAIN_PATHS = sorted(glob.glob(os.path.join(PREPROCESS_DIR_TRAIN, '*.parquet')))\n",
    "VALID_PATHS = sorted(glob.glob(os.path.join(PREPROCESS_DIR_VALID, '*.parquet')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the cardinality of each categorical variable along with its associated embedding size. Each entry is of the form `(cardinality, embedding_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Assortment': (4, 3),\n",
       " 'CompetitionMonthsOpen': (26, 10),\n",
       " 'CompetitionOpenSinceYear': (24, 9),\n",
       " 'Day': (32, 11),\n",
       " 'DayOfWeek': (8, 5),\n",
       " 'Events': (22, 9),\n",
       " 'Month': (13, 7),\n",
       " 'Promo2SinceYear': (9, 5),\n",
       " 'Promo2Weeks': (27, 10),\n",
       " 'PromoInterval': (4, 3),\n",
       " 'Promo_bw': (7, 5),\n",
       " 'Promo_fw': (7, 5),\n",
       " 'SchoolHoliday_bw': (9, 5),\n",
       " 'SchoolHoliday_fw': (9, 5),\n",
       " 'State': (13, 7),\n",
       " 'StateHoliday': (3, 3),\n",
       " 'StateHoliday_bw': (4, 3),\n",
       " 'StateHoliday_fw': (4, 3),\n",
       " 'Store': (1116, 81),\n",
       " 'StoreType': (5, 4),\n",
       " 'Week': (53, 15),\n",
       " 'Year': (4, 3)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_TABLE_SHAPES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose a Framework\n",
    "\n",
    "We're now ready to move on to framework-specific code.\n",
    "\n",
    "**The code for each framework can be run independently of the others, so feel free to skip to your framework of choice.**\n",
    "\n",
    "### [TensorFlow](#tensorflow)\n",
    "### [PyTorch](#pytorch)\n",
    "### [fast.ai](#fastai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tensorflow\"></a>\n",
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow: Preparing Datasets\n",
    "\n",
    "`KerasSequenceDataset` wraps a lightweight iterator around a `dataset` object to handle chunking, shuffling, and application of any workflows (which can be applied online as a preprocessing step). For column names, can use either a list of string names or a list of TensorFlow `feature_columns` that will be used to feed the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# we can control how much memory to give tensorflow with this environment variable\n",
    "# IMPORTANT: make sure you do this before you initialize TF's runtime, otherwise\n",
    "# it's too late and TF will have claimed all free GPU memory\n",
    "os.environ['TF_MEMORY_ALLOCATION'] = \"8192\" # explicit MB\n",
    "os.environ['TF_MEMORY_ALLOCATION'] = \"0.5\" # fraction of free memory\n",
    "from nvtabular.tf_dataloader import KerasSequenceDataset\n",
    "\n",
    "# cheap wrapper to keep things some semblance of neat\n",
    "def make_categorical_embedding_column(name, dictionary_size, embedding_dim):\n",
    "    return tf.feature_column.embedding_column(\n",
    "        tf.feature_column.categorical_column_with_identity(name, dictionary_size),\n",
    "        embedding_dim\n",
    "    )\n",
    "\n",
    "# instantiate our columns\n",
    "categorical_columns = [\n",
    "    make_categorical_embedding_column(name, *EMBEDDING_TABLE_SHAPES[name]) for\n",
    "        name in CATEGORICAL_COLUMNS\n",
    "]\n",
    "continuous_columns = [\n",
    "    tf.feature_column.numeric_column(name, (1,)) for name in CONTINUOUS_COLUMNS\n",
    "]\n",
    "\n",
    "# feed them to our datasets\n",
    "train_dataset_tf = KerasSequenceDataset(\n",
    "    TRAIN_PATHS, # you could also use a glob pattern\n",
    "    categorical_columns+continuous_columns,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_name=LABEL_COLUMNS[0],\n",
    "    shuffle=True,\n",
    "    buffer_size=48 # how many batches to load at once\n",
    ")\n",
    "valid_dataset_tf = KerasSequenceDataset(\n",
    "    VALID_PATHS, # you could also use a glob pattern\n",
    "    categorical_columns+continuous_columns,\n",
    "    batch_size=BATCH_SIZE*4,\n",
    "    label_name=LABEL_COLUMNS[0],\n",
    "    shuffle=False,\n",
    "    buffer_size=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow: Defining a Model\n",
    "\n",
    "Using Keras, we can define the layers of our model and their parameters explicitly. Here, for the sake of consistency, we'll mimic fast.ai's [TabularModel](https://docs.fast.ai/tabular.models.html#TabularModel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseFeatures layer needs a dictionary of {feature_name: input}\n",
    "categorical_inputs = {}\n",
    "for column_name in CATEGORICAL_COLUMNS:\n",
    "    categorical_inputs[column_name] = tf.keras.Input(name=column_name, shape=(1,), dtype=tf.int64)\n",
    "categorical_embedding_layer = tf.keras.layers.DenseFeatures(categorical_columns)\n",
    "categorical_x = categorical_embedding_layer(categorical_inputs)\n",
    "categorical_x = tf.keras.layers.Dropout(EMBEDDING_DROPOUT_RATE)(categorical_x)\n",
    "\n",
    "# Just concatenating continuous, so can use a list\n",
    "continuous_inputs = []\n",
    "for column_name in CONTINUOUS_COLUMNS:\n",
    "    continuous_inputs.append(tf.keras.Input(name=column_name, shape=(1,), dtype=tf.float32))\n",
    "continuous_embedding_layer = tf.keras.layers.Concatenate(axis=1)\n",
    "continuous_x = continuous_embedding_layer(continuous_inputs)\n",
    "continuous_x = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.1)(continuous_x)\n",
    "\n",
    "# concatenate and build MLP\n",
    "x = tf.keras.layers.Concatenate(axis=1)([categorical_x, continuous_x])\n",
    "for dim, dropout_rate in zip(HIDDEN_DIMS, DROPOUT_RATES):\n",
    "    x = tf.keras.layers.Dense(dim, activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.1)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "x = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "# TODO: Initialize model weights to fix saturation issues.\n",
    "# For now, we'll just scale the output of our model directly before\n",
    "# hitting the sigmoid.\n",
    "x = 0.1 * x\n",
    "\n",
    "x = MAX_LOG_SALES_PREDICTION * tf.keras.activations.sigmoid(x)\n",
    "\n",
    "# combine all our inputs into a single list\n",
    "# (note that you can still use .fit, .predict, etc. on a dict\n",
    "# that maps input tensor names to input values)\n",
    "inputs = list(categorical_inputs.values()) + continuous_inputs\n",
    "tf_model = tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe_tf(y_true, y_pred):\n",
    "    # map back into \"true\" space by undoing transform\n",
    "    y_true = tf.exp(y_true) - 1\n",
    "    y_pred = tf.exp(y_pred) - 1\n",
    "\n",
    "    percent_error = (y_true - y_pred) / y_true\n",
    "    return tf.sqrt(tf.reduce_mean(percent_error**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 13 steps, validate for 1 steps\n",
      "Epoch 1/25\n",
      "13/13 [==============================] - 10s 732ms/step - loss: 5.9483 - rmspe_tf: 0.8932 - val_loss: 5.3924 - val_rmspe_tf: 0.8932\n",
      "Epoch 2/25\n",
      "13/13 [==============================] - 2s 186ms/step - loss: 5.2029 - rmspe_tf: 0.8900 - val_loss: 5.7012 - val_rmspe_tf: 0.9032\n",
      "Epoch 3/25\n",
      "13/13 [==============================] - 2s 175ms/step - loss: 4.5385 - rmspe_tf: 0.8751 - val_loss: 4.2585 - val_rmspe_tf: 0.8654\n",
      "Epoch 4/25\n",
      "13/13 [==============================] - 2s 170ms/step - loss: 3.6924 - rmspe_tf: 0.8470 - val_loss: 3.9164 - val_rmspe_tf: 0.8545\n",
      "Epoch 5/25\n",
      "13/13 [==============================] - 3s 208ms/step - loss: 2.6890 - rmspe_tf: 0.7974 - val_loss: 2.7136 - val_rmspe_tf: 0.7926\n",
      "Epoch 6/25\n",
      "13/13 [==============================] - 3s 203ms/step - loss: 1.6778 - rmspe_tf: 0.7113 - val_loss: 1.5175 - val_rmspe_tf: 0.6895\n",
      "Epoch 7/25\n",
      "13/13 [==============================] - 2s 168ms/step - loss: 0.8746 - rmspe_tf: 0.5828 - val_loss: 0.6695 - val_rmspe_tf: 0.5370\n",
      "Epoch 8/25\n",
      "13/13 [==============================] - 2s 153ms/step - loss: 0.3822 - rmspe_tf: 0.4369 - val_loss: 0.1702 - val_rmspe_tf: 0.3515\n",
      "Epoch 9/25\n",
      "13/13 [==============================] - 2s 177ms/step - loss: 0.1623 - rmspe_tf: 0.3326 - val_loss: 0.1415 - val_rmspe_tf: 0.3576\n",
      "Epoch 10/25\n",
      "13/13 [==============================] - 2s 173ms/step - loss: 0.0935 - rmspe_tf: 0.3496 - val_loss: 0.1343 - val_rmspe_tf: 0.3473\n",
      "Epoch 11/25\n",
      "13/13 [==============================] - 2s 171ms/step - loss: 0.0889 - rmspe_tf: 0.3338 - val_loss: 0.0728 - val_rmspe_tf: 0.3115\n",
      "Epoch 12/25\n",
      "13/13 [==============================] - 2s 175ms/step - loss: 0.0956 - rmspe_tf: 0.3557 - val_loss: 0.1635 - val_rmspe_tf: 0.5155\n",
      "Epoch 13/25\n",
      "13/13 [==============================] - 2s 173ms/step - loss: 0.0902 - rmspe_tf: 0.3825 - val_loss: 0.1007 - val_rmspe_tf: 0.4128\n",
      "Epoch 14/25\n",
      "13/13 [==============================] - 2s 178ms/step - loss: 0.0713 - rmspe_tf: 0.3224 - val_loss: 0.0898 - val_rmspe_tf: 0.3835\n",
      "Epoch 15/25\n",
      "13/13 [==============================] - 2s 156ms/step - loss: 0.0644 - rmspe_tf: 0.2974 - val_loss: 0.0753 - val_rmspe_tf: 0.3369\n",
      "Epoch 16/25\n",
      "13/13 [==============================] - 2s 184ms/step - loss: 0.0669 - rmspe_tf: 0.3177 - val_loss: 0.0814 - val_rmspe_tf: 0.3377\n",
      "Epoch 17/25\n",
      "13/13 [==============================] - 2s 178ms/step - loss: 0.0669 - rmspe_tf: 0.3943 - val_loss: 0.0509 - val_rmspe_tf: 0.2348\n",
      "Epoch 18/25\n",
      "13/13 [==============================] - 2s 172ms/step - loss: 0.0505 - rmspe_tf: 0.2618 - val_loss: 0.0550 - val_rmspe_tf: 0.2158\n",
      "Epoch 19/25\n",
      "13/13 [==============================] - 2s 172ms/step - loss: 0.0475 - rmspe_tf: 0.2434 - val_loss: 0.0377 - val_rmspe_tf: 0.2117\n",
      "Epoch 20/25\n",
      "13/13 [==============================] - 2s 172ms/step - loss: 0.0472 - rmspe_tf: 0.2711 - val_loss: 0.0407 - val_rmspe_tf: 0.2258\n",
      "Epoch 21/25\n",
      "13/13 [==============================] - 2s 171ms/step - loss: 0.0460 - rmspe_tf: 0.2395 - val_loss: 0.0517 - val_rmspe_tf: 0.2394\n",
      "Epoch 22/25\n",
      "13/13 [==============================] - 2s 160ms/step - loss: 0.0426 - rmspe_tf: 0.2419 - val_loss: 0.0591 - val_rmspe_tf: 0.2238\n",
      "Epoch 23/25\n",
      "13/13 [==============================] - 2s 179ms/step - loss: 0.0404 - rmspe_tf: 0.2369 - val_loss: 0.1375 - val_rmspe_tf: 0.2947\n",
      "Epoch 24/25\n",
      "13/13 [==============================] - 2s 176ms/step - loss: 0.0401 - rmspe_tf: 0.2360 - val_loss: 0.1275 - val_rmspe_tf: 0.2890\n",
      "Epoch 25/25\n",
      "13/13 [==============================] - 2s 170ms/step - loss: 0.0451 - rmspe_tf: 0.2548 - val_loss: 0.1022 - val_rmspe_tf: 0.2793\n",
      "CPU times: user 2min 35s, sys: 29.6 s, total: 3min 5s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "tf_model.compile(optimizer, 'mse', metrics=[rmspe_tf])\n",
    "history = tf_model.fit(\n",
    "    train_dataset_tf,\n",
    "    validation_data=valid_dataset_tf,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pytorch\"></a>\n",
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Preparing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nvtabular.torch_dataloader import TensorItrDataset, DLDataLoader\n",
    "\n",
    "# TensorItrDataset returns a single batch of x_cat, x_cont, y.\n",
    "collate_fn = lambda x: x\n",
    "\n",
    "train_nvt_dataset = nvt.Dataset(TRAIN_PATHS)\n",
    "train_x_cat, train_x_cont, train_y = proc.ds_to_tensors(train_nvt_dataset.to_iter(columns=COLUMNS), apply_ops=False)\n",
    "train_dataset = TensorItrDataset((train_x_cat, train_x_cont, train_y), batch_size=BATCH_SIZE)\n",
    "train_loader = DLDataLoader(train_dataset, batch_size=None, collate_fn=collate_fn, pin_memory=False, num_workers=0)\n",
    "\n",
    "valid_nvt_dataset = nvt.Dataset(VALID_PATHS)\n",
    "valid_x_cat, valid_x_cont, valid_y = proc.ds_to_tensors(valid_nvt_dataset.to_iter(columns=COLUMNS), apply_ops=False)\n",
    "valid_dataset = TensorItrDataset((valid_x_cat, valid_x_cont, valid_y), batch_size=BATCH_SIZE)\n",
    "valid_loader = DLDataLoader(valid_dataset, batch_size=None, collate_fn=collate_fn, pin_memory=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Defining a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatenatedEmbeddings(torch.nn.Module):\n",
    "    \"\"\"Map multiple categorical variables to concatenated embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embedding_table_shapes: A dictionary mapping column names to\n",
    "            (cardinality, embedding_size) tuples.\n",
    "        dropout: A float.\n",
    "    \n",
    "    Inputs:\n",
    "        x: An int64 Tensor with shape [batch_size, num_variables].\n",
    "    \n",
    "    Outputs:\n",
    "        A Float Tensor with shape [batch_size, embedding_size_after_concat].\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_table_shapes, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embedding_layers = torch.nn.ModuleList([\n",
    "            torch.nn.Embedding(cat_size, emb_size)\n",
    "            for cat_size, emb_size in embedding_table_shapes.values()\n",
    "        ])\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = [layer(x[:, i]) for i, layer in enumerate(self.embedding_layers)]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_table_shapes, num_continuous,\n",
    "                 emb_dropout, layer_hidden_dims, layer_dropout_rates, max_output=None):\n",
    "        super().__init__()\n",
    "        self.max_output = max_output\n",
    "        self.initial_cat_layer = ConcatenatedEmbeddings(embedding_table_shapes, dropout=emb_dropout)\n",
    "        self.initial_cont_layer = torch.nn.BatchNorm1d(num_continuous)\n",
    "        \n",
    "        embedding_size = sum(emb_size for _, emb_size in embedding_table_shapes.values())\n",
    "        layer_input_sizes = [embedding_size + num_continuous] + layer_hidden_dims[:-1]\n",
    "        layer_output_sizes = layer_hidden_dims\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(input_size, output_size),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.BatchNorm1d(output_size),\n",
    "                torch.nn.Dropout(dropout_rate)\n",
    "            )\n",
    "            for input_size, output_size, dropout_rate\n",
    "            in zip(layer_input_sizes, layer_output_sizes, layer_dropout_rates)\n",
    "        )\n",
    "        \n",
    "        self.output_layer = torch.nn.Linear(layer_output_sizes[-1], 1)\n",
    "        \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x_cat = self.initial_cat_layer(x_cat)\n",
    "        x_cont = self.initial_cont_layer(x_cont)\n",
    "        x = torch.cat([x_cat, x_cont], dim=1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        if self.max_output:\n",
    "            x = self.max_output * torch.sigmoid(x)\n",
    "        x = x.view(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    embedding_table_shapes=EMBEDDING_TABLE_SHAPES,\n",
    "    num_continuous=len(CONTINUOUS_COLUMNS),\n",
    "    emb_dropout=EMBEDDING_DROPOUT_RATE,\n",
    "    layer_hidden_dims=HIDDEN_DIMS,\n",
    "    layer_dropout_rates=DROPOUT_RATES,\n",
    "    max_output=MAX_LOG_SALES_PREDICTION\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe_func(y_pred, y):\n",
    "    \"Return y_pred and y to non-log space and compute RMSPE\"\n",
    "    y_pred, y = torch.exp(y_pred) - 1, torch.exp(y) - 1\n",
    "    pct_var = (y_pred - y) / y\n",
    "    return (pct_var**2).mean().pow(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def process_epoch(dataloader, model, train=False, optimizer=None):\n",
    "    model.train(mode=train)\n",
    "    with torch.set_grad_enabled(train):\n",
    "        y_list, y_pred_list = [], []\n",
    "        for x_cat, x_cont, y in iter(dataloader):\n",
    "            y_list.append(y.detach())\n",
    "            y_pred = model(x_cat, x_cont)\n",
    "            y_pred_list.append(y_pred.detach())\n",
    "            loss = loss_func(y_pred, y)\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    y = torch.cat(y_list)\n",
    "    y_pred = torch.cat(y_pred_list)\n",
    "    epoch_loss = loss_func(y_pred, y).item()\n",
    "    epoch_rmspe = rmspe_func(y_pred, y).item()\n",
    "    return epoch_loss, epoch_rmspe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00. Train loss: 8.8671. Train RMSPE: 2.2987. Valid loss: 2.6919. Valid RMSPE: 0.7670.\n",
      "Epoch 01. Train loss: 4.8595. Train RMSPE: 0.8224. Valid loss: 4.6312. Valid RMSPE: 0.8517.\n",
      "Epoch 02. Train loss: 3.3106. Train RMSPE: 0.7901. Valid loss: 2.5927. Valid RMSPE: 0.7643.\n",
      "Epoch 03. Train loss: 2.2005. Train RMSPE: 0.7283. Valid loss: 1.3815. Valid RMSPE: 0.6430.\n",
      "Epoch 04. Train loss: 1.1530. Train RMSPE: 0.6067. Valid loss: 0.7933. Valid RMSPE: 0.5360.\n",
      "Epoch 05. Train loss: 0.5101. Train RMSPE: 0.5166. Valid loss: 0.4006. Valid RMSPE: 0.4706.\n",
      "Epoch 06. Train loss: 0.3265. Train RMSPE: 0.6420. Valid loss: 0.2997. Valid RMSPE: 0.5653.\n",
      "Epoch 07. Train loss: 0.2560. Train RMSPE: 0.6616. Valid loss: 0.2246. Valid RMSPE: 0.4996.\n",
      "Epoch 08. Train loss: 0.2240. Train RMSPE: 0.6011. Valid loss: 0.2566. Valid RMSPE: 0.4292.\n",
      "Epoch 09. Train loss: 0.2055. Train RMSPE: 0.5521. Valid loss: 0.3827. Valid RMSPE: 0.4276.\n",
      "Epoch 10. Train loss: 0.2054. Train RMSPE: 0.5306. Valid loss: 0.2646. Valid RMSPE: 0.4329.\n",
      "Epoch 11. Train loss: 0.2285. Train RMSPE: 0.5574. Valid loss: 0.4055. Valid RMSPE: 1.1187.\n",
      "Epoch 12. Train loss: 0.2990. Train RMSPE: 0.6968. Valid loss: 0.5344. Valid RMSPE: 1.2761.\n",
      "Epoch 13. Train loss: 0.3549. Train RMSPE: 0.7541. Valid loss: 0.1648. Valid RMSPE: 0.4232.\n",
      "Epoch 14. Train loss: 0.2552. Train RMSPE: 0.6286. Valid loss: 0.1397. Valid RMSPE: 0.3529.\n",
      "Epoch 15. Train loss: 0.1621. Train RMSPE: 0.4753. Valid loss: 0.1395. Valid RMSPE: 0.3302.\n",
      "Epoch 16. Train loss: 0.1341. Train RMSPE: 0.4260. Valid loss: 0.1196. Valid RMSPE: 0.3466.\n",
      "Epoch 17. Train loss: 0.1279. Train RMSPE: 0.4183. Valid loss: 0.1381. Valid RMSPE: 0.3936.\n",
      "Epoch 18. Train loss: 0.1313. Train RMSPE: 0.4225. Valid loss: 0.2062. Valid RMSPE: 0.5715.\n",
      "Epoch 19. Train loss: 0.1645. Train RMSPE: 0.4955. Valid loss: 0.4183. Valid RMSPE: 0.9371.\n",
      "Epoch 20. Train loss: 0.2715. Train RMSPE: 0.6390. Valid loss: 0.8459. Valid RMSPE: 1.5637.\n",
      "Epoch 21. Train loss: 0.4677. Train RMSPE: 0.9903. Valid loss: 0.1733. Valid RMSPE: 0.4707.\n",
      "Epoch 22. Train loss: 0.2436. Train RMSPE: 0.6403. Valid loss: 0.1375. Valid RMSPE: 0.3668.\n",
      "Epoch 23. Train loss: 0.1169. Train RMSPE: 0.3966. Valid loss: 0.0825. Valid RMSPE: 0.2943.\n",
      "Epoch 24. Train loss: 0.0938. Train RMSPE: 0.3569. Valid loss: 0.0736. Valid RMSPE: 0.2879.\n",
      "CPU times: user 22.8 s, sys: 14.5 s, total: 37.3 s\n",
      "Wall time: 37.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_rmspe = process_epoch(train_loader, model, train=True, optimizer=optimizer)\n",
    "    valid_loss, valid_rmspe = process_epoch(valid_loader, model, train=False)\n",
    "    print(f'Epoch {epoch:02d}. Train loss: {train_loss:.4f}. Train RMSPE: {train_rmspe:.4f}. Valid loss: {valid_loss:.4f}. Valid RMSPE: {valid_rmspe:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fastai\"></a>\n",
    "## fast.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fast.ai: Preparing Datasets\n",
    "\n",
    "`workflow.ds_to_tensors` maps a symbolic dataset object to `cat_features`, `cont_features`, `labels` PyTorch tenosrs by iterating through the dataset and concatenating the results. Note that this means that the whole of the dataset is _in memory_. For larger than memory datasets, see the example in [criteo-example.ipynb](./criteo-example.ipynb) leveraing PyTorch `ChainDataset`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nvtabular.torch_dataloader import TensorItrDataset, DLDataLoader\n",
    "from fastai.basic_data import DataBunch\n",
    "from fastai.tabular import TabularModel\n",
    "from fastai.basic_train import Learner\n",
    "from fastai.layers import MSELossFlat\n",
    "\n",
    "def make_batched_dataloader(paths, columns, batch_size):\n",
    "    dataset = nvt.Dataset(paths)\n",
    "    ds_tensors = proc.ds_to_tensors(dataset.to_iter(columns=columns), apply_ops=False)\n",
    "    ds_batch_sets = TensorItrDataset(ds_tensors, batch_size=batch_size)\n",
    "    return DLDataLoader(\n",
    "        ds_batch_sets,\n",
    "        batch_size=None,\n",
    "        pin_memory=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "# Our examples are of the form (cat, cont, label) whereas fast.ai\n",
    "# expects ((cat, cont), label).\n",
    "def collate_fn(x):\n",
    "    return (x[0], x[1]), x[2]\n",
    "\n",
    "train_dataset_pt = make_batched_dataloader(TRAIN_PATHS, COLUMNS, BATCH_SIZE)\n",
    "valid_dataset_pt = make_batched_dataloader(VALID_PATHS, COLUMNS, BATCH_SIZE*4)\n",
    "databunch = DataBunch(\n",
    "    train_dataset_pt,\n",
    "    valid_dataset_pt,\n",
    "    collate_fn=collate_fn,\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fast.ai: Defining a Model\n",
    "\n",
    "Next we'll need to define the inputs that will feed our model and build an architecture on top of them. For now, we'll just stick to a simple MLP model.\n",
    "\n",
    "Using FastAI's `TabularModel`, we can build an MLP under the hood by defining its high-level characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model = TabularModel(\n",
    "    emb_szs=list(EMBEDDING_TABLE_SHAPES.values()),\n",
    "    n_cont=len(CONTINUOUS_COLUMNS),\n",
    "    out_sz=1,\n",
    "    layers=HIDDEN_DIMS,\n",
    "    ps=DROPOUT_RATES,\n",
    "    use_bn=True,\n",
    "    emb_drop=EMBEDDING_DROPOUT_RATE,\n",
    "    y_range=torch.tensor([0.0, MAX_LOG_SALES_PREDICTION]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fast.ai: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>exp_rmspe</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>18.209251</td>\n",
       "      <td>9.415349</td>\n",
       "      <td>0.936290</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11.693658</td>\n",
       "      <td>3.877726</td>\n",
       "      <td>0.828576</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.320545</td>\n",
       "      <td>2.508565</td>\n",
       "      <td>0.745758</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.207951</td>\n",
       "      <td>1.004673</td>\n",
       "      <td>0.588488</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.637287</td>\n",
       "      <td>0.583443</td>\n",
       "      <td>0.499237</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.428541</td>\n",
       "      <td>0.161076</td>\n",
       "      <td>0.366772</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.535358</td>\n",
       "      <td>0.165929</td>\n",
       "      <td>0.542384</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.897769</td>\n",
       "      <td>0.212175</td>\n",
       "      <td>0.649155</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.441149</td>\n",
       "      <td>0.226141</td>\n",
       "      <td>0.678329</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.110257</td>\n",
       "      <td>0.205618</td>\n",
       "      <td>0.624237</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.867041</td>\n",
       "      <td>0.182241</td>\n",
       "      <td>0.502908</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.686771</td>\n",
       "      <td>0.180979</td>\n",
       "      <td>0.475812</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.553056</td>\n",
       "      <td>0.172989</td>\n",
       "      <td>0.433415</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.454524</td>\n",
       "      <td>0.166232</td>\n",
       "      <td>0.403378</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.382017</td>\n",
       "      <td>0.176784</td>\n",
       "      <td>0.373490</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.329587</td>\n",
       "      <td>0.274356</td>\n",
       "      <td>0.385104</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.293651</td>\n",
       "      <td>0.523390</td>\n",
       "      <td>0.462732</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.285270</td>\n",
       "      <td>0.636179</td>\n",
       "      <td>0.505070</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.317515</td>\n",
       "      <td>0.182834</td>\n",
       "      <td>0.453213</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.338240</td>\n",
       "      <td>1.073343</td>\n",
       "      <td>2.473520</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.324682</td>\n",
       "      <td>0.437561</td>\n",
       "      <td>1.450772</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.300302</td>\n",
       "      <td>0.232063</td>\n",
       "      <td>0.654905</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.271814</td>\n",
       "      <td>0.298693</td>\n",
       "      <td>0.945147</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.252695</td>\n",
       "      <td>0.120357</td>\n",
       "      <td>0.393314</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.231011</td>\n",
       "      <td>0.167242</td>\n",
       "      <td>0.329558</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25 s, sys: 14 s, total: 39 s\n",
      "Wall time: 37.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from fastai.basic_data import DatasetType\n",
    "from fastai.torch_core import flatten_check\n",
    "\n",
    "def exp_rmspe(pred, targ):\n",
    "    \"Exp RMSE between `pred` and `targ`.\"\n",
    "    pred,targ = flatten_check(pred,targ)\n",
    "    pred, targ = torch.exp(pred)-1, torch.exp(targ)-1\n",
    "    pct_var = (targ - pred)/targ\n",
    "    return torch.sqrt((pct_var**2).mean())\n",
    "\n",
    "opt_func = torch.optim.Adam\n",
    "loss_func = MSELossFlat()\n",
    "learner = Learner(databunch, pt_model, opt_func=opt_func, loss_func=loss_func, metrics=[exp_rmspe])\n",
    "learner.fit(EPOCHS, LEARNING_RATE)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
