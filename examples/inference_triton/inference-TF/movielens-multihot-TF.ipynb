{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "## Overview\n",
    "\n",
    "In the previous notebooks, we show how to preprocess a dataframe with single-hot categorical columns, and train a Tensorflow model with processed parquet files, and then to send request to Triton Inference Server. In this notebook, we deal with multi-hot categorical columns. To learn more about on multi-hot example with movielens dataset, you can visit the example notebooks under the [getting-started-movielens](https://github.com/NVIDIA/NVTabular/tree/main/examples/getting-started-movielens) folder.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "In this notebook, we show how to\n",
    "- join two dataframes with JoinExternal operator\n",
    "- preprocess single-hot categorical input features with NVTabular\n",
    "- preprocess multi-hot categorical input features with NVTabular\n",
    "- use LambdaOp for custom row-wise dataframe manipulations with NVTabular\n",
    "- train a TF MLP model and save it on disk\n",
    "- save the ensemble model to be used at the inference stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.feature_column import feature_column_v2 as fc\n",
    "\n",
    "# we can control how much memory to give tensorflow with this environment variable\n",
    "# IMPORTANT: make sure you do this before you initialize TF's runtime, otherwise\n",
    "# TF will have claimed all free GPU memory\n",
    "os.environ['TF_MEMORY_ALLOCATION'] = \"0.6\" # fraction of free memory\n",
    "from nvtabular.loader.tensorflow import KerasSequenceLoader, KerasSequenceValidater\n",
    "from nvtabular.framework_utils.tensorflow import layers\n",
    "from tensorflow.python.feature_column import feature_column_v2 as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import cudf                 # cuDF is an implementation of Pandas-like Dataframe on GPU\n",
    "import time\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "import nvtabular as nvt\n",
    "\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our base directory, containing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to store raw and preprocesses data\n",
    "BASE_DIR = os.environ.get('BASE_DIR', '/model/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is not available in the base directory, we will download and unzip the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.exists(BASE_DIR + 'ml-25m'):\n",
    "    if not path.exists(BASE_DIR + 'ml-25m.zip'):\n",
    "        os.system(\"wget http://files.grouplens.org/datasets/movielens/ml-25m.zip\")\n",
    "        os.system(\"mv ml-25m.zip \" + BASE_DIR)\n",
    "    os.system(\"unzip \" + BASE_DIR + \"ml-25m.zip -d \" + BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset with NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we take a look on the movie metadata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies= cudf.read_csv(os.path.join(BASE_DIR, \"ml-25m\", \"movies.csv\"))\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['genres'] = movies['genres'].str.split('|')\n",
    "movies = movies.drop('title', axis=1)\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the movie ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = cudf.read_csv(os.path.join(BASE_DIR, \"ml-25m\", \"ratings.csv\"))\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the timestamp column and split the ratings into training and test dataset. We use a simple random split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.drop('timestamp', axis=1)\n",
    "train, valid = train_test_split(ratings, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(BASE_DIR + 'train.parquet')\n",
    "valid.to_parquet(BASE_DIR + 'valid.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Layers of neural networks require, that categorical features are continuous, incremental Integers: 0, 1, 2, ... , |C|-1. We need to ensure that our categorical features fulfill the requirement. We should transform the single-hot categorical features userId and movieId, and multi-hot categorical feature genres. NVTabular provides the operator `Categorify`, which provides this functionality with a high-level API out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, our dataset consists of two separate dataframes. First, we use the JoinExternal operator to left-join the metadata (genres) to our rating dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = (\n",
    "    ['userId', 'movieId'] >> \n",
    "    nvt.ops.JoinExternal(movies, on=['movieId'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add Categorify for our categorical features (single hot: userId, movieId and multi-hot: genres)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = joined >> nvt.ops.Categorify(cat_cache=\"device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratings are on a scale between 1-5. We want to predict a binary target with 1 are all ratings `>=4` and 0 are all ratings `<=3`. We use the [LambdaOp](https://github.com/NVIDIA/NVTabular/blob/main/nvtabular/ops/lambdaop.py) for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = nvt.ColumnGroup(['rating']) >> (lambda col: (col>3).astype('int8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize our calculation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = cat_features+ratings\n",
    "(output).graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize our NVTabular `workflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = nvt.Workflow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save our train and valid datasets as parquet files on disk, and below we will read them in while initializing the Dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "del valid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize NVTabular Datasets, and use the `part_size` parameter, which defines the size read into GPU-memory at once, in `nvt.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = nvt.Dataset(BASE_DIR + 'train.parquet', part_size='100MB')\n",
    "valid_iter = nvt.Dataset(BASE_DIR + 'valid.parquet', part_size='100MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we collect the training dataset statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "workflow.fit(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clear our output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if path.exists(os.path.join(BASE_DIR, \"train\")): \n",
    "    shutil.rmtree(os.path.join(BASE_DIR, \"train\"))\n",
    "if path.exists(os.path.join(BASE_DIR, \"valid\")): \n",
    "    shutil.rmtree(os.path.join(BASE_DIR, \"valid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform our workflow with `.transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "workflow.transform(train_iter).to_parquet(output_path=BASE_DIR + 'train/')\n",
    "workflow.transform(valid_iter).to_parquet(output_path=BASE_DIR + 'valid/', shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look in the output dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls $BASE_DIR/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow: Training Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "TRAIN_PATHS = sorted(glob.glob(BASE_DIR + 'train/*.parquet'))\n",
    "VALID_PATHS = sorted(glob.glob(BASE_DIR + 'valid/*.parquet'))\n",
    "TRAIN_PATHS, VALID_PATHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our ratings are of only 0 and 1 after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.read_parquet(TRAIN_PATHS[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step we are going to generate config.pbtxt and we will save our workflow as a .pkl file to be able to load again to do transformation for the test (new coming) datasets at inference stage. This step actually does the serialization of the workflow that we created above using the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts a workflow to a triton mode \n",
    "MODEL_NAME_NVT = os.environ.get('MODEL_NAME_NVT', 'movielens_nvt_mh')\n",
    "MODEL_PATH_TEMP_NVT = os.path.join(BASE_DIR, MODEL_NAME_NVT)\n",
    "\n",
    "from nvtabular.inference.triton import generate_nvtabular_model\n",
    "generate_nvtabular_model(workflow, MODEL_NAME_NVT, MODEL_PATH_TEMP_NVT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train a TF MLP model using our preprocessed parquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the data schema, and define our single-hot categorical features. Note, that we do not have any numerical input features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024*32                            # Batch Size\n",
    "CATEGORICAL_COLUMNS = ['movieId', 'userId']     # Single-hot\n",
    "CATEGORICAL_MH_COLUMNS = ['genres']             # Multi-hot\n",
    "NUMERIC_COLUMNS = []\n",
    "\n",
    "TRAIN_PATHS = sorted(glob.glob(BASE_DIR + 'train/*.parquet')) # Output from ETL-with-NVTabular\n",
    "VALID_PATHS = sorted(glob.glob(BASE_DIR + 'valid/*.parquet')) # Output from ETL-with-NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the embedding input and output dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_TABLE_SHAPES = nvt.ops.get_embedding_sizes(workflow)\n",
    "EMBEDDING_TABLE_SHAPES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use emb_dim 64 for `movieId` and `userId` categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_TABLE_SHAPES = {'genres': (21, 16), 'movieId': (56586, 64), 'userId': (162542, 64)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing NVTabular Data Loader for Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NVTabular data loader is initialized as usual and we specify both single-hot and multi-hot categorical features as cat_names. The data loader will automatically recognize the single/multi-hot columns and represent them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_tf = KerasSequenceLoader(\n",
    "    TRAIN_PATHS, # you could also use a glob pattern\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_names=['rating'],\n",
    "    cat_names=CATEGORICAL_COLUMNS+CATEGORICAL_MH_COLUMNS,\n",
    "    cont_names=NUMERIC_COLUMNS,\n",
    "    engine='parquet',\n",
    "    shuffle=True,\n",
    "    buffer_size=0.06, # how many batches to load at once\n",
    "    parts_per_chunk=1\n",
    ")\n",
    "\n",
    "valid_dataset_tf = KerasSequenceLoader(\n",
    "    VALID_PATHS, # you could also use a glob pattern\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_names=['rating'],\n",
    "    cat_names = CATEGORICAL_COLUMNS+CATEGORICAL_MH_COLUMNS,\n",
    "    cont_names=NUMERIC_COLUMNS,\n",
    "    engine='parquet',\n",
    "    shuffle=False,\n",
    "    buffer_size=0.06,\n",
    "    parts_per_chunk=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a common neural network architecture for tabular data.\n",
    "\n",
    "- Single-hot categorical features are fed into an Embedding Layer\n",
    "- Each value of a multi-hot categorical features is fed into an Embedding Layer and the multiple Embedding outputs are combined via averaging\n",
    "- The output of the Embedding Layers are concatenated\n",
    "- The concatenated layers are fed through multiple feed-forward layers (Dense Layers with ReLU activations)\n",
    "- The final output is a single number with sigmoid activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define some dictonary/lists for our network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {}    # tf.keras.Input placeholders for each feature to be used\n",
    "emb_layers = []# output of all embedding layers, which will be concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create `tf.keras.Input` tensors for all 4 input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in CATEGORICAL_COLUMNS:\n",
    "    inputs[col] =  tf.keras.Input(\n",
    "        name=col,\n",
    "        dtype=tf.int64,\n",
    "        shape=(1,)\n",
    "    )\n",
    "# Note that we need two input tensors for multi-hot categorical features\n",
    "for col in CATEGORICAL_MH_COLUMNS:\n",
    "    inputs[col+'__values'] = tf.keras.Input(\n",
    "        name=f\"{col}__values\", \n",
    "        dtype=tf.int64, \n",
    "        shape=(1,)\n",
    "    )\n",
    "    inputs[col+'__nnzs'] = tf.keras.Input(\n",
    "        name=f\"{col}__nnzs\", \n",
    "        dtype=tf.int64, \n",
    "        shape=(1,)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize Embedding Layers with `tf.feature_column.embedding_column`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in CATEGORICAL_COLUMNS+CATEGORICAL_MH_COLUMNS:\n",
    "    emb_layers.append(\n",
    "        tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_identity(\n",
    "                col, \n",
    "                EMBEDDING_TABLE_SHAPES[col][0]                    # Input dimension (vocab size)\n",
    "            ), EMBEDDING_TABLE_SHAPES[col][1]                     # Embedding output dimension\n",
    "        )\n",
    "    )\n",
    "emb_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NVTabular implemented a custom TensorFlow layer `layers.DenseFeatures`, which takes as an input the different `tf.Keras.Input` and pre-initialized `tf.feature_column` and automatically concatenates them into a flat tensor. `DenseFeatures` can handle numeric inputs, as well, but MovieLens does not provide numerical input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_layer = layers.DenseFeatures(emb_layers)\n",
    "x_emb_output = emb_layer(inputs)\n",
    "x_emb_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the output shape of the concatenated layer is equal to the sum of the individual Embedding output dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add multiple Dense Layers. Finally, we initialize the `tf.keras.Model` and add the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Dense(128, activation='relu')(x_emb_output)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "model.compile('sgd', 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train our model with `model.fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_callback = KerasSequenceValidater(valid_dataset_tf)\n",
    "\n",
    "history = model.fit(train_dataset_tf, callbacks=[validation_callback], epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the trained TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME_TF = os.environ.get(\"MODEL_NAME_TF\", \"movielens_tf_mh\")\n",
    "MODEL_PATH_TEMP_TF = os.path.join(BASE_DIR, MODEL_NAME_TF, \"1/model.savedmodel\")\n",
    "\n",
    "model.save(MODEL_PATH_TEMP_TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an ensemble triton server model, where\n",
    "#   model: The tensorflow model that should be served\n",
    "#   workflow: The nvtabular workflow used in preprocessing\n",
    "#   name: The base name of the various triton models\n",
    "\n",
    "MODEL_NAME_ENSEMBLE = os.environ.get(\"MODEL_NAME_ENSEMBLE\", \"movielens_mh\")\n",
    "MODEL_PATH = os.environ.get(\"MODEL_PATH\", \"/model/models_multihot/\")\n",
    "\n",
    "from nvtabular.inference.triton import export_tensorflow_ensemble\n",
    "export_tensorflow_ensemble(model, workflow, MODEL_NAME_ENSEMBLE, MODEL_PATH, [\"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get install tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model in a model repository must include a model configuration that provides required and optional information about the model. Next, Triton needs a config file to understand how to interpret the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tree /model/models_multihot/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will move to the next notebook, [movielens-multihot-inference](), to send request to the Triton Inference Server."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
