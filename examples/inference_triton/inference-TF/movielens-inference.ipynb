{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment with Merlin Inference API\n",
    "\n",
    "## Overview\n",
    "\n",
    "In the previous notebook we explained and showed how we can preprocess data with NVTabular, and train an TF MLP model using NVTabular KerasSequenceLoader. We learned how to save a workflow, a trained TF model, and the ensemble model. In this notebook, we will show example request scripts sent to triton inference server\n",
    "- to transform new/streaming data with NVTabular library\n",
    "- to generate prediction results for new data from trained model \n",
    "- to deploy the end-to-end pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import os\n",
    "from time import time\n",
    "import warnings \n",
    "\n",
    "from tritonclient.utils import *\n",
    "import tritonclient.grpc as grpcclient\n",
    "import nvtabular\n",
    "import cudf\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our base directory containing the raw and processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.environ.get('MODEL_BASE_DIR', '/model/models/')\n",
    "INPUT_DATA_DIR = os.environ.get('INPUT_DATA_DIR', '/model/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deactivate the warnings before sending requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Triton Is Running Correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Triton’s ready endpoint to verify that the server and the models are ready for inference. Replace `localhost` with your host ip address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client created.\n"
     ]
    }
   ],
   "source": [
    "import tritonhttpclient\n",
    "try:\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.is_server_live() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTTP request returns status 200 if Triton is ready and non-200 if it is not ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send request to Triton IS to transform raw dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we send a request to the running triton inference server using our raw validation set in parquet format. This request is going to load the saved NVTabular workflow and then transform the new dataset samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data:\n",
      "           userId  movieId\n",
      "15347762   99476   104374\n",
      "16647840  107979     2634\n",
      "23915192  155372     1614 \n",
      "\n",
      "transformed data:\n",
      "    userId  movieId\n",
      "0   99476    19997\n",
      "1  107979     2543\n",
      "2  155372     1557\n"
     ]
    }
   ],
   "source": [
    "# read in the workflow (to get input/output schema to call triton with)\n",
    "MODEL_NAME_NVT = os.environ.get('MODEL_NAME_NVT', 'movielens_nvt')\n",
    "MODEL_PATH_NVT = os.path.join(MODEL_PATH, MODEL_NAME_NVT)\n",
    "\n",
    "workflow = nvtabular.Workflow.load(os.path.join(MODEL_PATH_NVT, \"1/workflow\"))\n",
    "\n",
    "# read in a batch of data to get transforms for\n",
    "batch = cudf.read_parquet(os.path.join(INPUT_DATA_DIR, \"valid.parquet\"), num_rows=3)[workflow.column_group.input_column_names]\n",
    "\n",
    "print(\"raw data:\\n\", batch, \"\\n\")\n",
    "# convert the batch to a triton inputs\n",
    "columns = [(col, batch[col][0:3]) for col in workflow.column_group.input_column_names]\n",
    "inputs = []\n",
    "\n",
    "col_dtypes = [np.int64, np.int64]\n",
    "\n",
    "for i, (name, col) in enumerate(columns):\n",
    "    d = col.values_host.astype(col_dtypes[i])\n",
    "    d = d.reshape(len(d),1)\n",
    "    inputs.append(grpcclient.InferInput(name, d.shape, np_to_triton_dtype(col_dtypes[i])))\n",
    "    inputs[i].set_data_from_numpy(d)\n",
    "\n",
    "# placeholder variables for the output\n",
    "outputs = [grpcclient.InferRequestedOutput(name) for name in workflow.column_group.columns]\n",
    "\n",
    "# make the request\n",
    "# replace <localhost> with your host ip address.\n",
    "with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer(MODEL_NAME_NVT, inputs, request_id=\"1\",outputs=outputs)\n",
    "    \n",
    "# convert output from triton back to a nvt dataframe  \n",
    "output = cudf.DataFrame({col: response.as_numpy(col).T[0] for col in workflow.column_group.columns})\n",
    "print(\"transformed data:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the MovieLens rating classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A minimal model repository for a TensorFlow SavedModel model is:\n",
    "```\n",
    "  <model-repository-path>/<model-name>/\n",
    "      config.pbtxt\n",
    "      1/\n",
    "        model.savedmodel/\n",
    "           <saved-model files>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out our model repository layout. You can install `tree` library with  `apt-get install tree`, and then run `tree /model/models/` to print out the model repository layout as below:\n",
    "```\n",
    "/model/models/\n",
    "├── movielens\n",
    "│   ├── 1\n",
    "│   └── config.pbtxt\n",
    "├── movielens_nvt\n",
    "│   ├── 1\n",
    "│   │   ├── model.py\n",
    "│   │   └── workflow\n",
    "│   │       ├── categories\n",
    "│   │       │   ├── unique.movieId.parquet\n",
    "│   │       │   └── unique.userId.parquet\n",
    "│   │       ├── metadata.json\n",
    "│   │       └── workflow.pkl\n",
    "│   └── config.pbtxt\n",
    "└── movielens_tf\n",
    "    ├── 1\n",
    "    │   └── model.savedmodel\n",
    "    │       ├── assets\n",
    "    │       ├── saved_model.pb\n",
    "    │       └── variables\n",
    "    │           ├── variables.data-00000-of-00001\n",
    "    │           └── variables.index\n",
    "    └── config.pbtxt\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we have a config.pbtxt file. Each model in a model repository must include a model configuration that provides required and optional information about the model. Typically, this configuration is provided in a `config.pbtxt` file specified as [ModelConfig protobuf](https://github.com/triton-inference-server/server/blob/r20.12/src/core/model_config.proto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data:\n",
      "    userId  movieId\n",
      "0   99476    19997\n",
      "1  107979     2543\n",
      "2  155372     1557 \n",
      "\n",
      "predicted sigmoid result:\n",
      " [[0.6250564]\n",
      " [0.6247618]\n",
      " [0.6251831]]\n"
     ]
    }
   ],
   "source": [
    "# read in a batch of data to get transforms for\n",
    "\n",
    "batch = cudf.read_parquet(os.path.join(INPUT_DATA_DIR, \"valid/*.parquet\"), num_rows=3)\n",
    "\n",
    "batch = batch[batch.columns][0:3]\n",
    "batch = batch.drop(columns=[\"rating\"])\n",
    "\n",
    "print(\"input data:\\n\", batch, \"\\n\")\n",
    "\n",
    "inputs = [] \n",
    "for i, col in enumerate(batch.columns):\n",
    "    d = batch[col].values_host.astype(np.int32)\n",
    "    d = d.reshape(len(d),1)\n",
    "    inputs.append(grpcclient.InferInput(col, d.shape, np_to_triton_dtype(np.int32)))\n",
    "    inputs[i].set_data_from_numpy(d)\n",
    "\n",
    "outputs = [grpcclient.InferRequestedOutput(\"dense_3\")]\n",
    "\n",
    "MODEL_NAME_TF = os.environ.get('MODEL_NAME_TF', 'movielens_tf')\n",
    "\n",
    "with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer(MODEL_NAME_TF, inputs, request_id=\"1\",outputs=outputs)\n",
    "\n",
    "print(\"predicted sigmoid result:\\n\", response.as_numpy('dense_3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END-2-END INFERENCE PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this request example below, we show that we can feed raw unprocessed parquet file, and obtain final prediction results coming from the last layer of the TF model that we built in `movilens_TF` notebook. The output we get is a sigmoid value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `InferInput` to describe the tensors we'll be sending to the server. It needs the name of the input, the shape of the tensor we'll be passing to the server, and its datatype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send request to Triton IS to generate prediction results for raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data:\n",
      "           userId  movieId\n",
      "15347762   99476   104374\n",
      "16647840  107979     2634\n",
      "23915192  155372     1614 \n",
      "\n",
      "predicted sigmoid result:\n",
      " [[0.6250564]\n",
      " [0.6247618]\n",
      " [0.6251831]]\n"
     ]
    }
   ],
   "source": [
    "# read in the workflow (to get input/output schema to call triton with)\n",
    "batc_size = 64\n",
    "batch = cudf.read_parquet(os.path.join(INPUT_DATA_DIR, \"valid.parquet\"), num_rows=3, columns=['userId', 'movieId'])\n",
    "batch = batch[batch.columns][0:3]\n",
    "\n",
    "print(\"raw data:\\n\", batch, \"\\n\")\n",
    "\n",
    "# convert the batch to a triton inputs\n",
    "inputs = []\n",
    "\n",
    "col_names = ['userId', 'movieId'] \n",
    "col_dtypes = [np.int64, np.int64]\n",
    "\n",
    "for i, col in enumerate(batch.columns):\n",
    "    d = batch[col].values_host.astype(col_dtypes[i])\n",
    "    d = d.reshape(len(d),1)\n",
    "    inputs.append(grpcclient.InferInput(col_names[i], d.shape, np_to_triton_dtype(col_dtypes[i])))\n",
    "    inputs[i].set_data_from_numpy(d)\n",
    "\n",
    "# placeholder variables for the output\n",
    "outputs = [grpcclient.InferRequestedOutput(\"dense_3\")]\n",
    "\n",
    "MODEL_NAME_ENSEMBLE = os.environ.get('MODEL_NAME_ENSEMBLE', 'movielens')\n",
    "\n",
    "# build a client to connect to our server. \n",
    "# This InferenceServerClient object is what we'll be using to talk to Triton.\n",
    "# make the request with tritonclient.grpc.InferInput object\n",
    "with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer(MODEL_NAME_ENSEMBLE, inputs, request_id=\"1\",outputs=outputs)\n",
    "\n",
    "print(\"predicted sigmoid result:\\n\", response.as_numpy('dense_3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's send request for a larger batch size and measure the total run time and throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data:\n",
      "           userId  movieId\n",
      "15347762   99476   104374\n",
      "16647840  107979     2634\n",
      "23915192  155372     1614\n",
      "10052313   65225     7153\n",
      "12214125   79161      500\n",
      "...          ...      ...\n",
      "17138306  111072     1625\n",
      "21326655  138575    81591\n",
      "5664631    36671     8861\n",
      "217658      1535   111759\n",
      "11842246   76766   109487\n",
      "\n",
      "[64 rows x 2 columns] \n",
      "\n",
      "predicted sigmoid result:\n",
      " [[0.6250564 ]\n",
      " [0.6247618 ]\n",
      " [0.6251831 ]\n",
      " [0.6250345 ]\n",
      " [0.62659883]\n",
      " [0.62566674]\n",
      " [0.62582093]\n",
      " [0.62555826]\n",
      " [0.62475634]\n",
      " [0.62531734]\n",
      " [0.62435687]\n",
      " [0.6254294 ]\n",
      " [0.6249607 ]\n",
      " [0.6250089 ]\n",
      " [0.62605655]\n",
      " [0.6254521 ]\n",
      " [0.62492216]\n",
      " [0.6259799 ]\n",
      " [0.6250382 ]\n",
      " [0.62497985]\n",
      " [0.6252896 ]\n",
      " [0.6264592 ]\n",
      " [0.625206  ]\n",
      " [0.6248848 ]\n",
      " [0.62532496]\n",
      " [0.6252597 ]\n",
      " [0.6252666 ]\n",
      " [0.6246876 ]\n",
      " [0.62549347]\n",
      " [0.6246998 ]\n",
      " [0.6252818 ]\n",
      " [0.62472236]\n",
      " [0.62552464]\n",
      " [0.62541705]\n",
      " [0.62485063]\n",
      " [0.62516356]\n",
      " [0.6248119 ]\n",
      " [0.62541467]\n",
      " [0.6259715 ]\n",
      " [0.6251849 ]\n",
      " [0.62577355]\n",
      " [0.62549895]\n",
      " [0.6247672 ]\n",
      " [0.62464315]\n",
      " [0.6257611 ]\n",
      " [0.6251645 ]\n",
      " [0.62580687]\n",
      " [0.625876  ]\n",
      " [0.6255983 ]\n",
      " [0.6256856 ]\n",
      " [0.6246587 ]\n",
      " [0.6260274 ]\n",
      " [0.62486887]\n",
      " [0.6250542 ]\n",
      " [0.6255111 ]\n",
      " [0.6261726 ]\n",
      " [0.62522423]\n",
      " [0.624463  ]\n",
      " [0.62572306]\n",
      " [0.62637323]\n",
      " [0.62592286]\n",
      " [0.62571806]\n",
      " [0.6257034 ]\n",
      " [0.62590635]] \n",
      "\n",
      "run_time(sec): 0.03922152519226074 - rows: 64 - dl_thru: 1631.7570437732134\n"
     ]
    }
   ],
   "source": [
    "# read in the workflow (to get input/output schema to call triton with)\n",
    "batch_size = 64\n",
    "batch = cudf.read_parquet(os.path.join(INPUT_DATA_DIR, \"valid.parquet\"), num_rows=batch_size, columns=['userId', 'movieId'])\n",
    "batch = batch[batch.columns][0:batch_size]\n",
    "\n",
    "print(\"raw data:\\n\", batch, \"\\n\")\n",
    "\n",
    "start = time()\n",
    "# convert the batch to a triton inputs\n",
    "inputs = []\n",
    "\n",
    "col_names = ['userId', 'movieId'] \n",
    "col_dtypes = [np.int64, np.int64]\n",
    "\n",
    "for i, col in enumerate(batch.columns):\n",
    "    d = batch[col].values_host.astype(col_dtypes[i])\n",
    "    d = d.reshape(len(d),1)\n",
    "    inputs.append(grpcclient.InferInput(col_names[i], d.shape, np_to_triton_dtype(col_dtypes[i])))\n",
    "    inputs[i].set_data_from_numpy(d)\n",
    "\n",
    "# placeholder variables for the output\n",
    "outputs = [grpcclient.InferRequestedOutput(\"dense_3\")]\n",
    "\n",
    "MODEL_NAME_ENSEMBLE = os.environ.get('MODEL_NAME_ENSEMBLE', 'movielens')\n",
    "\n",
    "# build a client to connect to our server. \n",
    "# This InferenceServerClient object is what we'll be using to talk to Triton.\n",
    "# make the request with tritonclient.grpc.InferInput object\n",
    "with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer(MODEL_NAME_ENSEMBLE, inputs, request_id=\"1\",outputs=outputs)\n",
    "\n",
    "t_final = time() - start\n",
    "print(\"predicted sigmoid result:\\n\", response.as_numpy('dense_3'), \"\\n\")\n",
    "\n",
    "print(f\"run_time(sec): {t_final} - rows: {batch_size} - dl_thru: {batch_size / t_final}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
