{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "## Overview\n",
    "\n",
    "In the previous notebook we explained and showed how we can preprocess data with multi-hot columns with NVTabular, and train an TF MLP model using NVTabular KerasSequenceLoader. We learned how to save a workflow, a trained TF model, and the ensemble model. In this notebook, we will show example request scripts sent to triton inference server\n",
    "\n",
    "- to transform new/streaming data with NVTabular library\n",
    "- to deploy the end-to-end pipeline to generate prediction results for new data from trained TF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import os\n",
    "from os import path                \n",
    "import time\n",
    "import gc\n",
    "\n",
    "import nvtabular\n",
    "import cudf \n",
    "from tritonclient.utils import *\n",
    "import tritonclient.grpc as grpcclient\n",
    "import nvtabular.inference.triton as nvt_triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our base directory, containing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to store raw and preprocesses data\n",
    "MODEL_BASE_DIR = os.environ.get('MODEL_BASE_DIR', '/model/models_multihot/')\n",
    "INPUT_DATA_DIR = os.environ.get('INPUT_DATA_DIR', '/model/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Triton Is Running Correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get install curl -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "\u001b[1mContent-Length\u001b[0m: 0\n",
      "\u001b[1mContent-Type\u001b[0m: text/plain\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!curl -i localhost:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send request to Triton IS to transform raw dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the raw validation set, and send 3 rows of `userid` and `movieId` as input to the saved NVTabular model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15347762</th>\n",
       "      <td>99476</td>\n",
       "      <td>104374</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16647840</th>\n",
       "      <td>107979</td>\n",
       "      <td>2634</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23915192</th>\n",
       "      <td>155372</td>\n",
       "      <td>1614</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10052313</th>\n",
       "      <td>65225</td>\n",
       "      <td>7153</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12214125</th>\n",
       "      <td>79161</td>\n",
       "      <td>500</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          userId  movieId  rating\n",
       "15347762   99476   104374     3.5\n",
       "16647840  107979     2634     4.0\n",
       "23915192  155372     1614     3.0\n",
       "10052313   65225     7153     4.0\n",
       "12214125   79161      500     5.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = cudf.read_parquet(INPUT_DATA_DIR + 'valid.parquet')\n",
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId [[ 99476]\n",
      " [107979]\n",
      " [155372]] (3, 1)\n",
      "movieId [[19997]\n",
      " [ 2543]\n",
      " [ 1557]] (3, 1)\n",
      "genres__nnzs [[3]\n",
      " [1]\n",
      " [1]] (3, 1)\n",
      "genres__values [[ 9]\n",
      " [10]\n",
      " [16]\n",
      " [12]\n",
      " [ 6]] (5, 1)\n"
     ]
    }
   ],
   "source": [
    "df = cudf.DataFrame({\"userId\": [99476, 107979, 155372], \"movieId\": [104374, 2634, 1614]})\n",
    "inputs = nvt_triton.convert_df_to_triton_input([\"userId\", \"movieId\"], df, grpcclient.InferInput)\n",
    "\n",
    "outputs = [\n",
    "    grpcclient.InferRequestedOutput(col)\n",
    "    for col in [\"userId\", \"movieId\", \"genres__nnzs\", \"genres__values\"]\n",
    "]\n",
    "\n",
    "MODEL_NAME_NVT = os.environ.get('MODEL_NAME_NVT', 'movielens_mh_nvt')\n",
    "\n",
    "with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer(MODEL_NAME_NVT, inputs, request_id=\"1\", outputs=outputs)\n",
    "\n",
    "for col in [\"userId\", \"movieId\", \"genres__nnzs\", \"genres__values\"]:\n",
    "    print(col, response.as_numpy(col), response.as_numpy(col).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that we don't need to send the genres column as an input. The reason for that is the nvt model will look up the genres for each movie as part of the `JoinExternal` op it applies. Also notice that when creating the request for the `movielens_mh_nvt` model, we return 2 columns (values and nnzs) for the `genres` column rather than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END-2-END INFERENCE PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the same, but this time we directly read in first 3 rows of the the raw `valid.parquet` file with cuDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          userId  movieId\n",
      "15347762   99476   104374\n",
      "16647840  107979     2634\n",
      "23915192  155372     1614 \n",
      "\n"
     ]
    },
    {
     "ename": "InferenceServerException",
     "evalue": "[StatusCode.INTERNAL] in ensemble 'movielens_mh', [_Derived_]{{function_node __inference_signature_wrapper_154847}} {{function_node __inference_signature_wrapper_154847}} input segment_ids[0] expected type int32 != int64, the type of model/dense_features/embedding_lookup/embedding_lookup_sparse/strided_slice:output:0[0]\n\tIn {{node model/dense_features/embedding_lookup/embedding_lookup_sparse}}\n\t [[StatefulPartitionedCall]]\n\t [[StatefulPartitionedCall]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInferenceServerException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ecaabd6275ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# make the request with tritonclient.grpc.InferInput object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgrpcclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceServerClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"localhost:8001\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME_ENSEMBLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predicted softmax result:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dense_3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/conda/envs/rapids/lib/python3.8/site-packages/tritonclient/grpc/__init__.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, model_name, inputs, model_version, outputs, request_id, sequence_id, sequence_start, sequence_end, priority, timeout, client_timeout, headers)\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrpc_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m             \u001b[0mraise_error_grpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrpc_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m     def async_infer(self,\n",
      "\u001b[0;32m/conda/envs/rapids/lib/python3.8/site-packages/tritonclient/grpc/__init__.py\u001b[0m in \u001b[0;36mraise_error_grpc\u001b[0;34m(rpc_error)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_error_grpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrpc_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mget_error_grpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrpc_error\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInferenceServerException\u001b[0m: [StatusCode.INTERNAL] in ensemble 'movielens_mh', [_Derived_]{{function_node __inference_signature_wrapper_154847}} {{function_node __inference_signature_wrapper_154847}} input segment_ids[0] expected type int32 != int64, the type of model/dense_features/embedding_lookup/embedding_lookup_sparse/strided_slice:output:0[0]\n\tIn {{node model/dense_features/embedding_lookup/embedding_lookup_sparse}}\n\t [[StatefulPartitionedCall]]\n\t [[StatefulPartitionedCall]]"
     ]
    }
   ],
   "source": [
    "# read in the workflow (to get input/output schema to call triton with)\n",
    "batch = cudf.read_parquet(os.path.join(INPUT_DATA_DIR, \"valid.parquet\"), num_rows=3, columns=['userId', 'movieId'])\n",
    "print(batch, \"\\n\")\n",
    "\n",
    "# convert the batch to a triton inputs\n",
    "inputs = nvt_triton.convert_df_to_triton_input([\"userId\", \"movieId\"], batch, grpcclient.InferInput)\n",
    "\n",
    "# placeholder variables for the output\n",
    "outputs = [grpcclient.InferRequestedOutput(\"dense_3\")]\n",
    "\n",
    "MODEL_NAME_ENSEMBLE = os.environ.get('MODEL_NAME_ENSEMBLE', 'movielens_mh')\n",
    "\n",
    "# build a client to connect to our server. \n",
    "# This InferenceServerClient object is what we'll be using to talk to Triton.\n",
    "# make the request with tritonclient.grpc.InferInput object\n",
    "with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer(MODEL_NAME_ENSEMBLE, inputs, request_id=\"1\",outputs=outputs)\n",
    "\n",
    "print(\"predicted softmax result:\\n\", response.as_numpy('dense_3'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_valid\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
