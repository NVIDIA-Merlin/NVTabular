{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment with Merlin Inference API\n",
    "\n",
    "## Overview\n",
    "\n",
    "In the previous notebook we explained and showed how we can preprocess data with NVTAbular, and train an TF MLP model using NVTabular KerasSequenceLoader. We learned how to save a workflow, a trained TF model, and the ensemble model. In this notebook, we will show example request scripts sent to triton inference server\n",
    "- to transform new/streaming data with NVTabular library\n",
    "- to generate prediction results for new data from trained model \n",
    "- to deploy the end-to-end pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import os\n",
    "\n",
    "from tritonclient.utils import *\n",
    "import tritonclient.http as httpclient\n",
    "import nvtabular\n",
    "import cudf\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our base directory, containing the raw and processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/working_dir/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml-25m\ttrain  train.parquet  valid  valid.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls $BASE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Triton Is Running Correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Triton’s ready endpoint to verify that the server and the models are ready for inference. From the host system use curl to access the HTTP endpoint that indicates server status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "\u001b[1mContent-Length\u001b[0m: 0\n",
      "\u001b[1mContent-Type\u001b[0m: text/plain\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!curl -i 10.110.20.127:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTTP request returns status 200 if Triton is ready and non-200 if it is not ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's send a request to the running triton inference server using our raw validation set in parquet format. This request is going to load the saved NVTabular workflow and then transform the new dataset samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send request to Triton IS to transform raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          userId  movieId\n",
      "15347762   99476   104374\n",
      "16647840  107979     2634\n",
      "23915192  155372     1614 \n",
      "\n",
      "   userId  movieId\n",
      "0   99476    19997\n",
      "1  107979     2543\n",
      "2  155372     1557\n"
     ]
    }
   ],
   "source": [
    "# read in the workflow (to get input/output schema to call triton with)\n",
    "\n",
    "workflow = nvtabular.Workflow.load(\"/working_dir/models/movielens_nvt/1/workflow\")\n",
    "\n",
    "# read in a batch of data to get transforms for\n",
    "batch = cudf.read_parquet(\"/working_dir/data/valid.parquet\", num_rows=3)[workflow.column_group.input_column_names]\n",
    "\n",
    "print(batch, \"\\n\")\n",
    "\n",
    "# convert the batch to a triton inputs\n",
    "columns = [(col, batch[col][0:3]) for col in workflow.column_group.input_column_names]\n",
    "inputs = []\n",
    "\n",
    "col_dtypes = [np.int64, np.int64]\n",
    "\n",
    "for i, (name, col) in enumerate(columns):\n",
    "    d = col.values_host.astype(col_dtypes[i])\n",
    "    d = d.reshape(len(d),1)\n",
    "    inputs.append(httpclient.InferInput(name, d.shape, np_to_triton_dtype(col_dtypes[i])))\n",
    "    inputs[i].set_data_from_numpy(d)\n",
    "\n",
    "# placeholder variables for the output\n",
    "outputs = [httpclient.InferRequestedOutput(name) for name in workflow.column_group.columns]\n",
    "\n",
    "# make the request\n",
    "with httpclient.InferenceServerClient(\"10.110.20.127:8000\") as client:\n",
    "    response = client.infer(\"movielens_nvt\", inputs, request_id=\"1\",outputs=outputs)\n",
    "    \n",
    "# convert output from triton back to a nvt dataframe  \n",
    "output = cudf.DataFrame({col: response.as_numpy(col).T[0] for col in workflow.column_group.columns})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the MovieLens rating classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [movilens_TF]() notebook we saved our TF model with the following script:\n",
    "\n",
    "```\n",
    "model.save('/working_dir/models/movielens_tf/1/model.savedmodel')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A minimal model repository for a TensorFlow SavedModel model is:\n",
    "```\n",
    "  <model-repository-path>/\n",
    "    <model-name>/\n",
    "      config.pbtxt\n",
    "      1/\n",
    "        model.savedmodel/\n",
    "           <saved-model files>\n",
    "```\n",
    "Let's check out our model repository layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get install tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/working_dir/models/movielens_tf\u001b[00m\n",
      "├── \u001b[01;34m1\u001b[00m\n",
      "│   └── \u001b[01;34mmodel.savedmodel\u001b[00m\n",
      "│       ├── \u001b[01;34massets\u001b[00m\n",
      "│       ├── keras_metadata.pb\n",
      "│       ├── saved_model.pb\n",
      "│       └── \u001b[01;34mvariables\u001b[00m\n",
      "│           ├── variables.data-00000-of-00001\n",
      "│           └── variables.index\n",
      "└── config.pbtxt\n",
      "\n",
      "4 directories, 5 files\n"
     ]
    }
   ],
   "source": [
    "!tree /working_dir/models/movielens_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we have a config.pbtxt file. Each model in a model repository must include a model configuration that provides required and optional information about the model. Typically, this configuration is provided in a `config.pbtxt` file specified as [ModelConfig protobuf](https://github.com/triton-inference-server/server/blob/r20.12/src/core/model_config.proto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.62585497]\n",
      " [0.6240477 ]\n",
      " [0.6258309 ]]\n"
     ]
    }
   ],
   "source": [
    "from tritonclient.utils import *\n",
    "import tritonclient.http as httpclient\n",
    "import nvtabular\n",
    "import cudf\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "# read in a batch of data to get transforms for\n",
    "batch = cudf.read_parquet(\"/working_dir/data/valid/*.parquet\", num_rows=3)\n",
    "\n",
    "batch = batch[batch.columns][0:3]\n",
    "batch = batch.drop(columns=[\"rating\"])\n",
    "\n",
    "inputs = [] \n",
    "\n",
    "for i, col in enumerate(batch.columns):\n",
    "    d = batch[col].values_host.astype(np.int32)\n",
    "    d = d.reshape(len(d),1)\n",
    "    inputs.append(httpclient.InferInput(col, d.shape, np_to_triton_dtype(np.int32)))\n",
    "    inputs[i].set_data_from_numpy(d)\n",
    "\n",
    "outputs = [httpclient.InferRequestedOutput(\"dense_3\")]\n",
    "\n",
    "with httpclient.InferenceServerClient(\"10.110.20.127:8000\") as client:\n",
    "    response = client.infer(\"movielens_tf\", inputs, request_id=\"1\",outputs=outputs)\n",
    "\n",
    "print(response.as_numpy(\"dense_3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END-2-END INFERENCE PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this request example below, we show that we can feed raw unprocessed parquet file, and obtain final prediction results coming from the last layer of the TF model that we built in `movilens_TF` notebook. The output we get is a softmax value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `InferInput` to describe the tensors we'll be sending to the server. It needs the name of the input, the shape of the tensor we'll be passing to the server, and its datatype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send request to Triton IS to generate prediction results for raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          userId  movieId\n",
      "15347762   99476   104374\n",
      "16647840  107979     2634\n",
      "23915192  155372     1614 \n",
      "\n",
      "predicted softmax result:\n",
      " [[0.62585497]\n",
      " [0.6240477 ]\n",
      " [0.6258309 ]]\n"
     ]
    }
   ],
   "source": [
    "from tritonclient.utils import *\n",
    "import tritonclient.http as httpclient\n",
    "import nvtabular\n",
    "import cudf\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "# read in the workflow (to get input/output schema to call triton with)\n",
    "batch = cudf.read_parquet(\"/working_dir/data/valid.parquet\", num_rows=3, columns=['userId', 'movieId'])\n",
    "batch = batch[batch.columns][0:3]\n",
    "\n",
    "print(batch, \"\\n\")\n",
    "\n",
    "# convert the batch to a triton inputs\n",
    "inputs = []\n",
    "\n",
    "col_names = ['userId', 'movieId'] \n",
    "col_dtypes = [np.int64, np.int64]\n",
    "\n",
    "for i, col in enumerate(batch.columns):\n",
    "    d = batch[col].values_host.astype(col_dtypes[i])\n",
    "    d = d.reshape(len(d),1)\n",
    "    inputs.append(httpclient.InferInput(col_names[i], d.shape, np_to_triton_dtype(col_dtypes[i])))\n",
    "    inputs[i].set_data_from_numpy(d)\n",
    "\n",
    "\n",
    "# placeholder variables for the output\n",
    "outputs = [httpclient.InferRequestedOutput(\"dense_3\")]\n",
    "\n",
    "# build a client to connect to our server. \n",
    "# This InferenceServerClient object is what we'll be using to talk to Triton.\n",
    "# make the request with tritonclient.http.InferInput object\n",
    "with httpclient.InferenceServerClient(\"10.110.20.127:8000\") as client:\n",
    "    response = client.infer(\"movielens\", inputs, request_id=\"1\",outputs=outputs)\n",
    "\n",
    "print(\"predicted softmax result:\\n\", response.as_numpy('dense_3'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
