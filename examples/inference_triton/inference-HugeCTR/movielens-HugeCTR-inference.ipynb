{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "special-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-timothy",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will show how we do inference with our trained deep learning recommender model using Triton Inference Server. In this example, we deploy the NVTabular workflow and HugeCTR model with Triton Inference Server. We deploy them as an ensemble. For each request, Triton Inference Server will feed the input data through the NVTabular workflow and its output through the HugeCR model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-bedroom",
   "metadata": {},
   "source": [
    "As we went through in the previous notebook, [movielens-HugeCTR](https://github.com/NVIDIA/NVTabular/blob/main/examples/inference_triton/inference-HugeCTR/movielens-HugeCTR.ipynb), NVTabular provides a function to save the NVTabular workflow via `export_hugectr_ensemble`. This function does not only save NVTabular workflow, but also saves the trained HugeCTR model and ensemble model to be served to Triton IS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-activation",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-manner",
   "metadata": {},
   "source": [
    "Let's import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "increased-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from tritonclient.utils import *\n",
    "import tritonclient.grpc as httpclient\n",
    "\n",
    "import nvtabular as nvt\n",
    "import cudf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-adobe",
   "metadata": {},
   "source": [
    "### Load Models on Triton Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-richards",
   "metadata": {},
   "source": [
    "At this staged, you should have already launched the Triton Inference Server docker container with the following script:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-belfast",
   "metadata": {},
   "source": [
    "```\n",
    "docker run -it --gpus=all -p 8000:8000 -p 8001:8001 -p 8002:8002 -v ${PWD}:/model nvcr.io/nvidia/merlin_inference\n",
    "\n",
    "source activate rapids\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-forge",
   "metadata": {},
   "source": [
    "After you started the container you can start triton server with the command below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-guitar",
   "metadata": {},
   "source": [
    "```\n",
    "tritonserver --model-repository=path_to_models --backend-config=hugectr,movielens=path_to_json_file --backend-config=hugectr,supportlonglong=true --model-control-mode=explicit \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-jonathan",
   "metadata": {},
   "source": [
    "Note: The model-repository path is `/model/models/`. The models haven't been loaded, yet. We can request triton server to load the saved ensemble.  We initialize a triton client. The path for the json file is `/model/models/movielens/1/movielens.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "global-approval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import tritonhttpclient\n",
    "try:\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aging-youth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.is_server_live() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "neural-cooperative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '162'}>\n",
      "bytearray(b'[{\"name\":\"movielens\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"movielens_ens\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"movielens_nvt\",\"version\":\"1\",\"state\":\"READY\"}]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'movielens', 'version': '1', 'state': 'READY'},\n",
       " {'name': 'movielens_ens', 'version': '1', 'state': 'READY'},\n",
       " {'name': 'movielens_nvt', 'version': '1', 'state': 'READY'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.get_model_repository_index() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-valuable",
   "metadata": {},
   "source": [
    "We load our ensemble model `movielens_ens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dominican-biotechnology",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/models/movielens_ens/load, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'movielens_ens'\n",
      "CPU times: user 1.17 ms, sys: 6.15 ms, total: 7.32 ms\n",
      "Wall time: 32.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "triton_client.load_model(model_name='movielens_ens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "jewish-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings can be disabled\n",
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-baptist",
   "metadata": {},
   "source": [
    "Let's send a request to Inference Server and print out the response. Since in our example above we do not have continuous columns, below our only inputs are categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "recent-signature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId  userId\n",
      "0    19997   99476\n",
      "1     2543  107979\n",
      "2     1557  155372 \n",
      "\n",
      "predicted softmax result:\n",
      " [0.57567745 0.5913081  0.53693664]\n"
     ]
    }
   ],
   "source": [
    "import tritonclient.grpc as httpclient\n",
    "import nvtabular\n",
    "import cudf\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "model_name = 'movielens_ens'\n",
    "col_names = [\"movieId\", \"userId\"]\n",
    "# read in a batch of data to get transforms for\n",
    "batch = cudf.read_parquet('/model/data/valid/*.parquet', num_rows=3)[col_names]\n",
    "print(batch, \"\\n\")\n",
    "\n",
    "# convert the batch to a triton inputs\n",
    "columns = [(col, batch[col][0:3]) for col in col_names]\n",
    "inputs = []\n",
    "\n",
    "col_dtypes = [np.int64, np.int64]\n",
    "for i, (name, col) in enumerate(columns):\n",
    "    d = col.values_host.astype(col_dtypes[i])\n",
    "    d = d.reshape(len(d), 1)\n",
    "    inputs.append(httpclient.InferInput(name, d.shape, np_to_triton_dtype(col_dtypes[i])))\n",
    "    inputs[i].set_data_from_numpy(d)\n",
    "# placeholder variables for the output\n",
    "outputs = []\n",
    "outputs.append(httpclient.InferRequestedOutput(\"OUTPUT0\"))\n",
    "# make the request\n",
    "with httpclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer(model_name, inputs, request_id=str(1), outputs=outputs)\n",
    "# print(response.as_numpy('OUTPUT0'))\n",
    "print(\"predicted softmax result:\\n\", response.as_numpy(\"OUTPUT0\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
