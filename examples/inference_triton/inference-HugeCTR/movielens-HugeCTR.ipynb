{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "personalized-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-batch",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we want to provide a overview what HugeCTR framework is, its features and benefits. We will use HugeCTR to train a basic neural network architecture and deploy the saved model to Triton Inference Server. \n",
    "\n",
    "<b>Learning Objectives</b>:\n",
    "* Adopt NVTabular workflow to provide input files to HugeCTR\n",
    "* Define HugeCTR neural network architecture\n",
    "* Train a deep learning model with HugeCTR\n",
    "* Deploy HugeCTR to Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-vessel",
   "metadata": {},
   "source": [
    "### Why using HugeCTR?\n",
    "\n",
    "HugeCTR is a GPU-accelerated recommender framework designed to distribute training across multiple GPUs and nodes and estimate Click-Through Rates (CTRs).<br>\n",
    "\n",
    "HugeCTR offers multiple advantages to train deep learning recommender systems:\n",
    "1. **Speed**: HugeCTR is a highly efficient framework written C++. We experienced upto 10x speed up. HugeCTR on a NVIDIA DGX A100 system proved to be the fastest commercially available solution for training the architecture Deep Learning Recommender Model (DLRM) developed by Facebook.\n",
    "2. **Scale**: HugeCTR supports model parallel scaling. It distributes the large embedding tables over multiple GPUs or multiple nodes. \n",
    "3. **Easy-to-use**: Easy-to-use Python API similar to Keras. Examples for popular deep learning recommender systems architectures (Wide&Deep, DLRM, DCN, DeepFM) are available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-accused",
   "metadata": {},
   "source": [
    "### Other Features of HugeCTR\n",
    "\n",
    "HugeCTR is designed to scale deep learning models for recommender systems. It provides a list of other important features:\n",
    "* Proficiency in oversubscribing models to train embedding tables with single nodes that donâ€™t fit within the GPU or CPU memory (only required embeddings are prefetched from a parameter server per batch)\n",
    "* Asynchronous and multithreaded data pipelines\n",
    "* A highly optimized data loader.\n",
    "* Supported data formats such as parquet and binary\n",
    "* Integration with Triton Inference Server for deployment to production\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-bibliography",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-captain",
   "metadata": {},
   "source": [
    "In this example, we will train a neural network with HugeCTR. We will use NVTabular for preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-swedish",
   "metadata": {},
   "source": [
    "#### Preprocessing and Feature Engineering with NVTabular\n",
    "\n",
    "We use NVTabular for `Categorify` our categorical input columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "palestinian-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "marked-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import nvtabular as nvt\n",
    "import cudf \n",
    "import numpy as np\n",
    "\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-sunset",
   "metadata": {},
   "source": [
    "We define our base directory, containing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "blessed-miracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to store raw and preprocesses data\n",
    "BASE_DIR = '/model/movie/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-showcase",
   "metadata": {},
   "source": [
    "If the data is not available in the base directory, we will download and unzip the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "handled-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.exists(BASE_DIR + 'ml-25m'):\n",
    "    if not path.exists(BASE_DIR + 'ml-25m.zip'):\n",
    "        os.system(\"wget http://files.grouplens.org/datasets/movielens/ml-25m.zip\")\n",
    "        os.system(\"mv ml-25m.zip \" + BASE_DIR)\n",
    "    os.system(\"unzip \" + BASE_DIR + \"ml-25m.zip -d \" + BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-cancer",
   "metadata": {},
   "source": [
    "## Preparing the dataset with NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-maria",
   "metadata": {},
   "source": [
    "First, we take a look on the movie metadata.\n",
    "\n",
    "We load the movie ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "flush-contest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147880044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>306</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1147868817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147868828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>665</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147878820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>899</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1147868510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1      296     5.0  1147880044\n",
       "1       1      306     3.5  1147868817\n",
       "2       1      307     5.0  1147868828\n",
       "3       1      665     5.0  1147878820\n",
       "4       1      899     3.5  1147868510"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = cudf.read_csv(os.path.join(BASE_DIR, \"ml-25m\", \"ratings.csv\"))\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-fairy",
   "metadata": {},
   "source": [
    "We drop the timestamp column and split the ratings into training and test dataset. We use a simple random split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "heard-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.drop('timestamp', axis=1)\n",
    "train, valid = train_test_split(ratings, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "authentic-agenda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19120594</th>\n",
       "      <td>124027</td>\n",
       "      <td>56587</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15253127</th>\n",
       "      <td>98809</td>\n",
       "      <td>2641</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12584278</th>\n",
       "      <td>81377</td>\n",
       "      <td>122886</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18034326</th>\n",
       "      <td>116853</td>\n",
       "      <td>78499</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18073299</th>\n",
       "      <td>117118</td>\n",
       "      <td>1302</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          userId  movieId  rating\n",
       "19120594  124027    56587     4.0\n",
       "15253127   98809     2641     2.5\n",
       "12584278   81377   122886     4.5\n",
       "18034326  116853    78499     4.5\n",
       "18073299  117118     1302     2.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-fifteen",
   "metadata": {},
   "source": [
    "We save our train and valid datasets as parquet files on disk, and below we will read them in while initializing the Dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "motivated-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(BASE_DIR + 'train.parquet')\n",
    "valid.to_parquet(BASE_DIR + 'valid.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "infinite-narrative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train\n",
    "del valid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-edgar",
   "metadata": {},
   "source": [
    "Let's define our categorical and label columns. Note that in that example wee do not have numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "perceived-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = ['userId', 'movieId']\n",
    "LABEL_COLUMNS = ['rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-occasions",
   "metadata": {},
   "source": [
    "Let's add Categorify op for our categorical features, userId, movieId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "handy-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = CATEGORICAL_COLUMNS >> nvt.ops.Categorify(cat_cache=\"device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-retail",
   "metadata": {},
   "source": [
    "The ratings are on a scale between 1-5. We want to predict a binary target with 1 are all ratings >=4 and 0 are all ratings <=3. We use the LambdaOp for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mysterious-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = nvt.ColumnGroup(['rating']) >> (lambda col: (col>3).astype('int8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-damage",
   "metadata": {},
   "source": [
    "We can visualize our calculation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "meaningful-acceptance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"510pt\" height=\"260pt\"\n",
       " viewBox=\"0.00 0.00 510.43 260.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-256 506.43,-256 506.43,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"372.64\" cy=\"-162\" rx=\"61.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"372.64\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">LambdaOp</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"272.64\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"272.64\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>0&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M349.95,-145.12C334.78,-134.5 314.77,-120.49 298.91,-109.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"300.78,-106.43 290.58,-103.56 296.77,-112.16 300.78,-106.43\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"403.64\" cy=\"-234\" rx=\"98.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"403.64\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">input cols=[rating]</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M395.97,-215.7C392.45,-207.73 388.18,-198.1 384.27,-189.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"387.46,-187.83 380.21,-180.1 381.06,-190.67 387.46,-187.83\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"143.64\" cy=\"-234\" rx=\"143.77\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"143.64\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">input cols=[userId, movieId]</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"203.64\" cy=\"-162\" rx=\"59.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"203.64\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">Categorify</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.16,-216.05C165.53,-207.46 174.62,-196.86 182.73,-187.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"185.4,-189.66 189.25,-179.79 180.08,-185.1 185.4,-189.66\"/>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M219.99,-144.41C229.26,-135 240.98,-123.12 251.01,-112.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"253.75,-115.15 258.27,-105.57 248.76,-110.24 253.75,-115.15\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"272.64\" cy=\"-18\" rx=\"183.87\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"272.64\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">output cols=[userId, movieId, rating]</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M272.64,-71.7C272.64,-63.98 272.64,-54.71 272.64,-46.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"276.14,-46.1 272.64,-36.1 269.14,-46.1 276.14,-46.1\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7effb6f23cd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = cat_features+ratings\n",
    "(output).graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-perception",
   "metadata": {},
   "source": [
    "We initialize our NVTabular workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "electrical-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = nvt.Workflow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-tunisia",
   "metadata": {},
   "source": [
    "We initialize NVTabular Datasets, and use the part_size parameter, which defines the size read into GPU-memory at once, in nvt.Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "protecting-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = nvt.Dataset(BASE_DIR + 'train.parquet', part_size='100MB')\n",
    "valid_dataset = nvt.Dataset(BASE_DIR + 'valid.parquet', part_size='100MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-burton",
   "metadata": {},
   "source": [
    "First, we collect the training dataset statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "informational-summit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 407 ms, total: 1.51 s\n",
      "Wall time: 1.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "workflow.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-stocks",
   "metadata": {},
   "source": [
    "This step is slightly different for HugeCTR. HugeCTR expect the categorical input columns as `int64` and continuous/label columns as `float32`  We can define output datatypes for our NVTabular workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "norwegian-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dtypes={}\n",
    "\n",
    "for col in CATEGORICAL_COLUMNS:\n",
    "    dict_dtypes[col] = np.int64\n",
    "    \n",
    "for col in LABEL_COLUMNS:\n",
    "    dict_dtypes[col] = np.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-photography",
   "metadata": {},
   "source": [
    "Note: We do not have numerical output columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "waiting-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "if path.exists(BASE_DIR + 'train'): \n",
    "    !rm -r $BASE_DIR/train\n",
    "if path.exists(BASE_DIR + 'valid'): \n",
    "    !rm -r $BASE_DIR/valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-suspect",
   "metadata": {},
   "source": [
    "In addition, we need to provide the data schema to the output calls. We need to define which output columns are `categorical`, `continuous` and which is the `label` columns. NVTabular will write metadata files, which HugeCTR requires to load the data and optimize training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "gorgeous-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.transform(train_dataset).to_parquet(output_path=BASE_DIR + 'train/', \n",
    "                                             shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "                                             cats=CATEGORICAL_COLUMNS,\n",
    "                                             labels=LABEL_COLUMNS,\n",
    "                                             dtypes=dict_dtypes\n",
    "                                            )\n",
    "workflow.transform(valid_dataset).to_parquet(output_path=BASE_DIR + 'valid/', \n",
    "                                             shuffle=False,\n",
    "                                             cats=CATEGORICAL_COLUMNS,\n",
    "                                             labels=LABEL_COLUMNS,\n",
    "                                             dtypes=dict_dtypes\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-reggae",
   "metadata": {},
   "source": [
    "## Scaling Accelerated training with HugeCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-ribbon",
   "metadata": {},
   "source": [
    "HugeCTR is a deep learning framework dedicated to recommendation systems. It is written in CUDA C++. As HugeCTR optimizes the training in CUDA++, we need to define the training pipeline and model architecture and execute it via the commandline. We will use the Python API, which is similar to Keras models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-portal",
   "metadata": {},
   "source": [
    "HugeCTR has three main components:\n",
    "* Solver: Specifies various details such as active GPU list, batchsize, and model_file\n",
    "* Optimizer: Specifies the type of optimizer and its hyperparameters\n",
    "* Model: Specifies training/evaluation data (and their paths), embeddings, and dense layers. Note that embeddings must precede the dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-spouse",
   "metadata": {},
   "source": [
    "**Solver**\n",
    "\n",
    "Let's take a look on the parameter for the `Solver`. We should be familiar from other frameworks for the hyperparameter.\n",
    "\n",
    "```\n",
    "solver = hugectr.solver_parser_helper(\n",
    "- vvgpu: GPU indices used in the training process, which has two levels. For example: [[0,1],[1,2]] indicates that two nodes are used in the first node. GPUs 0 and 1 are used while GPUs 1 and 2 are used for the second node. It is also possible to specify non-continuous GPU indices such as [0, 2, 4, 7]  \n",
    "- max_iter: Total number of training iterations\n",
    "- batchsize: Minibatch size used in training\n",
    "- display: Intervals to print loss on the screen\n",
    "- eval_interval: Evaluation interval in the unit of training iteration\n",
    "- max_eval_batches: Maximum number of batches used in evaluation. It is recommended that the number is equal to or bigger than the actual number of bathces in the evaluation dataset.\n",
    "If max_iter is used, the evaluation happens for max_eval_batches by repeating the evaluation dataset infinitely.\n",
    "On the other hand, with num_epochs, HugeCTR stops the evaluation if all the evaluation data is consumed    \n",
    "- batchsize_eval: Maximum number of batches used in evaluation. It is recommended that the number is equal to or\n",
    "  bigger than the actual number of bathces in the evaluation dataset\n",
    "- mixed_precision: Enables mixed precision training with the scaler specified here. Only 128,256, 512, and 1024 scalers are supported\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-american",
   "metadata": {},
   "source": [
    "**Optimizer**\n",
    "\n",
    "The optimizer is the algorithm to update the model parameters. HugeCTR supports the common algorithms.\n",
    "\n",
    "\n",
    "```\n",
    "optimizer = CreateOptimizer(\n",
    "- optimizer_type: Optimizer algorithm - Adam, MomentumSGD, Nesterov, and SGD \n",
    "- learning_rate: Learning Rate for optimizer\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-abuse",
   "metadata": {},
   "source": [
    "**Model**\n",
    "\n",
    "We initialize the model with the solver and optimizer:\n",
    "\n",
    "```\n",
    "model = hugectr.Model(solver, optimizer)\n",
    "```\n",
    "\n",
    "We can add multiple layers to the model with `model.add` function. We will focus on:\n",
    "- `Input` defines the input data\n",
    "- `SparseEmbedding` defines the embedding layer\n",
    "- `DenseLayer` defines dense layers, such as fully connected, ReLU, BatchNorm, etc.\n",
    "\n",
    "**HugeCTR organizes the layers by names. For each layer, we define the input and output names.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-belle",
   "metadata": {},
   "source": [
    "Input layer:\n",
    "\n",
    "This layer is required to define the input data.\n",
    "\n",
    "```\n",
    "hugectr.Input(\n",
    "    data_reader_type: Data format to read\n",
    "    source: The training dataset file list.\n",
    "    eval_source: The evaluation dataset file list.\n",
    "    check_type: The data error detection machanism (Sum: Checksum, None: no detection).\n",
    "    label_dim: Number of label columns\n",
    "    label_name: Name of label columns in network architecture\n",
    "    dense_dim: Number of continous columns\n",
    "    dense_name: Name of contiunous columns in network architecture\n",
    "    slot_size_array: The list of categorical feature cardinalities\n",
    "    data_reader_sparse_param_array: Configuration how to read sparse data\n",
    "    sparse_names: Name of sparse/categorical columns in network architecture\n",
    ")\n",
    "```\n",
    "\n",
    "SparseEmbedding:\n",
    "\n",
    "This layer defines embedding table\n",
    "\n",
    "```\n",
    "hugectr.SparseEmbedding(\n",
    "    embedding_type: Different embedding options to distribute embedding tables \n",
    "    max_vocabulary_size_per_gpu: Maximum vocabulary size or cardinality across all the input features\n",
    "    embedding_vec_size: Embedding vector size\n",
    "    combiner: Intra-slot reduction op (0=sum, 1=average)\n",
    "    sparse_embedding_name: Layer name\n",
    "    bottom_name: Input layer names\n",
    ")\n",
    "```\n",
    "\n",
    "DenseLayer:\n",
    "\n",
    "This layer is copied to each GPU and is normally used for the MLP tower.\n",
    "\n",
    "```\n",
    "hugectr.DenseLayer(\n",
    "    layer_type: Layer type, such as FullyConnected, Reshape, Concat, Loss, BatchNorm, etc.\n",
    "    bottom_names: Input layer names\n",
    "    top_names: Layer name\n",
    "    ...: Depending on the layer type additional parameter can be defined\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-taste",
   "metadata": {},
   "source": [
    "## Let's define our model\n",
    "\n",
    "We walked through the documentation, but it is useful to understand the API. Finally, we can define our model. We will write the model to `./model.py` and execute it afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-colombia",
   "metadata": {},
   "source": [
    "We need the cardinalities of each categorical feature to assign as `slot_size_array` in the model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "demographic-giant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movieId': (56586, 512), 'userId': (162542, 512)}\n"
     ]
    }
   ],
   "source": [
    "from nvtabular.ops import get_embedding_sizes\n",
    "\n",
    "embeddings = get_embedding_sizes(workflow)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-protein",
   "metadata": {},
   "source": [
    "In addition, we need the total cardinalities to be assigned as `max_vocabulary_size_per_gpu` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "peaceful-terrain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219128"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_cardinality = embeddings['movieId'][0] + embeddings['userId'][0] \n",
    "total_cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "specific-physics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './model.py'\n",
    "\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "\n",
    "solver = hugectr.solver_parser_helper(vvgpu = [[0]],\n",
    "                                      max_iter = 2000,\n",
    "                                      batchsize = 2048,\n",
    "                                      display = 100,\n",
    "                                      eval_interval = 200,\n",
    "                                      batchsize_eval = 2048,\n",
    "                                      max_eval_batches = 160,\n",
    "                                      i64_input_key = True,\n",
    "                                      use_mixed_precision = False,\n",
    "                                      repeat_dataset = True,\n",
    "                                      snapshot = 1900\n",
    "                                      )\n",
    "optimizer = hugectr.optimizer.CreateOptimizer(\n",
    "    optimizer_type = hugectr.Optimizer_t.Adam,\n",
    "    use_mixed_precision = False\n",
    ")\n",
    "model = hugectr.Model(solver, optimizer)\n",
    "\n",
    "model.add(\n",
    "    hugectr.Input(\n",
    "        data_reader_type = hugectr.DataReaderType_t.Parquet,\n",
    "        source = \"/model/movie/data/train/_file_list.txt\",\n",
    "        eval_source = \"/model/movie/data/valid/_file_list.txt\",\n",
    "        check_type = hugectr.Check_t.Non,\n",
    "        label_dim = 1, \n",
    "        label_name = \"label\",\n",
    "        dense_dim = 0, \n",
    "        dense_name = \"dense\",\n",
    "        slot_size_array = [56586, 162542],\n",
    "        data_reader_sparse_param_array = [\n",
    "            hugectr.DataReaderSparseParam(hugectr.DataReaderSparse_t.Distributed, 3, 1, 2)\n",
    "        ],\n",
    "        sparse_names = [\"data1\"]\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.SparseEmbedding(\n",
    "        embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "        max_vocabulary_size_per_gpu = 219128,\n",
    "        embedding_vec_size = 16,\n",
    "        combiner = 0,\n",
    "        sparse_embedding_name = \"sparse_embedding1\",\n",
    "        bottom_name = \"data1\"\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type = hugectr.Layer_t.Reshape,\n",
    "        bottom_names = [\"sparse_embedding1\"],\n",
    "        top_names = [\"reshape1\"],\n",
    "        leading_dim=32\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type = hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names = [\"reshape1\"], \n",
    "        top_names = [\"fc1\"],\n",
    "        num_output=128\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type = hugectr.Layer_t.ReLU,\n",
    "        bottom_names = [\"fc1\"], \n",
    "        top_names = [\"relu1\"],\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type = hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names = [\"relu1\"], \n",
    "        top_names = [\"fc2\"],\n",
    "        num_output=128\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type = hugectr.Layer_t.ReLU,\n",
    "        bottom_names = [\"fc2\"], \n",
    "        top_names = [\"relu2\"],\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type = hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names = [\"relu2\"], \n",
    "        top_names = [\"fc3\"],\n",
    "        num_output=1\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "        bottom_names = [\"fc3\", \"label\"],\n",
    "        top_names = [\"loss\"])\n",
    ")\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "silent-leadership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================Model Init====================================\n",
      "[08d20h25m32s][HUGECTR][INFO]: Global seed is 4040723604\n",
      "[08d20h25m34s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 0: Tesla V100-DGXS-16GB\n",
      "[08d20h25m34s][HUGECTR][INFO]: num of DataReader workers: 1\n",
      "[08d20h25m34s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[08d20h25m34s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[08d20h25m34s][HUGECTR][INFO]: Vocabulary size: 219128\n",
      "[08d20h25m34s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=219128\n",
      "[08d20h25m36s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[08d20h25m36s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "==================================Model Summary==================================\n",
      "Label Name                    Dense Name                    Sparse Name                   \n",
      "label                         dense                         data1                         \n",
      "--------------------------------------------------------------------------------\n",
      "Layer Type                    Input Name                    Output Name                   \n",
      "--------------------------------------------------------------------------------\n",
      "DistributedHash               data1                         sparse_embedding1             \n",
      "Reshape                       sparse_embedding1             reshape1                      \n",
      "InnerProduct                  reshape1                      fc1                           \n",
      "ReLU                          fc1                           relu1                         \n",
      "InnerProduct                  relu1                         fc2                           \n",
      "ReLU                          fc2                           relu2                         \n",
      "InnerProduct                  relu2                         fc3                           \n",
      "BinaryCrossEntropyLoss        fc3, label                    loss                          \n",
      "--------------------------------------------------------------------------------\n",
      "=====================================Model Fit====================================\n",
      "[80d20h25m36s][HUGECTR][INFO]: Use non-epoch mode with number of iterations: 2000\n",
      "[80d20h25m36s][HUGECTR][INFO]: Training batchsize: 2048, evaluation batchsize: 2048\n",
      "[80d20h25m36s][HUGECTR][INFO]: Evaluation interval: 200, snapshot interval: 1900\n",
      "[80d20h25m36s][HUGECTR][INFO]: Iter: 100 Time(100 iters): 0.055625s Loss: 0.607102 lr:0.001000\n",
      "[80d20h25m36s][HUGECTR][INFO]: Iter: 200 Time(100 iters): 0.053111s Loss: 0.540543 lr:0.001000\n",
      "[80d20h25m36s][HUGECTR][INFO]: Evaluation, AUC: 0.744238\n",
      "[80d20h25m36s][HUGECTR][INFO]: Eval Time for 160 iters: 0.036843s\n",
      "[80d20h25m36s][HUGECTR][INFO]: Iter: 300 Time(100 iters): 0.099981s Loss: 0.545304 lr:0.001000\n",
      "[80d20h25m36s][HUGECTR][INFO]: Iter: 400 Time(100 iters): 0.053300s Loss: 0.575900 lr:0.001000\n",
      "[80d20h25m36s][HUGECTR][INFO]: Evaluation, AUC: 0.763224\n",
      "[80d20h25m36s][HUGECTR][INFO]: Eval Time for 160 iters: 0.036459s\n",
      "[80d20h25m36s][HUGECTR][INFO]: Iter: 500 Time(100 iters): 0.102709s Loss: 0.557538 lr:0.001000\n",
      "[80d20h25m36s][HUGECTR][INFO]: Iter: 600 Time(100 iters): 0.053383s Loss: 0.548302 lr:0.001000\n",
      "[80d20h25m36s][HUGECTR][INFO]: Evaluation, AUC: 0.773010\n",
      "[80d20h25m36s][HUGECTR][INFO]: Eval Time for 160 iters: 0.035354s\n",
      "[80d20h25m36s][HUGECTR][INFO]: Iter: 700 Time(100 iters): 0.090095s Loss: 0.527441 lr:0.001000\n",
      "[80d20h25m36s][HUGECTR][INFO]: Iter: 800 Time(100 iters): 0.053509s Loss: 0.540253 lr:0.001000\n",
      "[80d20h25m36s][HUGECTR][INFO]: Evaluation, AUC: 0.779688\n",
      "[80d20h25m36s][HUGECTR][INFO]: Eval Time for 160 iters: 0.046150s\n",
      "[80d20h25m36s][HUGECTR][INFO]: Iter: 900 Time(100 iters): 0.100960s Loss: 0.526807 lr:0.001000\n",
      "[80d20h25m36s][HUGECTR][INFO]: Iter: 1000 Time(100 iters): 0.064367s Loss: 0.533041 lr:0.001000\n",
      "[80d20h25m36s][HUGECTR][INFO]: Evaluation, AUC: 0.784037\n",
      "[80d20h25m36s][HUGECTR][INFO]: Eval Time for 160 iters: 0.035768s\n",
      "[80d20h25m36s][HUGECTR][INFO]: Iter: 1100 Time(100 iters): 0.090310s Loss: 0.531932 lr:0.001000\n",
      "[80d20h25m36s][HUGECTR][INFO]: Iter: 1200 Time(100 iters): 0.053292s Loss: 0.509043 lr:0.001000\n",
      "[80d20h25m36s][HUGECTR][INFO]: Evaluation, AUC: 0.784648\n",
      "[80d20h25m36s][HUGECTR][INFO]: Eval Time for 160 iters: 0.035238s\n",
      "[80d20h25m36s][HUGECTR][INFO]: Iter: 1300 Time(100 iters): 0.089956s Loss: 0.535366 lr:0.001000\n",
      "[80d20h25m37s][HUGECTR][INFO]: Iter: 1400 Time(100 iters): 0.053450s Loss: 0.515414 lr:0.001000\n",
      "[80d20h25m37s][HUGECTR][INFO]: Evaluation, AUC: 0.789208\n",
      "[80d20h25m37s][HUGECTR][INFO]: Eval Time for 160 iters: 0.046380s\n",
      "[80d20h25m37s][HUGECTR][INFO]: Iter: 1500 Time(100 iters): 0.112580s Loss: 0.518985 lr:0.001000\n",
      "[80d20h25m37s][HUGECTR][INFO]: Iter: 1600 Time(100 iters): 0.053552s Loss: 0.512319 lr:0.001000\n",
      "[80d20h25m37s][HUGECTR][INFO]: Evaluation, AUC: 0.792395\n",
      "[80d20h25m37s][HUGECTR][INFO]: Eval Time for 160 iters: 0.037367s\n",
      "[80d20h25m37s][HUGECTR][INFO]: Iter: 1700 Time(100 iters): 0.092504s Loss: 0.526403 lr:0.001000\n",
      "[80d20h25m37s][HUGECTR][INFO]: Iter: 1800 Time(100 iters): 0.053652s Loss: 0.524173 lr:0.001000\n",
      "[80d20h25m37s][HUGECTR][INFO]: Evaluation, AUC: 0.793604\n",
      "[80d20h25m37s][HUGECTR][INFO]: Eval Time for 160 iters: 0.037604s\n",
      "[80d20h25m37s][HUGECTR][INFO]: Iter: 1900 Time(100 iters): 0.092954s Loss: 0.530401 lr:0.001000\n",
      "[80d20h25m37s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[80d20h25m37s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[80d20h25m37s][HUGECTR][INFO]: Done\n"
     ]
    }
   ],
   "source": [
    "!python model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-voltage",
   "metadata": {},
   "source": [
    "We trained our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-approval",
   "metadata": {},
   "source": [
    "After training terminates, we can see that two `.model` files are generated. We need to move them inside `1` folder under the `movielens_hugectr` folder. Let's create these folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "blessed-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /model/movielens_hugectr/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-cursor",
   "metadata": {},
   "source": [
    "Now we move our saved `.model` files inside `1` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "personalized-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv *.model /model/movielens_hugectr/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-airport",
   "metadata": {},
   "source": [
    "Note that these stored model files under the `movielens/1/` will be used in the inference. Now we have to create a JSON file for inference which has a similar configuration as our training file. We should remove the solver and optimizer clauses and add the inference clause in the JSON file. The paths of the stored dense model and sparse model(s) should be specified at dense_model_file and sparse_model_file within the inference clause. We need to make some modifications to data in the layers clause. Besides, we need to change the last layer from BinaryCrossEntropyLoss to Sigmoid. The rest of \"layers\" should be exactly the same as that in the training model.py file.\n",
    "\n",
    "Now let's create a `movielens.json` file inside the `movielens/1` folder. We have already retrieved the cardinality of each categorical column using get_embedding_sizes function above. We will use these cardinalities below in the `movielens.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "compressed-spoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /model/movielens_hugectr/1/movielens.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile '/model/movielens_hugectr/1/movielens.json'\n",
    "\n",
    "{\n",
    "   \"inference\": {\n",
    "    \"max_batchsize\": 64,\n",
    "    \"hit_rate_threshold\": 0.6,\n",
    "    \"dense_model_file\": \"/model/models/movielens/1/_dense_1900.model\",\n",
    "    \"sparse_model_file\": \"/model/models/movielens/1/0_sparse_1900.model\",\n",
    "    \"label\": 1,\n",
    "    \"input_key_type\": \"I64\"\n",
    "  },\n",
    "  \"layers\": [\n",
    "    {\n",
    "      \"name\": \"data\",\n",
    "      \"type\": \"Data\",\n",
    "      \"format\": \"Parquet\",\n",
    "      \"slot_size_array\": [56586, 162542],\n",
    "      \"source\": \"/model/movie/data/train/_file_list.txt\",\n",
    "      \"eval_source\": \"/model/movie/data/valid/_file_list.txt\",\n",
    "      \"check\": \"Sum\",\n",
    "      \"label\": {\n",
    "        \"top\": \"label\",\n",
    "        \"label_dim\": 1\n",
    "      },\n",
    "      \"dense\": {\n",
    "        \"top\": \"dense\",\n",
    "        \"dense_dim\": 0\n",
    "      },\n",
    "      \"sparse\": [\n",
    "        {\n",
    "          \"top\": \"data1\",\n",
    "          \"type\": \"DistributedSlot\",\n",
    "          \"max_feature_num_per_sample\": 3,\n",
    "          \"slot_num\": 2\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"sparse_embedding1\",\n",
    "      \"type\": \"DistributedSlotSparseEmbeddingHash\",\n",
    "      \"bottom\": \"data1\",\n",
    "      \"top\": \"sparse_embedding1\",\n",
    "      \"sparse_embedding_hparam\": {\n",
    "        \"max_vocabulary_size_per_gpu\": 219128,\n",
    "        \"embedding_vec_size\": 16,\n",
    "        \"combiner\": 0\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"reshape1\",\n",
    "      \"type\": \"Reshape\",\n",
    "      \"bottom\": \"sparse_embedding1\",\n",
    "      \"top\": \"reshape1\",\n",
    "      \"leading_dim\": 32\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc1\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"reshape1\",\n",
    "      \"top\": \"fc1\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 128\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"relu1\",\n",
    "      \"type\": \"ReLU\",\n",
    "      \"bottom\": \"fc1\",\n",
    "      \"top\": \"relu1\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc2\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"relu1\",\n",
    "      \"top\": \"fc2\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 128\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"relu2\",\n",
    "      \"type\": \"ReLU\",\n",
    "      \"bottom\": \"fc2\",\n",
    "      \"top\": \"relu2\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc3\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"relu2\",\n",
    "      \"top\": \"fc3\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 1\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"sigmoid\",\n",
    "      \"type\": \"Sigmoid\",\n",
    "      \"bottom\": \"fc3\",\n",
    "      \"top\": \"sigmoid\"\n",
    "    } \n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-harris",
   "metadata": {},
   "source": [
    "Now we can save our models to be deployed at the inference stage. To do so we will use `export_hugectr_ensemble` method below. With this method, we can generate the `config.pbtxt` files automatically for each model. In doing so, we should also create a `hugectr_params` dictionary, and define the parameters  like where the `movielens.json` file will be read, `slots` which corresponds to number of categorical features, `embedding_vector_size`, `max_nnz`, and `n_outputs` which is number of outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-effectiveness",
   "metadata": {},
   "source": [
    "The script below creates an ensemble triton server model where  \n",
    "\n",
    "- `workflow` is the the nvtabular workflow used in preprocessing, \n",
    "- `hugectr_model_path` is the HugeCTR model that should be served. This path includes the `.model` files.\n",
    "- `name` is the base name of the various triton models\n",
    "- `output_path` is the path where is model will be saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "proved-mineral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvtabular.inference.triton import export_hugectr_ensemble\n",
    "hugectr_params = dict()\n",
    "hugectr_params[\"config\"] = \"/model/models/movielens/1/movielens.json\"\n",
    "hugectr_params[\"slots\"] = 2\n",
    "hugectr_params[\"max_nnz\"] = 2\n",
    "hugectr_params[\"embedding_vector_size\"] = 16\n",
    "hugectr_params[\"n_outputs\"] = 1\n",
    "export_hugectr_ensemble(workflow=workflow, \n",
    "                        hugectr_model_path=\"/model/movielens_hugectr/1/\",\n",
    "                        hugectr_params=hugectr_params,\n",
    "                        name=\"movielens\", \n",
    "                        output_path=\"/model/models/\", \n",
    "                        label_columns=[\"rating\"], \n",
    "                        cats=CATEGORICAL_COLUMNS,\n",
    "                        max_batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
