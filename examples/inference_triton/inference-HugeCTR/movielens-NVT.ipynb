{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "## Overview\n",
    "\n",
    "NVTabular is a feature engineering and preprocessing library for tabular data designed to quickly and easily manipulate terabyte scale datasets used to train deep learning based recommender systems.  It provides a high level abstraction to simplify code and accelerates computation on the GPU using the RAPIDS cuDF library.<br><br>\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "In this notebook, we show how to\n",
    "\n",
    "- do preprocessing with NVTabular\n",
    "- serialize and save a workflow to load later to transform new dataset\n",
    "- train a TF MLP model and save it in the /models directory.\n",
    "- save the ensemble model to be used at the deployment stage.\n",
    "\n",
    "### MovieLens25M\n",
    "\n",
    "The [MovieLens25M](https://grouplens.org/datasets/movielens/25m/) is a popular dataset for recommender systems and is used in academic publications. The dataset contains 25M movie ratings for 62,000 movies given by 162,000 users. Many projects use only the user/item/rating information of MovieLens, but the original dataset provides metadata for the movies, as well. For example, which genres a movie has. Although we may not improve state-of-the-art results with our neural network architecture, the purpose of this notebook is to explain how to save an ensemble model which consists of NVTabular workflow and Tensorflow model to be used later at model deployment stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import cudf                 # cuDF is an implementation of Pandas-like Dataframe on GPU\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nvtabular as nvt\n",
    "\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to store raw and preprocesses data\n",
    "BASE_DIR = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our base directory, containing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is not available in the base directory, we will download and unzip the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.exists(BASE_DIR + 'ml-25m'):\n",
    "    if not path.exists(BASE_DIR + 'ml-25m.zip'):\n",
    "        os.system(\"wget http://files.grouplens.org/datasets/movielens/ml-25m.zip\")\n",
    "        os.system(\"mv ml-25m.zip \" + BASE_DIR)\n",
    "    os.system(\"unzip \" + BASE_DIR + \"ml-25m.zip -d \" + BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset with NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we take a look on the movie metadata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the movie ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147880044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>306</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1147868817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147868828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>665</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147878820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>899</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1147868510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1      296     5.0  1147880044\n",
       "1       1      306     3.5  1147868817\n",
       "2       1      307     5.0  1147868828\n",
       "3       1      665     5.0  1147878820\n",
       "4       1      899     3.5  1147868510"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = cudf.read_csv(os.path.join(BASE_DIR, \"ml-25m\", \"ratings.csv\"))\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the timestamp column and split the ratings into training and test dataset. We use a simple random split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.drop('timestamp', axis=1)\n",
    "train, valid = train_test_split(ratings, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19120594</th>\n",
       "      <td>124027</td>\n",
       "      <td>56587</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15253127</th>\n",
       "      <td>98809</td>\n",
       "      <td>2641</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12584278</th>\n",
       "      <td>81377</td>\n",
       "      <td>122886</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18034326</th>\n",
       "      <td>116853</td>\n",
       "      <td>78499</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18073299</th>\n",
       "      <td>117118</td>\n",
       "      <td>1302</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          userId  movieId  rating\n",
       "19120594  124027    56587     4.0\n",
       "15253127   98809     2641     2.5\n",
       "12584278   81377   122886     4.5\n",
       "18034326  116853    78499     4.5\n",
       "18073299  117118     1302     2.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000076, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(BASE_DIR + 'train.parquet')\n",
    "valid.to_parquet(BASE_DIR + 'valid.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to define our NVTabular processing pipelines. NVTabular has already implemented multiple calculations, called `ops`. An `op` can be applied to a `ColumnGroup` from an overloaded `>>` operator, which in turn returns a new `ColumnGroup`. A `ColumnGroup` is a list of column names as text.<br><br>\n",
    "**Example:**<br>\n",
    "features = [*\\<column name\\>*, ...] >> *\\<op1\\>* >> *\\<op2\\>* >> ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Layers of neural networks require, that categorial features are continuous, incremental Integers: 0, 1, 2, ... , |C|-1. We need to ensure that our categorical features fullfil the requirement.<br>\n",
    "\n",
    "We should transform the single-hot categorical features userId and movieId.<br>\n",
    "\n",
    "NVTabular provides the operator `Categorify`, which provides this functionality with a high-level API out of the box. In NVTabular release v0.3, list support was added for multi-hot categorical features. Both works in the same way with no need for changes.\n",
    "\n",
    "Next, we will add `Categorify`  for our categorical features, userId, movieId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_names = ['userId', 'movieId']\n",
    "\n",
    "cat_features = cat_names >> nvt.ops.Categorify(cat_cache=\"device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratings are on a scale between 1-5. We want to predict a binary target with 1 are all ratings `>=4` and 0 are all ratings `<=3`. We use the [LambdaOp](https://nvidia.github.io/NVTabular/main/api/ops/lambdaop.html) for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = nvt.ColumnGroup(['rating']) >> (lambda col: (col>3).astype('int8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize our calculation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"510pt\" height=\"260pt\"\n",
       " viewBox=\"0.00 0.00 510.43 260.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-256 506.43,-256 506.43,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"98.79\" cy=\"-234\" rx=\"98.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"98.79\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">input cols=[rating]</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"158.79\" cy=\"-162\" rx=\"61.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"158.79\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">LambdaOp</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M113.32,-216.05C120.69,-207.46 129.77,-196.86 137.88,-187.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"140.55,-189.66 144.4,-179.79 135.24,-185.1 140.55,-189.66\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"327.79\" cy=\"-162\" rx=\"59.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"327.79\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">Categorify</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"227.79\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.79\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;4 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M305.1,-145.12C289.93,-134.5 269.92,-120.49 254.06,-109.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"255.93,-106.43 245.73,-103.56 251.92,-112.16 255.93,-106.43\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"358.79\" cy=\"-234\" rx=\"143.77\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"358.79\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">input cols=[userId, movieId]</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>3&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M351.13,-215.7C347.6,-207.73 343.34,-198.1 339.42,-189.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"342.62,-187.83 335.37,-180.1 336.22,-190.67 342.62,-187.83\"/>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M175.14,-144.41C184.42,-135 196.13,-123.12 206.16,-112.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"208.9,-115.15 213.43,-105.57 203.92,-110.24 208.9,-115.15\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"227.79\" cy=\"-18\" rx=\"183.87\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.79\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">output cols=[userId, movieId, rating]</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M227.79,-71.7C227.79,-63.98 227.79,-54.71 227.79,-46.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"231.29,-46.1 227.79,-36.1 224.29,-46.1 231.29,-46.1\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f980c6f23a0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = cat_features+ratings\n",
    "(output).graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize our NVTabular `workflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = nvt.Workflow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize NVTabular Datasets. First, we save it to a parquet file, that we can use the `part_size` parameter in `nvt.Dataset`. We want to have multiple output files and therefore, we need to use multiple partition by loading the dataset from disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train\n",
    "del valid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = ['userId', 'movieId']\n",
    "LABEL_COLUMNS = ['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dtypes={}\n",
    "\n",
    "for col in CATEGORICAL_COLUMNS:\n",
    "    dict_dtypes[col] = np.int64\n",
    "    \n",
    "for col in LABEL_COLUMNS:\n",
    "    dict_dtypes[col] = np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = nvt.Dataset(BASE_DIR + '/ml-25m/train.parquet', part_size='100MB')\n",
    "valid_dataset = nvt.Dataset(BASE_DIR + '/ml-25m/valid.parquet', part_size='100MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we collect the training dataset statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 s, sys: 417 ms, total: 1.54 s\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "workflow.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clear our output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove './/train': No such file or directory\n",
      "rm: cannot remove './/valid': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r $BASE_DIR/train\n",
    "!rm -r $BASE_DIR/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_dir=BASE_DIR + '/ml-25m/train/'\n",
    "output_valid_dir=BASE_DIR + '/ml-25m/valid/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform our workflow with `.transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 3 µs, total: 7 µs\n",
      "Wall time: 20.7 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "workflow.transform(train_dataset).to_parquet(output_path=output_train_dir, dtypes=dict_dtypes, cats=CATEGORICAL_COLUMNS, labels=LABEL_COLUMNS)\n",
    "\n",
    "workflow.transform(valid_dataset).to_parquet(output_path=output_valid_dir, \n",
    "                                             dtypes=dict_dtypes,\n",
    "                                             cats=CATEGORICAL_COLUMNS,\n",
    "                                             labels=LABEL_COLUMNS,\n",
    "                                             shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look in the output dir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow: Training Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.//ml-25m/train/0.e0c5959510a4481e8e320f9480195226.parquet']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "TRAIN_PATHS = sorted(glob.glob(output_train_dir + '*.parquet'))\n",
    "# VALID_PATHS = sorted(glob.glob(BASE_DIR + 'valid/*.parquet'))\n",
    "TRAIN_PATHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our ratings are of only 0 and 1 after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>124027</td>\n",
       "      <td>11994</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98809</td>\n",
       "      <td>2550</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81377</td>\n",
       "      <td>24262</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>116853</td>\n",
       "      <td>14786</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117118</td>\n",
       "      <td>1269</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating\n",
       "0  124027    11994     1.0\n",
       "1   98809     2550     0.0\n",
       "2   81377    24262     1.0\n",
       "3  116853    14786     1.0\n",
       "4  117118     1269     0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = cudf.read_parquet(TRAIN_PATHS[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId       int64\n",
       "movieId      int64\n",
       "rating     float32\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162541\n",
      "56585\n"
     ]
    }
   ],
   "source": [
    "# calculate cardinalities to assign \n",
    "# as slot_size_array in the model json file\n",
    "cardinalities = []\n",
    "for col in ['userId', 'movieId']:\n",
    "    print(df[col].nunique())\n",
    "    cardinalities.append(df[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[162541, 56585]\n"
     ]
    }
   ],
   "source": [
    "print(cardinalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219126"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_card = sum(cardinalities)\n",
    "total_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "638"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN HUGECTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /nvtabular/examples/movielens_config.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile '/nvtabular/examples/movielens_config.json'\n",
    "\n",
    "{\n",
    "  \"solver\": {\n",
    "    \"lr_policy\": \"fixed\",\n",
    "    \"display\": 200,\n",
    "    \"max_iter\": 10200,\n",
    "    \"gpu\": [2,3],\n",
    "    \"batchsize\": 2048,\n",
    "    \"snapshot\": 10000,\n",
    "    \"snapshot_prefix\": \"./1/\",\n",
    "    \"eval_interval\": 1000,\n",
    "    \"max_eval_batches\": 300,\n",
    "    \"input_key_type\": \"I64\"\n",
    "  },\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"Adam\",\n",
    "    \"update_type\": \"Global\",\n",
    "    \"adam_hparam\": {\n",
    "      \"learning_rate\": 0.001,\n",
    "      \"beta1\": 0.9,\n",
    "      \"beta2\": 0.999,\n",
    "      \"epsilon\": 0.0000001\n",
    "    }\n",
    "  },\n",
    "  \"layers\": [\n",
    "    {\n",
    "      \"name\": \"data\",\n",
    "      \"type\": \"Data\",\n",
    "      \"format\": \"Parquet\",\n",
    "      \"slot_size_array\": [162541, 56585],\n",
    "      \"source\": \"/working_dir/movie/data/train/_file_list.txt\",\n",
    "      \"eval_source\": \"/working_dir/movie/data/valid/_file_list.txt\",\n",
    "      \"check\": \"Sum\",\n",
    "      \"label\": {\n",
    "        \"top\": \"label\",\n",
    "        \"label_dim\": 1\n",
    "      },\n",
    "      \"dense\": {\n",
    "        \"top\": \"dense\",\n",
    "        \"dense_dim\": 0\n",
    "      },\n",
    "      \"sparse\": [\n",
    "        {\n",
    "          \"top\": \"data1\",\n",
    "          \"type\": \"DistributedSlot\",\n",
    "          \"max_feature_num_per_sample\": 3,\n",
    "          \"slot_num\": 2\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"sparse_embedding1\",\n",
    "      \"type\": \"DistributedSlotSparseEmbeddingHash\",\n",
    "      \"bottom\": \"data1\",\n",
    "      \"top\": \"sparse_embedding1\",\n",
    "      \"sparse_embedding_hparam\": {\n",
    "        \"max_vocabulary_size_per_gpu\": 219126,\n",
    "        \"embedding_vec_size\": 64,\n",
    "        \"combiner\": 0\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"reshape1\",\n",
    "      \"type\": \"Reshape\",\n",
    "      \"bottom\": \"sparse_embedding1\",\n",
    "      \"top\": \"reshape1\",\n",
    "      \"leading_dim\": 128\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc1\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"reshape1\",\n",
    "      \"top\": \"fc1\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 128\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"relu1\",\n",
    "      \"type\": \"ReLU\",\n",
    "      \"bottom\": \"fc1\",\n",
    "      \"top\": \"relu1\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc2\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"relu1\",\n",
    "      \"top\": \"fc2\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 128\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"relu2\",\n",
    "      \"type\": \"ReLU\",\n",
    "      \"bottom\": \"fc2\",\n",
    "      \"top\": \"relu2\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc3\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"relu2\",\n",
    "      \"top\": \"fc3\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 128\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"relu3\",\n",
    "      \"type\": \"ReLU\",\n",
    "      \"bottom\": \"fc3\",\n",
    "      \"top\": \"relu3\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc4\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"relu3\",\n",
    "      \"top\": \"fc4\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 1\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"loss\",\n",
    "      \"type\": \"BinaryCrossEntropyLoss\",\n",
    "      \"bottom\": [\n",
    "        \"fc4\",\n",
    "        \"label\"\n",
    "      ],\n",
    "      \"top\": \"loss\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001, init_start, ]\n",
      "HugeCTR Version: 3.0.0\n",
      "Config file: ./movielens_config.json\n",
      "[26d22h01m40s][HUGECTR][INFO]: batchsize_eval is not specified using default: 2048\n",
      "[26d22h01m40s][HUGECTR][INFO]: enable_tf32_compute is not specified using default: 0\n",
      "[26d22h01m40s][HUGECTR][INFO]: TF32 Compute: OFF\n",
      "[26d22h01m40s][HUGECTR][INFO]: Default evaluation metric is AUC without threshold value\n",
      "[26d22h01m40s][HUGECTR][INFO]: algorithm_search is not specified using default: 1\n",
      "[26d22h01m40s][HUGECTR][INFO]: Algorithm search: ON\n",
      "[26d22h01m40s][HUGECTR][INFO]: cuda_graph is not specified using default: 1\n",
      "[26d22h01m40s][HUGECTR][INFO]: CUDA Graph: ON\n",
      "[26d22h01m40s][HUGECTR][INFO]: Export prediction: OFF\n",
      "[26d22h01m40s][HUGECTR][INFO]: Global seed is 2949963101\n",
      "Device 2: Tesla V100-DGXS-16GB\n",
      "Device 3: Tesla V100-DGXS-16GB\n",
      "[26d22h01m43s][HUGECTR][INFO]: cache_eval_data is not specified using default: 0\n",
      "[26d22h01m43s][HUGECTR][INFO]: num_workers is not specified using default: 2\n",
      "[26d22h01m43s][HUGECTR][INFO]: num of DataReader workers: 2\n",
      "[26d22h01m43s][HUGECTR][INFO]: max_nnz is not specified using default: 3\n",
      "[26d22h01m43s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[26d22h01m43s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[26d22h01m43s][HUGECTR][INFO]: Vocabulary size: 219126\n",
      "[26d22h01m43s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=219126\n",
      "[26d22h01m45s][HUGECTR][INFO]: gpu1 start to init embedding\n",
      "[26d22h01m45s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[26d22h01m45s][HUGECTR][INFO]: gpu1 init embedding done\n",
      "[26d22h01m45s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[26d22h01m45s][HUGECTR][INFO]: gpu1 start to init embedding\n",
      "[26d22h01m45s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[26d22h01m45s][HUGECTR][INFO]: gpu1 init embedding done\n",
      "[26d22h01m45s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[26d22h01m45s][HUGECTR][INFO]: warmup_steps is not specified using default: 1\n",
      "[26d22h01m45s][HUGECTR][INFO]: decay_start is not specified using default: 0\n",
      "[26d22h01m45s][HUGECTR][INFO]: decay_steps is not specified using default: 1\n",
      "[26d22h01m45s][HUGECTR][INFO]: decay_power is not specified using default: 2.000000\n",
      "[26d22h01m45s][HUGECTR][INFO]: end_lr is not specified using default: 0.000000\n",
      "[5220.55, init_end, ]\n",
      "[5220.56, run_start, ]\n",
      "[26d22h01m45s][HUGECTR][INFO]: HugeCTR training start:\n",
      "[5220.57, train_epoch_start, 0, ]\n",
      "[26d22h01m45s][HUGECTR][INFO]: Iter: 200 Time(200 iters): 0.206302s Loss: 0.576882 lr:0.001000\n",
      "[26d22h01m45s][HUGECTR][INFO]: Iter: 400 Time(200 iters): 0.203501s Loss: 0.572607 lr:0.001000\n",
      "[26d22h01m45s][HUGECTR][INFO]: Iter: 600 Time(200 iters): 0.204609s Loss: 0.563783 lr:0.001000\n",
      "[26d22h01m45s][HUGECTR][INFO]: Iter: 800 Time(200 iters): 0.204776s Loss: 0.550207 lr:0.001000\n",
      "[26d22h01m46s][HUGECTR][INFO]: Iter: 1000 Time(200 iters): 0.219418s Loss: 0.527647 lr:0.001000\n",
      "[6260.69, eval_start, 0.0980392, ]\n",
      "[26d22h01m46s][HUGECTR][INFO]: Evaluation, AUC: 0.771987\n",
      "[6358.04, eval_accuracy, 0.771987, 0.0980392, 1000, ]\n",
      "[26d22h01m46s][HUGECTR][INFO]: Eval Time for 300 iters: 0.097339s\n",
      "[6358.06, eval_stop, 0.0980392, ]\n",
      "[26d22h01m46s][HUGECTR][INFO]: Iter: 1200 Time(200 iters): 0.303098s Loss: 0.542161 lr:0.001000\n",
      "[26d22h01m46s][HUGECTR][INFO]: Iter: 1400 Time(200 iters): 0.204807s Loss: 0.526745 lr:0.001000\n",
      "[26d22h01m46s][HUGECTR][INFO]: Iter: 1600 Time(200 iters): 0.204562s Loss: 0.537047 lr:0.001000\n",
      "[26d22h01m47s][HUGECTR][INFO]: Iter: 1800 Time(200 iters): 0.203729s Loss: 0.521392 lr:0.001000\n",
      "[26d22h01m47s][HUGECTR][INFO]: Iter: 2000 Time(200 iters): 0.218057s Loss: 0.541166 lr:0.001000\n",
      "[7395.14, eval_start, 0.196078, ]\n",
      "[26d22h01m47s][HUGECTR][INFO]: Evaluation, AUC: 0.786099\n",
      "[7473.16, eval_accuracy, 0.786099, 0.196078, 2000, ]\n",
      "[26d22h01m47s][HUGECTR][INFO]: Eval Time for 300 iters: 0.078014s\n",
      "[7473.18, eval_stop, 0.196078, ]\n",
      "[26d22h01m47s][HUGECTR][INFO]: Iter: 2200 Time(200 iters): 0.282912s Loss: 0.521731 lr:0.001000\n",
      "[26d22h01m47s][HUGECTR][INFO]: Iter: 2400 Time(200 iters): 0.203591s Loss: 0.521812 lr:0.001000\n",
      "[26d22h01m47s][HUGECTR][INFO]: Iter: 2600 Time(200 iters): 0.203338s Loss: 0.519827 lr:0.001000\n",
      "[26d22h01m48s][HUGECTR][INFO]: Iter: 2800 Time(200 iters): 0.203928s Loss: 0.531390 lr:0.001000\n",
      "[26d22h01m48s][HUGECTR][INFO]: Iter: 3000 Time(200 iters): 0.217994s Loss: 0.531942 lr:0.001000\n",
      "[8507.31, eval_start, 0.294118, ]\n",
      "[26d22h01m48s][HUGECTR][INFO]: Evaluation, AUC: 0.795449\n",
      "[8583.77, eval_accuracy, 0.795449, 0.294118, 3000, ]\n",
      "[26d22h01m48s][HUGECTR][INFO]: Eval Time for 300 iters: 0.076454s\n",
      "[8583.8, eval_stop, 0.294118, ]\n",
      "[26d22h01m48s][HUGECTR][INFO]: Iter: 3200 Time(200 iters): 0.280813s Loss: 0.525190 lr:0.001000\n",
      "[26d22h01m48s][HUGECTR][INFO]: Iter: 3400 Time(200 iters): 0.203590s Loss: 0.529874 lr:0.001000\n",
      "[26d22h01m49s][HUGECTR][INFO]: Iter: 3600 Time(200 iters): 0.203769s Loss: 0.515904 lr:0.001000\n",
      "[26d22h01m49s][HUGECTR][INFO]: Iter: 3800 Time(200 iters): 0.204028s Loss: 0.501873 lr:0.001000\n",
      "[26d22h01m49s][HUGECTR][INFO]: Iter: 4000 Time(200 iters): 0.217705s Loss: 0.520061 lr:0.001000\n",
      "[9617.6, eval_start, 0.392157, ]\n",
      "[26d22h01m49s][HUGECTR][INFO]: Evaluation, AUC: 0.799154\n",
      "[9702.18, eval_accuracy, 0.799154, 0.392157, 4000, ]\n",
      "[26d22h01m49s][HUGECTR][INFO]: Eval Time for 300 iters: 0.084572s\n",
      "[9702.2, eval_stop, 0.392157, ]\n",
      "[26d22h01m49s][HUGECTR][INFO]: Iter: 4200 Time(200 iters): 0.288623s Loss: 0.501097 lr:0.001000\n",
      "[26d22h01m49s][HUGECTR][INFO]: Iter: 4400 Time(200 iters): 0.203220s Loss: 0.512142 lr:0.001000\n",
      "[26d22h01m50s][HUGECTR][INFO]: Iter: 4600 Time(200 iters): 0.203890s Loss: 0.529760 lr:0.001000\n",
      "[26d22h01m50s][HUGECTR][INFO]: Iter: 4800 Time(200 iters): 0.203920s Loss: 0.501861 lr:0.001000\n",
      "[26d22h01m50s][HUGECTR][INFO]: Iter: 5000 Time(200 iters): 0.217876s Loss: 0.524264 lr:0.001000\n",
      "[10735.9, eval_start, 0.490196, ]\n",
      "[26d22h01m50s][HUGECTR][INFO]: Evaluation, AUC: 0.805117\n",
      "[10806.3, eval_accuracy, 0.805117, 0.490196, 5000, ]\n",
      "[26d22h01m50s][HUGECTR][INFO]: Eval Time for 300 iters: 0.070470s\n",
      "[10806.4, eval_stop, 0.490196, ]\n",
      "[26d22h01m50s][HUGECTR][INFO]: Iter: 5200 Time(200 iters): 0.274545s Loss: 0.508696 lr:0.001000\n",
      "[26d22h01m51s][HUGECTR][INFO]: Iter: 5400 Time(200 iters): 0.204293s Loss: 0.508217 lr:0.001000\n",
      "[26d22h01m51s][HUGECTR][INFO]: Iter: 5600 Time(200 iters): 0.203618s Loss: 0.538368 lr:0.001000\n",
      "[26d22h01m51s][HUGECTR][INFO]: Iter: 5800 Time(200 iters): 0.203632s Loss: 0.499672 lr:0.001000\n",
      "[26d22h01m51s][HUGECTR][INFO]: Iter: 6000 Time(200 iters): 0.217138s Loss: 0.509102 lr:0.001000\n",
      "[11839.5, eval_start, 0.588235, ]\n",
      "[26d22h01m51s][HUGECTR][INFO]: Evaluation, AUC: 0.806811\n",
      "[11910.2, eval_accuracy, 0.806811, 0.588235, 6000, ]\n",
      "[26d22h01m51s][HUGECTR][INFO]: Eval Time for 300 iters: 0.070651s\n",
      "[11910.2, eval_stop, 0.588235, ]\n",
      "[26d22h01m51s][HUGECTR][INFO]: Iter: 6200 Time(200 iters): 0.274602s Loss: 0.501924 lr:0.001000\n",
      "[26d22h01m52s][HUGECTR][INFO]: Iter: 6400 Time(200 iters): 0.203869s Loss: 0.519305 lr:0.001000\n",
      "[26d22h01m52s][HUGECTR][INFO]: Iter: 6600 Time(200 iters): 0.203588s Loss: 0.496442 lr:0.001000\n",
      "[26d22h01m52s][HUGECTR][INFO]: Iter: 6800 Time(200 iters): 0.203590s Loss: 0.503692 lr:0.001000\n",
      "[26d22h01m52s][HUGECTR][INFO]: Iter: 7000 Time(200 iters): 0.218011s Loss: 0.492777 lr:0.001000\n",
      "[12943.6, eval_start, 0.686275, ]\n",
      "[26d22h01m52s][HUGECTR][INFO]: Evaluation, AUC: 0.810722\n",
      "[13025.9, eval_accuracy, 0.810722, 0.686275, 7000, ]\n",
      "[26d22h01m52s][HUGECTR][INFO]: Eval Time for 300 iters: 0.082273s\n",
      "[13025.9, eval_stop, 0.686275, ]\n",
      "[26d22h01m53s][HUGECTR][INFO]: Iter: 7200 Time(200 iters): 0.286658s Loss: 0.499391 lr:0.001000\n",
      "[26d22h01m53s][HUGECTR][INFO]: Iter: 7400 Time(200 iters): 0.203891s Loss: 0.491908 lr:0.001000\n",
      "[26d22h01m53s][HUGECTR][INFO]: Iter: 7600 Time(200 iters): 0.203992s Loss: 0.527083 lr:0.001000\n",
      "[26d22h01m53s][HUGECTR][INFO]: Iter: 7800 Time(200 iters): 0.204297s Loss: 0.493934 lr:0.001000\n",
      "[26d22h01m53s][HUGECTR][INFO]: Iter: 8000 Time(200 iters): 0.218216s Loss: 0.491232 lr:0.001000\n",
      "[14061, eval_start, 0.784314, ]\n",
      "[26d22h01m54s][HUGECTR][INFO]: Evaluation, AUC: 0.812954\n",
      "[14134, eval_accuracy, 0.812954, 0.784314, 8000, ]\n",
      "[26d22h01m54s][HUGECTR][INFO]: Eval Time for 300 iters: 0.073007s\n",
      "[14134, eval_stop, 0.784314, ]\n",
      "[26d22h01m54s][HUGECTR][INFO]: Iter: 8200 Time(200 iters): 0.279301s Loss: 0.492354 lr:0.001000\n",
      "[26d22h01m54s][HUGECTR][INFO]: Iter: 8400 Time(200 iters): 0.205024s Loss: 0.502615 lr:0.001000\n",
      "[26d22h01m54s][HUGECTR][INFO]: Iter: 8600 Time(200 iters): 0.204074s Loss: 0.531410 lr:0.001000\n",
      "[26d22h01m54s][HUGECTR][INFO]: Iter: 8800 Time(200 iters): 0.218030s Loss: 0.505093 lr:0.001000\n",
      "[26d22h01m55s][HUGECTR][INFO]: Iter: 9000 Time(200 iters): 0.203541s Loss: 0.525441 lr:0.001000\n",
      "[15171.6, eval_start, 0.882353, ]\n",
      "[26d22h01m55s][HUGECTR][INFO]: Evaluation, AUC: 0.812555\n",
      "[15239, eval_accuracy, 0.812555, 0.882353, 9000, ]\n",
      "[26d22h01m55s][HUGECTR][INFO]: Eval Time for 300 iters: 0.067393s\n",
      "[15239, eval_stop, 0.882353, ]\n",
      "[26d22h01m55s][HUGECTR][INFO]: Iter: 9200 Time(200 iters): 0.271559s Loss: 0.508481 lr:0.001000\n",
      "[26d22h01m55s][HUGECTR][INFO]: Iter: 9400 Time(200 iters): 0.203384s Loss: 0.484132 lr:0.001000\n",
      "[26d22h01m55s][HUGECTR][INFO]: Iter: 9600 Time(200 iters): 0.203574s Loss: 0.509076 lr:0.001000\n",
      "[26d22h01m55s][HUGECTR][INFO]: Iter: 9800 Time(200 iters): 0.217636s Loss: 0.511766 lr:0.001000\n",
      "[26d22h01m56s][HUGECTR][INFO]: Iter: 10000 Time(200 iters): 0.205228s Loss: 0.483611 lr:0.001000\n",
      "[26d22h01m56s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[26d22h01m56s][HUGECTR][INFO]: Rank0: Dump hash table from GPU1\n",
      "[26d22h01m56s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[26d22h01m56s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[26d22h01m56s][HUGECTR][INFO]: Done\n",
      "[26d22h01m56s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[26d22h01m56s][HUGECTR][INFO]: Done\n",
      "[26d22h01m56s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[26d22h01m56s][HUGECTR][INFO]: Done\n",
      "[16776, eval_start, 0.980392, ]\n",
      "[26d22h01m56s][HUGECTR][INFO]: Evaluation, AUC: 0.815686\n",
      "[16859.1, eval_accuracy, 0.815686, 0.980392, 10000, ]\n",
      "[26d22h01m56s][HUGECTR][INFO]: Eval Time for 300 iters: 0.083046s\n",
      "[16859.1, eval_stop, 0.980392, ]\n",
      "[17061.8, train_epoch_end, 1, ]\n",
      "[17061.8, run_stop, ]\n"
     ]
    }
   ],
   "source": [
    "!huge_ctr --train ./movielens_config.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
