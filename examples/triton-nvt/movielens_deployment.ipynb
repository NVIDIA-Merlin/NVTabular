{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment with Merlin Inference API\n",
    "\n",
    "## Overview\n",
    "\n",
    "In the previous notebook we explained and showed how we can preprocess data with NVTAbular, and train an TF MLP model using NVTabular TF dataloader. We learned how to save a workflow, and a trained TF model. In this notebook, we will show example request scripts sent to triton inference server \n",
    "- to transform new/streaming data with NVTabular library\n",
    "- to generate prediction results for new data from trained model \n",
    "- to deploy the end-to-end pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import os\n",
    "\n",
    "from tritonclient.utils import *\n",
    "import tritonclient.http as httpclient\n",
    "import nvtabular\n",
    "import cudf\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/working_dir/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our base directory, containing the raw and processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml-25m\ttrain  train.parquet  valid  valid.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls $BASE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Triton Is Running Correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Triton’s ready endpoint to verify that the server and the models are ready for inference. From the host system use curl to access the HTTP endpoint that indicates server status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "\u001b[1mContent-Length\u001b[0m: 0\n",
      "\u001b[1mContent-Type\u001b[0m: text/plain\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!curl -i 10.110.20.127:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTTP request returns status 200 if Triton is ready and non-200 if it is not ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step we are going to generate config.pbtxt and we will save our workflow as a .pkl file to be able to load again to do transformation for the test (new coming) datasets at inference stage. This step actually does the serialization of the workflow that we created above using training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvtabular.inference.triton import generate_triton_model\n",
    "generate_triton_model(workflow, \"movielens_nvt\", \"/working_dir/models/models/movielens_nvt/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's send a request to the running triton inference server using our raw validation set in parquet format. This request is going to load the saved NVTabular workflow and then transform the new dataset samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nvtabular/nvtabular/workflow.py:236: UserWarning: Loading workflow generated with cudf version 0.16.0 - but we are running cudf 0+untagged.1.gbd321d1. This might cause issues\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          userId  movieId  rating\n",
      "15347762   99476   104374     3.5\n",
      "16647840  107979     2634     4.0\n",
      "   userId  movieId  rating\n",
      "0   99476    19997       1\n",
      "1  107979     2543       1\n"
     ]
    }
   ],
   "source": [
    "# read in the workflow (to get input/output schema to call triton with)\n",
    "\n",
    "workflow = nvtabular.Workflow.load(\"/working_dir/models/movielens_nvt/1/workflow\")\n",
    "\n",
    "# read in a batch of data to get transforms for\n",
    "batch = cudf.read_parquet(\"/working_dir/data/valid.parquet\", num_rows=2)[workflow.column_group.input_column_names]\n",
    "\n",
    "print(batch)\n",
    "\n",
    "# convert the batch to a triton inputs\n",
    "columns = [(col, batch[col][0:2]) for col in workflow.column_group.input_column_names]\n",
    "inputs = []\n",
    "\n",
    "col_dtypes = [np.int32, np.int32, np.float32]\n",
    "\n",
    "for i, (name, col) in enumerate(columns):\n",
    "    d = col.values_host.astype(col_dtypes[i])\n",
    "    d = d.reshape(len(d),1)\n",
    "    inputs.append(httpclient.InferInput(name, d.shape, np_to_triton_dtype(col_dtypes[i])))\n",
    "    inputs[i].set_data_from_numpy(d)\n",
    "\n",
    "# placeholder variables for the output\n",
    "outputs = [httpclient.InferRequestedOutput(name) for name in workflow.column_group.columns]\n",
    "\n",
    "# make the request\n",
    "with httpclient.InferenceServerClient(\"<ip-host>:8000\") as client:\n",
    "    response = client.infer(\"movielens_nvt\", inputs, request_id=\"1\",outputs=outputs)\n",
    "    \n",
    "# convert output from triton back to a nvt dataframe  \n",
    "output = cudf.DataFrame({col: response.as_numpy(col).T[0] for col in workflow.column_group.columns})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the MovieLens rating classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [movilens_TF]() notebook we saved our TF model with the following script:\n",
    "\n",
    "```\n",
    "model.save('/working_dir/models/movielens_tf/1/model.savedmodel')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A minimal model repository for a TensorFlow SavedModel model is:\n",
    "```\n",
    "  <model-repository-path>/\n",
    "    <model-name>/\n",
    "      config.pbtxt\n",
    "      1/\n",
    "        model.savedmodel/\n",
    "           <saved-model files>\n",
    "```\n",
    "Let's check out our model repository layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get install tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/working_dir/models/movielens_tf\u001b[00m\n",
      "├── \u001b[01;34m1\u001b[00m\n",
      "│   └── \u001b[01;34mmodel.savedmodel\u001b[00m\n",
      "│       ├── \u001b[01;34massets\u001b[00m\n",
      "│       ├── saved_model.pb\n",
      "│       └── \u001b[01;34mvariables\u001b[00m\n",
      "│           ├── variables.data-00000-of-00001\n",
      "│           └── variables.index\n",
      "└── config.pbtxt\n",
      "\n",
      "4 directories, 4 files\n"
     ]
    }
   ],
   "source": [
    "!tree /working_dir/models/movielens_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we have a config.pbtxt file. Each model in a model repository must include a model configuration that provides required and optional information about the model. Typically, this configuration is provided in a `config.pbtxt` file specified as [ModelConfig protobuf](https://github.com/triton-inference-server/server/blob/r20.12/src/core/model_config.proto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6248126]\n",
      " [0.6249962]]\n"
     ]
    }
   ],
   "source": [
    "from tritonclient.utils import *\n",
    "import tritonclient.http as httpclient\n",
    "import nvtabular\n",
    "import cudf\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "# read in a batch of data to get transforms for\n",
    "batch = cudf.read_parquet(\"/working_dir/data/valid/*.parquet\", num_rows=2)\n",
    "\n",
    "batch = batch[batch.columns][0:2]\n",
    "batch = batch.drop(columns=[\"rating\"])\n",
    "\n",
    "inputs = [] \n",
    "\n",
    "for i, col in enumerate(batch.columns):\n",
    "    d = batch[col].values_host.astype(np.int32)\n",
    "    d = d.reshape(len(d),1)\n",
    "    inputs.append(httpclient.InferInput(col, d.shape, np_to_triton_dtype(np.int32)))\n",
    "    inputs[i].set_data_from_numpy(d)\n",
    "\n",
    "outputs = [httpclient.InferRequestedOutput(\"dense_3\")]\n",
    "\n",
    "with httpclient.InferenceServerClient(\"<ip-host>:8000\") as client:\n",
    "    response = client.infer(\"movielens_tf\", inputs, request_id=\"1\",outputs=outputs)\n",
    "\n",
    "print(response.as_numpy(\"dense_3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END-2-END INFERENCE PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this request example below, we show that we can feed raw unprocessed parquet file, and obtain final prediction results coming from the last layer of the TF model that we built in `movilens_TF` notebook. The output we get is a softmax value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          userId  movieId  rating\n",
      "15347762   99476   104374     3.5\n",
      "16647840  107979     2634     4.0 \n",
      "\n",
      "predicted softmax result:\n",
      " [[0.6248126]\n",
      " [0.6249962]]\n"
     ]
    }
   ],
   "source": [
    "from tritonclient.utils import *\n",
    "import tritonclient.http as httpclient\n",
    "import nvtabular\n",
    "import cudf\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "# read in the workflow (to get input/output schema to call triton with)\n",
    "batch = cudf.read_parquet(\"/working_dir/data/valid.parquet\", num_rows=2)\n",
    "batch = batch[batch.columns][0:2]\n",
    "\n",
    "print(batch, \"\\n\")\n",
    "\n",
    "# convert the batch to a triton inputs\n",
    "inputs = []\n",
    "\n",
    "col_names = ['userId_ens', 'movieId_ens', 'rating_ens'] \n",
    "col_dtypes = [np.int32, np.int32, np.float32]\n",
    "\n",
    "for i, col in enumerate(batch.columns):\n",
    "    d = batch[col].values_host.astype(col_dtypes[i])\n",
    "    d = d.reshape(len(d),1)\n",
    "    inputs.append(httpclient.InferInput(col_names[i], d.shape, np_to_triton_dtype(col_dtypes[i])))\n",
    "    inputs[i].set_data_from_numpy(d)\n",
    "\n",
    "# placeholder variables for the output\n",
    "outputs = [httpclient.InferRequestedOutput(\"predicted_rating\")]\n",
    "\n",
    "# make the request\n",
    "with httpclient.InferenceServerClient(\"<ip-host>:8000\") as client:\n",
    "    response = client.infer(\"movielens\", inputs, request_id=\"1\",outputs=outputs)\n",
    "\n",
    "print(\"predicted softmax result:\\n\", response.as_numpy('predicted_rating'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
