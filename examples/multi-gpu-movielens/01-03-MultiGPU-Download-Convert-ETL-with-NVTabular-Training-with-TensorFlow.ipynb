{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be62766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd62b5",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Multi-GPU with MovieLens: ETL and Training \n",
    "\n",
    "## Overview\n",
    "\n",
    "NVIDIA Merlin is a open source framework to accelerate and scale end-to-end recommender system pipelines on GPU. In this notebook, we use NVTabular, Merlin’s ETL component, to scale feature engineering and pre-processing to multiple GPUs and then perform data-parallel distributed training of a neural network on multiple GPUs with TensorFlow, [Horovod](https://horovod.readthedocs.io/en/stable/), and [NCCL](https://developer.nvidia.com/nccl).\n",
    "\n",
    "The pre-requisites for this notebook are to be familiar with NVTabular and its API:\n",
    "- You can read more about NVTabular, its API and specialized dataloaders in [Getting Started with Movielens notebooks](../getting-started-movielens).\n",
    "- You can read more about scaling NVTabular ETL in [Scaling Criteo notebooks](../scaling-criteo).\n",
    "\n",
    "**In this notebook, we will focus only on the new information related to multi-GPU training, so please check out the other notebooks first (if you haven’t already.)**\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this notebook, we learn how to scale ETL and deep learning taining to multiple GPUs\n",
    "- Learn to use larger than GPU/host memory datasets for ETL and training\n",
    "- Use multi-GPU or multi node for ETL with NVTabular\n",
    "- Use NVTabular dataloader to accelerate TensorFlow pipelines\n",
    "- Scale TensorFlow training with Horovod\n",
    "\n",
    "### Dataset\n",
    "\n",
    "In this notebook, we use the [MovieLens25M](https://grouplens.org/datasets/movielens/25m/) dataset. It is popular for recommender systems and is used in academic publications. The dataset contains 25M movie ratings for 62,000 movies given by 162,000 users. Many projects use only the user/item/rating information of MovieLens, but the original dataset provides metadata for the movies, as well.\n",
    "\n",
    "Note: We are using the MovieLens 25M dataset in this example for simplicity, although the dataset is not large enough to require multi-GPU training. However, the functionality demonstrated in this notebook can be easily extended to scale recommender pipelines for larger datasets in the same way.\n",
    "\n",
    "### Tools\n",
    "\n",
    "- [Horovod](https://horovod.readthedocs.io/en/stable/) is a distributed deep learning framework that provides tools for multi-GPU optimization.\n",
    "- The [NVIDIA Collective Communication Library (NCCL)](https://developer.nvidia.com/nccl) provides the underlying GPU-based implementations of the [allgather](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html#allgather) and [allreduce](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html#allreduce) cross-GPU communication operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7332a3be",
   "metadata": {},
   "source": [
    "## Download and Convert\n",
    "\n",
    "First, we will download and convert the dataset to Parquet. This section is based on [01-Download-Convert.ipynb](../getting-started-movielens/01-Download-Convert.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7abbc39",
   "metadata": {},
   "source": [
    "#### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54d7869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unzipping files: 100%|█████████████████████████| 8/8 [00:09<00:00,  1.17s/files]\n"
     ]
    }
   ],
   "source": [
    "# External dependencies\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import cudf  # cuDF is an implementation of Pandas-like Dataframe on GPU\n",
    "\n",
    "from nvtabular.utils import download_file\n",
    "\n",
    "INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", \"~/nvt-examples/multigpu-movielens/data/\")\n",
    "BASE_DIR = pathlib.Path(INPUT_DATA_DIR).expanduser()\n",
    "zip_path = pathlib.Path(BASE_DIR, \"ml-25m.zip\")\n",
    "download_file(\n",
    "    \"http://files.grouplens.org/datasets/movielens/ml-25m.zip\", zip_path, redownload=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb82e7",
   "metadata": {},
   "source": [
    "#### Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6a7ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = cudf.read_csv(pathlib.Path(BASE_DIR, \"ml-25m\", \"movies.csv\"))\n",
    "movies[\"genres\"] = movies[\"genres\"].str.split(\"|\")\n",
    "movies = movies.drop(\"title\", axis=1)\n",
    "movies.to_parquet(pathlib.Path(BASE_DIR, \"ml-25m\", \"movies_converted.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8da86e",
   "metadata": {},
   "source": [
    "#### Split into train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9a686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = cudf.read_csv(pathlib.Path(BASE_DIR, \"ml-25m\", \"ratings.csv\"))\n",
    "ratings = ratings.drop(\"timestamp\", axis=1)\n",
    "\n",
    "# shuffle the dataset\n",
    "ratings = ratings.sample(len(ratings), replace=False)\n",
    "# split the train_df as training and validation data sets.\n",
    "num_valid = int(len(ratings) * 0.2)\n",
    "train = ratings[:-num_valid]\n",
    "valid = ratings[-num_valid:]\n",
    "\n",
    "train.to_parquet(pathlib.Path(BASE_DIR, \"train.parquet\"))\n",
    "valid.to_parquet(pathlib.Path(BASE_DIR, \"valid.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e24ff4b",
   "metadata": {},
   "source": [
    "## ETL with NVTabular\n",
    "\n",
    "We finished downloading and converting the dataset. We will preprocess and engineer features with NVTabular on multiple GPUs. You can read more\n",
    "- about NVTabular's features and API in [getting-started-movielens/02-ETL-with-NVTabular.ipynb](../getting-started-movielens/02-ETL-with-NVTabular.ipynb).\n",
    "- scaling NVTabular ETL to multiple GPUs [scaling-criteo/02-ETL-with-NVTabular.ipynb](../scaling-criteo/02-ETL-with-NVTabular.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7308a8",
   "metadata": {},
   "source": [
    "#### Deploy a Distributed-Dask Cluster\n",
    "\n",
    "This section is based on [scaling-criteo/02-ETL-with-NVTabular.ipynb](../scaling-criteo/02-ETL-with-NVTabular.ipynb) and [multi-gpu-toy-example/multi-gpu_dask.ipynb](../multi-gpu-toy-example/multi-gpu_dask.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c962f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import shutil\n",
    "\n",
    "# External Dependencies\n",
    "import cupy as cp\n",
    "import cudf\n",
    "import dask_cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "from dask.utils import parse_bytes\n",
    "from dask.delayed import delayed\n",
    "import rmm\n",
    "\n",
    "# NVTabular\n",
    "import nvtabular as nvt\n",
    "import nvtabular.ops as ops\n",
    "from nvtabular.io import Shuffle\n",
    "from nvtabular.utils import device_mem_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5332e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some information about where to get our data\n",
    "input_path = pathlib.Path(BASE_DIR, \"converted\", \"movielens\")\n",
    "dask_workdir = pathlib.Path(BASE_DIR, \"test_dask\", \"workdir\")\n",
    "output_path = pathlib.Path(BASE_DIR, \"test_dask\", \"output\")\n",
    "stats_path = pathlib.Path(BASE_DIR, \"test_dask\", \"stats\")\n",
    "\n",
    "# Make sure we have a clean worker space for Dask\n",
    "if pathlib.Path.is_dir(dask_workdir):\n",
    "    shutil.rmtree(dask_workdir)\n",
    "dask_workdir.mkdir(parents=True)\n",
    "\n",
    "# Make sure we have a clean stats space for Dask\n",
    "if pathlib.Path.is_dir(stats_path):\n",
    "    shutil.rmtree(stats_path)\n",
    "stats_path.mkdir(parents=True)\n",
    "\n",
    "# Make sure we have a clean output path\n",
    "if pathlib.Path.is_dir(output_path):\n",
    "    shutil.rmtree(output_path)\n",
    "output_path.mkdir(parents=True)\n",
    "\n",
    "# Get device memory capacity\n",
    "capacity = device_mem_size(kind=\"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eca2e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.8/site-packages/distributed-2021.7.1-py3.8.egg/distributed/node.py:160: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 40549 instead\n",
      "  warnings.warn(\n",
      "distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div style=\"\n",
       "                    width: 24px;\n",
       "                    height: 24px;\n",
       "                    background-color: #e1e1e1;\n",
       "                    border: 3px solid #9D9D9D;\n",
       "                    border-radius: 5px;\n",
       "                    position: absolute;\"> </div>\n",
       "                <div style=\"margin-left: 48px;\">\n",
       "                    <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "                    <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-5dda590c-5ccc-11ec-b73d-54ab3a8c9e18</p>\n",
       "                    <table style=\"width: 100%; text-align: left;\">\n",
       "                    \n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "                    <td style=\"text-align: left;\"><strong>Cluster type:</strong> LocalCUDACluster</td>\n",
       "                </tr>\n",
       "                \n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard: </strong>\n",
       "                        <a href=\"http://127.0.0.1:40549/status\">http://127.0.0.1:40549/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\"></td>\n",
       "                </tr>\n",
       "                \n",
       "                    </table>\n",
       "                    \n",
       "                <details>\n",
       "                <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "                \n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "                <div style=\"\n",
       "                    width: 24px;\n",
       "                    height: 24px;\n",
       "                    background-color: #e1e1e1;\n",
       "                    border: 3px solid #9D9D9D;\n",
       "                    border-radius: 5px;\n",
       "                    position: absolute;\"> </div>\n",
       "                <div style=\"margin-left: 48px;\">\n",
       "                    <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">LocalCUDACluster</h3>\n",
       "                    <p style=\"color: #9D9D9D; margin-bottom: 0px;\">752a7aa2</p>\n",
       "                    <table style=\"width: 100%; text-align: left;\">\n",
       "                    \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\"><strong>Status:</strong> running</td>\n",
       "                <td style=\"text-align: left;\"><strong>Using processes:</strong> True</td>\n",
       "            </tr>\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:40549/status\">http://127.0.0.1:40549/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"><strong>Workers:</strong> 2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong>\n",
       "                    2\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong>\n",
       "                    0.98 TiB\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "                    </table>\n",
       "                    <details>\n",
       "                    <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Scheduler Info</h3></summary>\n",
       "                    \n",
       "        <div style=\"\">\n",
       "            \n",
       "            <div>\n",
       "                <div style=\"\n",
       "                    width: 24px;\n",
       "                    height: 24px;\n",
       "                    background-color: #FFF7E5;\n",
       "                    border: 3px solid #FF6132;\n",
       "                    border-radius: 5px;\n",
       "                    position: absolute;\"> </div>\n",
       "                <div style=\"margin-left: 48px;\">\n",
       "                    <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "                    <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-43a87f8b-d2f7-4193-9f9b-5922bef381e3</p>\n",
       "                    <table style=\"width: 100%; text-align: left;\">\n",
       "                        <tr>\n",
       "                            <td style=\"text-align: left;\"><strong>Comm:</strong> tcp://127.0.0.1:35775</td>\n",
       "                            <td style=\"text-align: left;\"><strong>Workers:</strong> 2</td>\n",
       "                        </tr>\n",
       "                        <tr>\n",
       "                            <td style=\"text-align: left;\">\n",
       "                                <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:40549/status\">http://127.0.0.1:40549/status</a>\n",
       "                            </td>\n",
       "                            <td style=\"text-align: left;\">\n",
       "                                <strong>Total threads:</strong>\n",
       "                                2\n",
       "                            </td>\n",
       "                        </tr>\n",
       "                        <tr>\n",
       "                            <td style=\"text-align: left;\">\n",
       "                                <strong>Started:</strong>\n",
       "                                Just now\n",
       "                            </td>\n",
       "                            <td style=\"text-align: left;\">\n",
       "                                <strong>Total memory:</strong>\n",
       "                                0.98 TiB\n",
       "                            </td>\n",
       "                        </tr>\n",
       "                    </table>\n",
       "                </div>\n",
       "            </div>\n",
       "        \n",
       "            <details style=\"margin-left: 48px;\">\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Workers</h3></summary>\n",
       "            \n",
       "            <div style=\"margin-bottom: 20px;\">\n",
       "                <div style=\"width: 24px;\n",
       "                            height: 24px;\n",
       "                            background-color: #DBF5FF;\n",
       "                            border: 3px solid #4CC9FF;\n",
       "                            border-radius: 5px;\n",
       "                            position: absolute;\"> </div>\n",
       "                <div style=\"margin-left: 48px;\">\n",
       "                <details>\n",
       "                    <summary>\n",
       "                        <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 0</h4>\n",
       "                    </summary>\n",
       "                    <table style=\"width: 100%; text-align: left;\">\n",
       "                        <tr>\n",
       "                            <td style=\"text-align: left;\"><strong>Comm: </strong> tcp://127.0.0.1:39427</td>\n",
       "                            <td style=\"text-align: left;\"><strong>Total threads: </strong> 1</td>\n",
       "                        </tr>\n",
       "                        <tr>\n",
       "                            <td style=\"text-align: left;\">\n",
       "                                <strong>Dashboard: </strong>\n",
       "                                <a href=\"http://127.0.0.1:42443/status\">http://127.0.0.1:42443/status</a>\n",
       "                            </td>\n",
       "                            <td style=\"text-align: left;\">\n",
       "                                <strong>Memory: </strong>\n",
       "                                503.90 GiB\n",
       "                            </td>\n",
       "                        </tr>\n",
       "                        <tr>\n",
       "                            <td style=\"text-align: left;\"><strong>Nanny: </strong> tcp://127.0.0.1:43775</td>\n",
       "                            <td style=\"text-align: left;\"></td>\n",
       "                        </tr>\n",
       "                        <tr>\n",
       "                            <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                                <strong>Local directory: </strong>\n",
       "                                /root/nvt-examples/multigpu-movielens/data/test_dask/workdir/dask-worker-space/worker-dov8uz9s\n",
       "                            </td>\n",
       "                        </tr>\n",
       "                        \n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>GPU: </strong>Tesla V100-SXM2-32GB\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>GPU memory: </strong>\n",
       "                        31.75 GiB\n",
       "                    </td>\n",
       "                </tr>\n",
       "                \n",
       "                        \n",
       "                    </table>\n",
       "                </details>\n",
       "                </div>\n",
       "            </div>\n",
       "            \n",
       "            <div style=\"margin-bottom: 20px;\">\n",
       "                <div style=\"width: 24px;\n",
       "                            height: 24px;\n",
       "                            background-color: #DBF5FF;\n",
       "                            border: 3px solid #4CC9FF;\n",
       "                            border-radius: 5px;\n",
       "                            position: absolute;\"> </div>\n",
       "                <div style=\"margin-left: 48px;\">\n",
       "                <details>\n",
       "                    <summary>\n",
       "                        <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 1</h4>\n",
       "                    </summary>\n",
       "                    <table style=\"width: 100%; text-align: left;\">\n",
       "                        <tr>\n",
       "                            <td style=\"text-align: left;\"><strong>Comm: </strong> tcp://127.0.0.1:35267</td>\n",
       "                            <td style=\"text-align: left;\"><strong>Total threads: </strong> 1</td>\n",
       "                        </tr>\n",
       "                        <tr>\n",
       "                            <td style=\"text-align: left;\">\n",
       "                                <strong>Dashboard: </strong>\n",
       "                                <a href=\"http://127.0.0.1:41367/status\">http://127.0.0.1:41367/status</a>\n",
       "                            </td>\n",
       "                            <td style=\"text-align: left;\">\n",
       "                                <strong>Memory: </strong>\n",
       "                                503.90 GiB\n",
       "                            </td>\n",
       "                        </tr>\n",
       "                        <tr>\n",
       "                            <td style=\"text-align: left;\"><strong>Nanny: </strong> tcp://127.0.0.1:46283</td>\n",
       "                            <td style=\"text-align: left;\"></td>\n",
       "                        </tr>\n",
       "                        <tr>\n",
       "                            <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                                <strong>Local directory: </strong>\n",
       "                                /root/nvt-examples/multigpu-movielens/data/test_dask/workdir/dask-worker-space/worker-nx_hc7kq\n",
       "                            </td>\n",
       "                        </tr>\n",
       "                        \n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>GPU: </strong>Tesla V100-SXM2-32GB\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>GPU memory: </strong>\n",
       "                        31.75 GiB\n",
       "                    </td>\n",
       "                </tr>\n",
       "                \n",
       "                        \n",
       "                    </table>\n",
       "                </details>\n",
       "                </div>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        </div>\n",
       "        \n",
       "                    </details>\n",
       "                </div>\n",
       "            </div>\n",
       "        \n",
       "                </details>\n",
       "                \n",
       "                </div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:35775' processes=2 threads=2, memory=0.98 TiB>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deploy a Single-Machine Multi-GPU Cluster\n",
    "protocol = \"tcp\"  # \"tcp\" or \"ucx\"\n",
    "visible_devices = \"0,1\"  # Delect devices to place workers\n",
    "device_spill_frac = 0.5  # Spill GPU-Worker memory to host at this limit.\n",
    "# Reduce if spilling fails to prevent\n",
    "# device memory errors.\n",
    "cluster = None  # (Optional) Specify existing scheduler port\n",
    "if cluster is None:\n",
    "    cluster = LocalCUDACluster(\n",
    "        protocol=protocol,\n",
    "        CUDA_VISIBLE_DEVICES=visible_devices,\n",
    "        local_directory=dask_workdir,\n",
    "        device_memory_limit=capacity * device_spill_frac,\n",
    "    )\n",
    "\n",
    "# Create the distributed client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88a35091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:35267': None, 'tcp://127.0.0.1:39427': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize RMM pool on ALL workers\n",
    "def _rmm_pool():\n",
    "    rmm.reinitialize(\n",
    "        pool_allocator=True,\n",
    "        initial_pool_size=None,  # Use default size\n",
    "    )\n",
    "\n",
    "\n",
    "client.run(_rmm_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb097e6",
   "metadata": {},
   "source": [
    "#### Defining our Preprocessing Pipeline\n",
    "\n",
    "This subsection is based on [getting-started-movielens/02-ETL-with-NVTabular.ipynb](../getting-started-movielens/02-ETL-with-NVTabular.ipynb). The only difference is that we initialize the NVTabular workflow using the LocalCUDACluster client with `nvt.Workflow(output, client=client)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33f1b593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "asyncio.exceptions.CancelledError\n"
     ]
    }
   ],
   "source": [
    "movies = cudf.read_parquet(pathlib.Path(BASE_DIR, \"ml-25m\", \"movies_converted.parquet\"))\n",
    "joined = [\"userId\", \"movieId\"] >> nvt.ops.JoinExternal(movies, on=[\"movieId\"])\n",
    "cat_features = joined >> nvt.ops.Categorify()\n",
    "ratings = nvt.ColumnSelector([\"rating\"]) >> nvt.ops.LambdaOp(lambda col: (col > 3).astype(\"int8\"))\n",
    "output = cat_features + ratings\n",
    "# USE client in NVTabular workflow\n",
    "workflow = nvt.Workflow(output, client=client)\n",
    "!rm -rf $BASE_DIR/train\n",
    "!rm -rf $BASE_DIR/valid\n",
    "train_iter = nvt.Dataset([str(pathlib.Path(BASE_DIR, \"train.parquet\"))], part_size=\"100MB\")\n",
    "valid_iter = nvt.Dataset([str(pathlib.Path(BASE_DIR, \"valid.parquet\"))], part_size=\"100MB\")\n",
    "workflow.fit(train_iter)\n",
    "workflow.save(str(pathlib.Path(BASE_DIR, \"workflow\")))\n",
    "shuffle = Shuffle.PER_WORKER  # Shuffle algorithm\n",
    "out_files_per_proc = 4  # Number of output files per worker\n",
    "workflow.transform(train_iter).to_parquet(\n",
    "    output_path=pathlib.Path(BASE_DIR, \"train\"),\n",
    "    shuffle=shuffle,\n",
    "    out_files_per_proc=out_files_per_proc,\n",
    ")\n",
    "workflow.transform(valid_iter).to_parquet(\n",
    "    output_path=pathlib.Path(BASE_DIR, \"valid\"),\n",
    "    shuffle=shuffle,\n",
    "    out_files_per_proc=out_files_per_proc,\n",
    ")\n",
    "\n",
    "client.shutdown()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e220013",
   "metadata": {},
   "source": [
    "## Training with TensorFlow on multiGPUs\n",
    "\n",
    "In this section, we will train a TensorFlow model with multi-GPU support. In the NVTabular v0.5 release, we added multi-GPU support for NVTabular dataloaders. We will modify the [getting-started-movielens/03-Training-with-TF.ipynb](../getting-started-movielens/03-Training-with-TF.ipynb) to use multiple GPUs. Please review that notebook, if you have questions about the general functionality of the NVTabular dataloaders or the neural network architecture.\n",
    "\n",
    "#### NVTabular dataloader for TensorFlow\n",
    "\n",
    "We’ve identified that the dataloader is one bottleneck in deep learning recommender systems when training pipelines with TensorFlow. The normal TensorFlow dataloaders cannot prepare the next training batches fast enough and therefore, the GPU is not fully utilized. \n",
    "\n",
    "We developed a highly customized tabular dataloader for accelerating existing pipelines in TensorFlow. In our experiments, we see a speed-up by 9x of the same training workflow with NVTabular dataloader. NVTabular dataloader’s features are:\n",
    "- removing bottleneck of item-by-item dataloading\n",
    "- enabling larger than memory dataset by streaming from disk\n",
    "- reading data directly into GPU memory and remove CPU-GPU communication\n",
    "- preparing batch asynchronously in GPU to avoid CPU-GPU communication\n",
    "- supporting commonly used .parquet format\n",
    "- easy integration into existing TensorFlow pipelines by using similar API - works with tf.keras models\n",
    "- **supporting multi-GPU training with Horovod**\n",
    "\n",
    "You can find more information on the dataloaders in our [blogpost](https://medium.com/nvidia-merlin/training-deep-learning-based-recommender-systems-9x-faster-with-tensorflow-cc5a2572ea49)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad9141",
   "metadata": {},
   "source": [
    "#### Using Horovod with Tensorflow and NVTabular\n",
    "\n",
    "The training script below is based on [getting-started-movielens/03-Training-with-TF.ipynb](../getting-started-movielens/03-Training-with-TF.ipynb), with a few important changes:\n",
    "\n",
    "- We provide several additional parameters to the `KerasSequenceLoader` class, including the total number of workers `hvd.size()`, the current worker's id number `hvd.rank()`, and a function for generating random seeds `seed_fn()`. \n",
    "\n",
    "```python\n",
    "    train_dataset_tf = KerasSequenceLoader(\n",
    "        ...\n",
    "        global_size=hvd.size(),\n",
    "        global_rank=hvd.rank(),\n",
    "        seed_fn=seed_fn,\n",
    "    )\n",
    "\n",
    "```\n",
    "- The seed function uses Horovod to collectively generate a random seed that's shared by all workers so that they can each shuffle the dataset in a consistent way and select partitions to work on without overlap. The seed function is called by the dataloader during the shuffling process at the beginning of each epoch:\n",
    "\n",
    "```python\n",
    "    def seed_fn():\n",
    "        min_int, max_int = tf.int32.limits\n",
    "        max_rand = max_int // hvd.size()\n",
    "\n",
    "        # Generate a seed fragment on each worker\n",
    "        seed_fragment = cupy.random.randint(0, max_rand).get()\n",
    "\n",
    "        # Aggregate seed fragments from all Horovod workers\n",
    "        seed_tensor = tf.constant(seed_fragment)\n",
    "        reduced_seed = hvd.allreduce(seed_tensor, name=\"shuffle_seed\", op=hvd.mpi_ops.Sum) \n",
    "\n",
    "        return reduced_seed % max_rand\n",
    "```\n",
    "\n",
    "- We wrap the TensorFlow optimizer with Horovod's `DistributedOptimizer` class and scale the learning rate by the number of workers:\n",
    "\n",
    "```python\n",
    "    opt = tf.keras.optimizers.SGD(0.01 * hvd.size())\n",
    "    opt = hvd.DistributedOptimizer(opt)\n",
    "```\n",
    "\n",
    "- We add a callback to broadcast initial variable states from rank 0 to all other processes:\n",
    "\n",
    "```python\n",
    "    callbacks = [\n",
    "        hvd.keras.callbacks.BroadcastGlobalVariablesCallback(0)\n",
    "    ]\n",
    "```\n",
    "\n",
    "- We add callbacks to the first process (rank==0) to save checkpoints and create tensorboard logs:\n",
    "\n",
    "```python\n",
    "    if hvd.rank() == 0:\n",
    "        callbacks.append(tf.keras.callbacks.ModelCheckpoint('./checkpoint-{epoch}.h5'))\n",
    "        callbacks.append(tf.keras.callbacks.TensorBoard('./logs'))\n",
    "```\n",
    "\n",
    "- We set verbose to 1 only for the first process (rank==0):\n",
    "\n",
    "```python\n",
    "    verbose = 1 if hvd.rank() == 0 else 0\n",
    "```\n",
    "\n",
    "Note: The NVTabular data loader will iterate over the full dataset. If we train wth multi-GPU, we need to adjust `STEPS_PER_EPOCH = ~ total dataset size // batch_size // number of GPUs`.\n",
    "\n",
    "The rest of the script is the same as the MovieLens example in [getting-started-movielens/03-Training-with-TF.ipynb](../getting-started-movielens/03-Training-with-TF.ipynb). In order to run it with Horovod, we first need to write it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a00b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tf_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './tf_trainer.py'\n",
    "\n",
    "# External dependencies\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import cupy\n",
    "\n",
    "# we can control how much memory to give tensorflow with this environment variable\n",
    "# IMPORTANT: make sure you do this before you initialize TF's runtime, otherwise\n",
    "# TF will have claimed all free GPU memory\n",
    "os.environ[\"TF_MEMORY_ALLOCATION\"] = \"0.3\"  # fraction of free memory\n",
    "\n",
    "import nvtabular as nvt  # noqa: E402 isort:skip\n",
    "from nvtabular.framework_utils.tensorflow import layers  # noqa: E402 isort:skip\n",
    "from nvtabular.loader.tensorflow import KerasSequenceLoader  # noqa: E402 isort:skip\n",
    "\n",
    "import tensorflow as tf  # noqa: E402 isort:skip\n",
    "import horovod.tensorflow as hvd  # noqa: E402 isort:skip\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Process some integers.\")\n",
    "parser.add_argument(\"--dir_in\", default=None, help=\"Input directory\")\n",
    "parser.add_argument(\"--batch_size\", default=None, help=\"batch size\")\n",
    "parser.add_argument(\"--cats\", default=None, help=\"categorical columns\")\n",
    "parser.add_argument(\"--cats_mh\", default=None, help=\"categorical multihot columns\")\n",
    "parser.add_argument(\"--conts\", default=None, help=\"continuous columns\")\n",
    "parser.add_argument(\"--labels\", default=None, help=\"continuous columns\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "BASE_DIR = args.dir_in or \"./data/\"\n",
    "BATCH_SIZE = int(args.batch_size or 16384)  # Batch Size\n",
    "CATEGORICAL_COLUMNS = args.cats or [\"movieId\", \"userId\"]  # Single-hot\n",
    "CATEGORICAL_MH_COLUMNS = args.cats_mh or [\"genres\"]  # Multi-hot\n",
    "NUMERIC_COLUMNS = args.conts or []\n",
    "TRAIN_PATHS = sorted(\n",
    "    glob.glob(os.path.join(BASE_DIR, \"train/*.parquet\"))\n",
    ")  # Output from ETL-with-NVTabular\n",
    "STEPS_PER_EPOCH = 100\n",
    "hvd.init()\n",
    "\n",
    "# Seed with system randomness (or a static seed)\n",
    "cupy.random.seed(None)\n",
    "\n",
    "\n",
    "def seed_fn():\n",
    "    \"\"\"\n",
    "    Generate consistent dataloader shuffle seeds across workers\n",
    "\n",
    "    Reseeds each worker's dataloader each epoch to get fresh a shuffle\n",
    "    that's consistent across workers.\n",
    "    \"\"\"\n",
    "    min_int, max_int = tf.int32.limits\n",
    "    max_rand = max_int // hvd.size()\n",
    "\n",
    "    # Generate a seed fragment on each worker\n",
    "    seed_fragment = cupy.random.randint(0, max_rand).get()\n",
    "\n",
    "    # Aggregate seed fragments from all Horovod workers\n",
    "    seed_tensor = tf.constant(seed_fragment)\n",
    "    reduced_seed = hvd.allreduce(seed_tensor, name=\"shuffle_seed\", op=hvd.mpi_ops.Sum)\n",
    "\n",
    "    return reduced_seed % max_rand\n",
    "\n",
    "\n",
    "proc = nvt.Workflow.load(os.path.join(BASE_DIR, \"workflow/\"))\n",
    "EMBEDDING_TABLE_SHAPES, MH_EMBEDDING_TABLE_SHAPES = nvt.ops.get_embedding_sizes(proc)\n",
    "EMBEDDING_TABLE_SHAPES.update(MH_EMBEDDING_TABLE_SHAPES)\n",
    "\n",
    "train_dataset_tf = KerasSequenceLoader(\n",
    "    TRAIN_PATHS,  # you could also use a glob pattern\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_names=[\"rating\"],\n",
    "    cat_names=CATEGORICAL_COLUMNS + CATEGORICAL_MH_COLUMNS,\n",
    "    cont_names=NUMERIC_COLUMNS,\n",
    "    engine=\"parquet\",\n",
    "    shuffle=True,\n",
    "    buffer_size=0.06,  # how many batches to load at once\n",
    "    parts_per_chunk=1,\n",
    "    global_size=hvd.size(),\n",
    "    global_rank=hvd.rank(),\n",
    "    seed_fn=seed_fn,\n",
    ")\n",
    "inputs = {}  # tf.keras.Input placeholders for each feature to be used\n",
    "emb_layers = []  # output of all embedding layers, which will be concatenated\n",
    "for col in CATEGORICAL_COLUMNS:\n",
    "    inputs[col] = tf.keras.Input(name=col, dtype=tf.int32, shape=(1,))\n",
    "# Note that we need two input tensors for multi-hot categorical features\n",
    "for col in CATEGORICAL_MH_COLUMNS:\n",
    "    inputs[col] = (\n",
    "        tf.keras.Input(name=f\"{col}__values\", dtype=tf.int64, shape=(1,)),\n",
    "        tf.keras.Input(name=f\"{col}__nnzs\", dtype=tf.int64, shape=(1,)),\n",
    "    )\n",
    "for col in CATEGORICAL_COLUMNS + CATEGORICAL_MH_COLUMNS:\n",
    "    emb_layers.append(\n",
    "        tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_identity(\n",
    "                col, EMBEDDING_TABLE_SHAPES[col][0]\n",
    "            ),  # Input dimension (vocab size)\n",
    "            EMBEDDING_TABLE_SHAPES[col][1],  # Embedding output dimension\n",
    "        )\n",
    "    )\n",
    "emb_layer = layers.DenseFeatures(emb_layers)\n",
    "x_emb_output = emb_layer(inputs)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(x_emb_output)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "loss = tf.losses.BinaryCrossentropy()\n",
    "opt = tf.keras.optimizers.SGD(0.01 * hvd.size())\n",
    "opt = hvd.DistributedOptimizer(opt)\n",
    "\n",
    "model.compile(loss=tf.losses.BinaryCrossentropy(), optimizer=opt)\n",
    "\n",
    "callbacks = [hvd.keras.callbacks.BroadcastGlobalVariablesCallback(0)]\n",
    "\n",
    "if hvd.rank() == 0:\n",
    "    callbacks.append(tf.keras.callbacks.ModelCheckpoint(\"./checkpoint-{epoch}.h5\"))\n",
    "    callbacks.append(tf.keras.callbacks.TensorBoard(\"./logs\"))\n",
    "\n",
    "verbose = 1 if hvd.rank() == 0 else 0\n",
    "\n",
    "model.fit(\n",
    "    train_dataset_tf,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    callbacks=callbacks,\n",
    "    epochs=2,\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c998f1e9",
   "metadata": {},
   "source": [
    "We'll also need a small wrapper script to check environment variables set by the Horovod runner to see which rank we'll be assigned, in order to set CUDA_VISIBLE_DEVICES properly for each worker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c905420d",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./hvd_wrapper.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile './hvd_wrapper.sh'\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# Get local process ID from OpenMPI or alternatively from SLURM\n",
    "if [ -z \"${CUDA_VISIBLE_DEVICES:-}\" ]; then\n",
    "    if [ -n \"${OMPI_COMM_WORLD_LOCAL_RANK:-}\" ]; then\n",
    "        LOCAL_RANK=\"${OMPI_COMM_WORLD_LOCAL_RANK}\"\n",
    "    elif [ -n \"${SLURM_LOCALID:-}\" ]; then\n",
    "        LOCAL_RANK=\"${SLURM_LOCALID}\"\n",
    "    fi\n",
    "    export CUDA_VISIBLE_DEVICES=${LOCAL_RANK}\n",
    "fi\n",
    "\n",
    "exec \"$@\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf8300c",
   "metadata": {},
   "source": [
    "OpenMPI and Slurm are tools for running distributed computed jobs. In this example, we’re using OpenMPI, but depending on the environment you run distributed training jobs in, you may need to check slightly different environment variables to find the total number of workers (global size) and each process’s worker number (global rank.)\n",
    "\n",
    "Why do we have to check environment variables instead of using `hvd.rank()` and `hvd.local_rank()`? NVTabular does some GPU configuration when imported and needs to be imported before Horovod to avoid conflicts. We need to set GPU visibility before NVTabular is imported (when Horovod isn’t yet available) so that multiple processes don’t each try to configure all the GPUs, so as a workaround, we “cheat” and peek at environment variables set by horovodrun to decide which GPU each process should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a0b3979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,1]<stderr>:2021-12-14 11:06:01.880128: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "[1,1]<stderr>:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[1,0]<stderr>:2021-12-14 11:06:01.890943: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "[1,0]<stderr>:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[1,1]<stderr>:2021-12-14 11:06:01.902788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9753 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:85:00.0, compute capability: 7.0\n",
      "[1,0]<stderr>:2021-12-14 11:06:01.922702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9753 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:0b:00.0, compute capability: 7.0\n",
      "[1,0]<stderr>:2021-12-14 11:06:02.565520: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "[1,0]<stderr>:2021-12-14 11:06:02.565586: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "[1,0]<stderr>:2021-12-14 11:06:02.565672: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
      "[1,0]<stderr>:2021-12-14 11:06:02.566153: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so.11.2'; dlerror: libcupti.so.11.2: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/hpcx/ompi/lib:/usr/local/cuda/compat/lib.real:/usr/local/hugectr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib\n",
      "[1,0]<stderr>:2021-12-14 11:06:03.258335: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "[1,0]<stderr>:2021-12-14 11:06:03.258707: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
      "[1,1]<stderr>:2021-12-14 11:06:19.845432: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "[1,0]<stderr>:2021-12-14 11:06:20.460602: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "[1,0]<stdout>:Epoch 1/2\n",
      "[1,0]<stdout>:  1/100 [..............................] - ETA: 1:30:45 - loss: 0.6937[1,0]<stderr>:2021-12-14 11:07:38.227212: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "[1,0]<stderr>:2021-12-14 11:07:38.227270: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "[1,0]<stdout>:  2/100 [..............................] - ETA: 43:38 - loss: 0.6935  [1,0]<stderr>:2021-12-14 11:07:42.495197: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "[1,0]<stderr>:2021-12-14 11:07:42.495734: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
      "[1,0]<stderr>:2021-12-14 11:07:42.576685: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 259 callback api events and 254 activity events. \n",
      "[1,0]<stderr>:2021-12-14 11:07:42.586029: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "[1,0]<stderr>:2021-12-14 11:07:42.597816: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/train/plugins/profile/2021_12_14_11_07_42\n",
      "[1,0]<stderr>:\n",
      "[1,0]<stderr>:2021-12-14 11:07:42.605674: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./logs/train/plugins/profile/2021_12_14_11_07_42/dgx06.trace.json.gz\n",
      "[1,0]<stderr>:2021-12-14 11:07:42.619981: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/train/plugins/profile/2021_12_14_11_07_42\n",
      "[1,0]<stderr>:\n",
      "[1,0]<stderr>:2021-12-14 11:07:42.623859: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./logs/train/plugins/profile/2021_12_14_11_07_42/dgx06.memory_profile.json.gz\n",
      "[1,0]<stderr>:2021-12-14 11:07:42.629506: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./logs/train/plugins/profile/2021_12_14_11_07_42\n",
      "[1,0]<stderr>:Dumped tool data for xplane.pb to ./logs/train/plugins/profile/2021_12_14_11_07_42/dgx06.xplane.pb\n",
      "[1,0]<stderr>:Dumped tool data for overview_page.pb to ./logs/train/plugins/profile/2021_12_14_11_07_42/dgx06.overview_page.pb\n",
      "[1,0]<stderr>:Dumped tool data for input_pipeline.pb to ./logs/train/plugins/profile/2021_12_14_11_07_42/dgx06.input_pipeline.pb\n",
      "[1,0]<stderr>:Dumped tool data for tensorflow_stats.pb to ./logs/train/plugins/profile/2021_12_14_11_07_42/dgx06.tensorflow_stats.pb\n",
      "[1,0]<stderr>:Dumped tool data for kernel_stats.pb to ./logs/train/plugins/profile/2021_12_14_11_07_42/dgx06.kernel_stats.pb\n",
      "[1,0]<stderr>:\n",
      "[1,0]<stdout>:  6/100 [>.............................] - ETA: 9:42 - loss: 0.6924 [1,0]<stderr>:/usr/local/lib/python3.8/dist-packages/horovod/common/util.py:258: UserWarning: Framework tensorflow installed with version 2.6.0 but found version 2.6.1.\n",
      "[1,0]<stderr>:             This can result in unexpected behavior including runtime errors.\n",
      "[1,0]<stderr>:             Reinstall Horovod using `pip install --no-cache-dir` to build with the new version.\n",
      "[1,0]<stderr>:  warnings.warn(get_version_mismatch_message(name, version, installed_version))\n",
      "[1,0]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.5993s vs `on_train_batch_begin` time: 0.6520s). Check your callbacks.\n",
      "[1,0]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.5993s vs `on_train_batch_end` time: 4.0090s). Check your callbacks.\n",
      "[1,0]<stdout>:100/100 [==============================] - 88s 335ms/step - loss: 0.6760tdout>t>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>\n",
      "[1,0]<stdout>:Epoch 2/2\n",
      "[1,0]<stdout>:100/100 [==============================] - 54s 336ms/step - loss: 0.6630tdout>ut[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>\n",
      "[1,1]<stderr>:/usr/local/lib/python3.8/dist-packages/horovod/common/util.py:258: UserWarning: Framework tensorflow installed with version 2.6.0 but found version 2.6.1.\n",
      "[1,1]<stderr>:             This can result in unexpected behavior including runtime errors.\n",
      "[1,1]<stderr>:             Reinstall Horovod using `pip install --no-cache-dir` to build with the new version.\n",
      "[1,1]<stderr>:  warnings.warn(get_version_mismatch_message(name, version, installed_version))\n"
     ]
    }
   ],
   "source": [
    "!horovodrun -np 2 sh hvd_wrapper.sh python tf_trainer.py --dir_in $BASE_DIR --batch_size 16384"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
