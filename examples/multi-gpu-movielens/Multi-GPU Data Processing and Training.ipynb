{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "satisfactory-consequence",
   "metadata": {},
   "source": [
    "### From Multi-GPU Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "guided-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# External Dependencies\n",
    "import cupy as cp\n",
    "import cudf\n",
    "import dask_cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "from dask.utils import parse_bytes\n",
    "from dask.delayed import delayed\n",
    "import rmm\n",
    "\n",
    "# NVTabular\n",
    "import nvtabular as nvt\n",
    "import nvtabular.ops as ops\n",
    "from nvtabular.io import Shuffle\n",
    "from nvtabular.utils import device_mem_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "round-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a \"fast\" root directory for this example\n",
    "BASE_DIR = os.environ.get(\"BASE_DIR\", \"./basedir\")\n",
    "\n",
    "# Define and clean our worker/output directories\n",
    "dask_workdir = os.path.join(BASE_DIR, \"workdir\")\n",
    "demo_output_path = os.path.join(BASE_DIR, \"demo_output\")\n",
    "demo_dataset_path = os.path.join(BASE_DIR, \"demo_dataset\")\n",
    "\n",
    "# Ensure BASE_DIR exists\n",
    "if not os.path.isdir(BASE_DIR):\n",
    "    os.mkdir(BASE_DIR)\n",
    "\n",
    "# Make sure we have a clean worker space for Dask\n",
    "if os.path.isdir(dask_workdir):\n",
    "    shutil.rmtree(dask_workdir)\n",
    "os.mkdir(dask_workdir)\n",
    "\n",
    "# Make sure we have a clean output path\n",
    "if os.path.isdir(demo_output_path):\n",
    "    shutil.rmtree(demo_output_path)\n",
    "os.mkdir(demo_output_path)\n",
    "\n",
    "# Get device memory capacity\n",
    "capacity = device_mem_size(kind=\"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "correct-remains",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:40513</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>3</li>\n",
       "  <li><b>Cores: </b>3</li>\n",
       "  <li><b>Memory: </b>135.10 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:40513' processes=3 threads=3, memory=135.10 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deploy a Single-Machine Multi-GPU Cluster\n",
    "protocol = \"tcp\"             # \"tcp\" or \"ucx\"\n",
    "visible_devices = \"0,1,2\"  # Delect devices to place workers\n",
    "device_spill_frac = 0.9      # Spill GPU-Worker memory to host at this limit.\n",
    "                             # Reduce if spilling fails to prevent\n",
    "                             # device memory errors.\n",
    "cluster = None               # (Optional) Specify existing scheduler port\n",
    "if cluster is None:\n",
    "    cluster = LocalCUDACluster(\n",
    "        protocol = protocol,\n",
    "        CUDA_VISIBLE_DEVICES = visible_devices,\n",
    "        local_directory = dask_workdir,\n",
    "        device_memory_limit = capacity * device_spill_frac,\n",
    "    )\n",
    "\n",
    "# Create the distributed client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "second-warrant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:36181': None,\n",
       " 'tcp://127.0.0.1:43041': None,\n",
       " 'tcp://127.0.0.1:45801': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize RMM pool on ALL workers\n",
    "def _rmm_pool():\n",
    "    rmm.reinitialize(\n",
    "        pool_allocator=True,\n",
    "        initial_pool_size=None, # Use default size\n",
    "    )\n",
    "    \n",
    "client.run(_rmm_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-apparel",
   "metadata": {},
   "source": [
    "### From Getting Started MovieLens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-nudist",
   "metadata": {},
   "source": [
    "#### Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "latest-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import os\n",
    "import cudf                 # cuDF is an implementation of Pandas-like Dataframe on GPU\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "spanish-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.exists(BASE_DIR + 'ml-25m'):\n",
    "    os.makedirs(BASE_DIR, exist_ok=True)\n",
    "    zip_path = os.path.join(BASE_DIR, 'ml-25m.zip')\n",
    "    if not path.exists(zip_path):\n",
    "        os.system(\"mkdir -p \" + BASE_DIR)\n",
    "        os.system(\"wget http://files.grouplens.org/datasets/movielens/ml-25m.zip\")\n",
    "        os.system(\"mv ml-25m.zip \" + BASE_DIR)\n",
    "    \n",
    "    os.system(\"unzip \" + zip_path + \" -d \" + BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-stake",
   "metadata": {},
   "source": [
    "#### Convert the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = cudf.read_csv(os.path.join(BASE_DIR, 'ml-25m/movies.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['genres'] = movies['genres'].str.split('|')\n",
    "movies = movies.drop('title', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.to_parquet(os.path.join(BASE_DIR, \"ml-25m\", \"movies_converted.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-agent",
   "metadata": {},
   "source": [
    "#### Splitting into train and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = cudf.read_csv(os.path.join(BASE_DIR, \"ml-25m\", \"ratings.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.drop('timestamp', axis=1)\n",
    "train, valid = train_test_split(ratings, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = (\n",
    "    ['userId', 'movieId'] >> \n",
    "    nvt.ops.JoinExternal(movies, on=['movieId'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-element",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(os.path.join(BASE_DIR, \"train.parquet\"))\n",
    "valid.to_parquet(os.path.join(BASE_DIR, \"valid.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-scotland",
   "metadata": {},
   "source": [
    "### ETL with NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies= cudf.read_parquet(os.path.join(BASE_DIR, \"ml-25m\", \"movies_converted.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-parts",
   "metadata": {},
   "source": [
    "#### Defining our Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-canal",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = joined >> nvt.ops.Categorify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = nvt.ColumnGroup(['rating']) >> (lambda col: (col>3).astype('int8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = cat_features+ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = nvt.Workflow(output, client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.save(os.path.join(BASE_DIR, \"workflow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-villa",
   "metadata": {},
   "source": [
    "#### Running the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r $BASE_DIR/train\n",
    "!rm -r $BASE_DIR/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = nvt.Dataset([os.path.join(BASE_DIR, \"train.parquet\")], part_size=\"100MB\")\n",
    "valid_iter = nvt.Dataset([os.path.join(BASE_DIR, \"valid.parquet\")], part_size=\"100MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "shuffle = Shuffle.PER_WORKER  # Shuffle algorithm\n",
    "out_files_per_proc = 8        # Number of output files per worker\n",
    "workflow.fit_transform(train_iter).to_parquet(\n",
    "    output_path=os.path.join(BASE_DIR, \"train\"),\n",
    "    shuffle=shuffle,\n",
    "    out_files_per_proc=out_files_per_proc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "shuffle = Shuffle.PER_WORKER  # Shuffle algorithm\n",
    "out_files_per_proc = 8        # Number of output files per worker\n",
    "workflow.fit_transform(valid_iter).to_parquet(\n",
    "    output_path=os.path.join(BASE_DIR, \"valid\"),\n",
    "    shuffle=shuffle,\n",
    "    out_files_per_proc=out_files_per_proc,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-calibration",
   "metadata": {},
   "source": [
    "#### Checking the pre-processing outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "TRAIN_PATHS = sorted(glob.glob(os.path.join(BASE_DIR, \"train\", \"*.parquet\")))\n",
    "VALID_PATHS = sorted(glob.glob(os.path.join(BASE_DIR, \"valid\", \"*.parquet\")))\n",
    "TRAIN_PATHS, VALID_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.read_parquet(TRAIN_PATHS[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-surname",
   "metadata": {},
   "source": [
    "### From Tensorflow Multi-GPU Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile './hvd_wrapper.sh'\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "## Get local process ID from OpenMPI or alternatively from SLURM\n",
    "if [ -z \"${CUDA_VISIBLE_DEVICES:-}\" ]; then\n",
    "    if [ -n \"${OMPI_COMM_WORLD_LOCAL_RANK:-}\" ]; then\n",
    "        LOCAL_RANK=\"${OMPI_COMM_WORLD_LOCAL_RANK}\"\n",
    "    elif [ -n \"${SLURM_LOCALID:-}\" ]; then\n",
    "        LOCAL_RANK=\"${SLURM_LOCALID}\"\n",
    "    fi\n",
    "    export CUDA_VISIBLE_DEVICES=${LOCAL_RANK}\n",
    "fi\n",
    "\n",
    "exec \"$@\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile './trainer.py'\n",
    "\n",
    "# External dependencies\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import cupy\n",
    "\n",
    "# we can control how much memory to give tensorflow with this environment variable\n",
    "# IMPORTANT: make sure you do this before you initialize TF's runtime, otherwise\n",
    "# TF will have claimed all free GPU memory\n",
    "os.environ[\"TF_MEMORY_ALLOCATION\"] = \"0.3\"  # fraction of free memory\n",
    "import horovod.tensorflow as hvd  # noqa: E402\n",
    "import tensorflow as tf  # noqa: E402\n",
    "\n",
    "import nvtabular as nvt  # noqa: E402\n",
    "from nvtabular.framework_utils.tensorflow import layers  # noqa: E402\n",
    "from nvtabular.loader.tensorflow import KerasSequenceLoader  # noqa: E402\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Process some integers.\")\n",
    "parser.add_argument(\"--dir_in\", default=None, help=\"Input directory\")\n",
    "parser.add_argument(\"--b_size\", default=None, help=\"batch size\")\n",
    "parser.add_argument(\"--cats\", default=None, help=\"categorical columns\")\n",
    "parser.add_argument(\"--cats_mh\", default=None, help=\"categorical multihot columns\")\n",
    "parser.add_argument(\"--conts\", default=None, help=\"continuous columns\")\n",
    "parser.add_argument(\"--labels\", default=None, help=\"continuous columns\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "BASE_DIR = args.dir_in or \"./data/\"\n",
    "BATCH_SIZE = args.b_size or 16384  # Batch Size\n",
    "CATEGORICAL_COLUMNS = args.cats or [\"movieId\", \"userId\"]  # Single-hot\n",
    "CATEGORICAL_MH_COLUMNS = args.cats_mh or [\"genres\"]  # Multi-hot\n",
    "NUMERIC_COLUMNS = args.conts or []\n",
    "TRAIN_PATHS = sorted(\n",
    "    glob.glob(os.path.join(BASE_DIR, \"train/*.parquet\"))\n",
    ")  # Output from ETL-with-NVTabular\n",
    "hvd.init()\n",
    "\n",
    "# Seed with system randomness (or a static seed)\n",
    "cupy.random.seed(None)\n",
    "\n",
    "\n",
    "def seed_fn():\n",
    "    \"\"\"\n",
    "    Generate consistent dataloader shuffle seeds across workers\n",
    "\n",
    "    Reseeds each worker's dataloader each epoch to get fresh a shuffle\n",
    "    that's consistent across workers.\n",
    "    \"\"\"\n",
    "    min_int, max_int = tf.int32.limits\n",
    "    max_rand = max_int // hvd.size()\n",
    "\n",
    "    # Generate a seed fragment\n",
    "    seed_fragment = cupy.random.randint(0, max_rand).get()\n",
    "\n",
    "    # Aggregate seed fragments from all Horovod workers\n",
    "    seed_tensor = tf.constant(seed_fragment)\n",
    "    reduced_seed = hvd.allreduce(seed_tensor, name=\"shuffle_seed\", op=hvd.mpi_ops.Sum) % max_rand\n",
    "\n",
    "    return reduced_seed\n",
    "\n",
    "\n",
    "proc = nvt.Workflow.load(os.path.join(BASE_DIR, \"workflow/\"))\n",
    "EMBEDDING_TABLE_SHAPES = nvt.ops.get_embedding_sizes(proc)\n",
    "\n",
    "train_dataset_tf = KerasSequenceLoader(\n",
    "    TRAIN_PATHS,  # you could also use a glob pattern\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_names=[\"rating\"],\n",
    "    cat_names=CATEGORICAL_COLUMNS + CATEGORICAL_MH_COLUMNS,\n",
    "    cont_names=NUMERIC_COLUMNS,\n",
    "    engine=\"parquet\",\n",
    "    shuffle=True,\n",
    "    seed_fn=seed_fn,\n",
    "    buffer_size=0.06,  # how many batches to load at once\n",
    "    parts_per_chunk=1,\n",
    "    global_size=hvd.size(),\n",
    "    global_rank=hvd.rank(),\n",
    ")\n",
    "inputs = {}  # tf.keras.Input placeholders for each feature to be used\n",
    "emb_layers = []  # output of all embedding layers, which will be concatenated\n",
    "for col in CATEGORICAL_COLUMNS:\n",
    "    inputs[col] = tf.keras.Input(name=col, dtype=tf.int32, shape=(1,))\n",
    "# Note that we need two input tensors for multi-hot categorical features\n",
    "for col in CATEGORICAL_MH_COLUMNS:\n",
    "    inputs[col + \"__values\"] = tf.keras.Input(name=f\"{col}__values\", dtype=tf.int64, shape=(1,))\n",
    "    inputs[col + \"__nnzs\"] = tf.keras.Input(name=f\"{col}__nnzs\", dtype=tf.int64, shape=(1,))\n",
    "for col in CATEGORICAL_COLUMNS + CATEGORICAL_MH_COLUMNS:\n",
    "    emb_layers.append(\n",
    "        tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_identity(\n",
    "                col, EMBEDDING_TABLE_SHAPES[col][0]  # Input dimension (vocab size)\n",
    "            ),\n",
    "            EMBEDDING_TABLE_SHAPES[col][1],  # Embedding output dimension\n",
    "        )\n",
    "    )\n",
    "emb_layer = layers.DenseFeatures(emb_layers)\n",
    "x_emb_output = emb_layer(inputs)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(x_emb_output)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "loss = tf.losses.BinaryCrossentropy()\n",
    "opt = tf.keras.optimizers.SGD(0.01 * hvd.size())\n",
    "opt = hvd.DistributedOptimizer(opt)\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "checkpoint = tf.train.Checkpoint(model=model, optimizer=opt)\n",
    "\n",
    "\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def training_step(examples, labels, first_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        probs = model(examples, training=True)\n",
    "        loss_value = loss(labels, probs)\n",
    "    # Horovod: add Horovod Distributed GradientTape.\n",
    "    tape = hvd.DistributedGradientTape(tape, sparse_as_dense=True)\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "    # This is necessary to ensure consistent initialization of all workers when\n",
    "    # training is started with random weights or restored from a checkpoint.\n",
    "    #\n",
    "    # Note: broadcast should be done after the first gradient step to ensure optimizer\n",
    "    # initialization.\n",
    "    if first_batch:\n",
    "        hvd.broadcast_variables(model.variables, root_rank=0)\n",
    "        hvd.broadcast_variables(opt.variables(), root_rank=0)\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "# Horovod: adjust number of steps based on number of GPUs.\n",
    "for batch, (examples, labels) in enumerate(train_dataset_tf):\n",
    "    loss_value = training_step(examples, labels, batch == 0)\n",
    "    if batch % 10 == 0 and hvd.local_rank() == 0:\n",
    "        print(\"Step #%d\\tLoss: %.6f\" % (batch, loss_value))\n",
    "hvd.join()\n",
    "# Horovod: save checkpoints only on worker 0 to prevent other workers from\n",
    "# corrupting it.\n",
    "if hvd.rank() == 0:\n",
    "    checkpoint.save(checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "!horovodrun -np 3 sh hvd_wrapper.sh python trainer.py --dir_in $BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-refund",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
