{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f070d053-c865-4410-baa8-0629caac7b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500b9db8-f808-4646-aa66-38c3e3202656",
   "metadata": {},
   "source": [
    "The last step is to deploy the ETL workflow and saved model to production. In the production setting, we want to transform the input data as during training (ETL). We need to apply the same mean/std for continuous features and use the same categorical mapping to convert the categories to continuous integer before we use the deep learning model for a prediction. Therefore, we deploy the NVTabular workflow with the HugeCTR model as an ensemble model to Triton Inference. The ensemble model garantuees that the same transformation are applied to the raw inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e303228b-74b7-4831-8534-2cff18118ca7",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "In this notebook, we learn how to deploy our models to production\n",
    "\n",
    "- Use NVTabular to generate config and model files for Triton Inference Server\n",
    "- Deploy an ensemble of NVTabular workflow and HugeCTR model\n",
    "- Send example request to Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc6dc6c-91d6-49d8-9e49-13190686b9cc",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6972f20f-380a-47e1-b966-19595b500eeb",
   "metadata": {},
   "source": [
    "Before we get started, you should launch the Triton Inference Server docker container with the following script. This script will mount your local `model-repository` folder where you stored the models to `/model` into the `merlin-inference` docker container.\n",
    "\n",
    "```\n",
    "docker run -it --gpus=all -p 8000:8000 -p 8001:8001 -p 8002:8002 -v ${PWD}:/model nvcr.io/nvidia/merlin/merlin-inference:0.5\n",
    "```\n",
    "Once the container launches, activate the `merlin` environment:\n",
    "```\n",
    "source activate merlin\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752f2d68-df64-4621-8362-2a60b46d3d5f",
   "metadata": {},
   "source": [
    "### Saving Ensemble Model for Triton Inference ServerÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd185eb8-20d8-4c07-ada7-9405c1891dcc",
   "metadata": {},
   "source": [
    "First, we need to generate the Triton Inference Server configurations and save the models in the correct format. In the previous notebooks [02-ETL-with-NVTabular](https://github.com/NVIDIA/NVTabular/blob/main/examples/getting-started-movielens/02-ETL-with-NVTabular.ipynb) and [03c-Training-with-HugeCTR](https://github.com/NVIDIA/NVTabular/blob/main/examples/getting-started-movielens/03a-Training-with-HugeCTR.ipynb) we saved the NVTabular workflow and HugeCTR model to disk. We will load them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3767b16-7089-48da-99e3-ad9802539eae",
   "metadata": {},
   "source": [
    "After training terminates, we can see that two `.model` files are generated. We need to move them inside `1` folder under the `movielens_hugectr` folder. Let's create these folders first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcb9d3b9-7e81-4b02-bf8e-cddd6ab78a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c6a8811-e127-440a-ab8a-19e28929febb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('mkdir -p /model/movielens_hugectr/1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bd05a5-81c7-4261-9b86-5610d3adedcf",
   "metadata": {},
   "source": [
    "Now we move our saved .model files inside 1 folder. We use only the last snapshot after 1900 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba17131-0a7b-4a6a-bd2b-64345f3bbb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('mv *.model /model/movielens_hugectr/1/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eb16bd-bfe5-43bb-91ea-4fd4f2f6a280",
   "metadata": {},
   "source": [
    "Note that these stored .model files will be used in the inference. Now we have to create a JSON file for inference which has a similar configuration as our training file. We should remove the solver and optimizer clauses and add the inference clause in the JSON file. The paths of the stored dense model and sparse model(s) should be specified at dense_model_file and sparse_model_file within the inference clause. We need to make some modifications to data in the layers clause. Besides, we need to change the last layer from BinaryCrossEntropyLoss to Sigmoid. The rest of \"layers\" should be exactly the same as that in the training model.py file.\n",
    "\n",
    "Now let's create a movielens.json file inside the movielens/1 folder. We have already retrieved the cardinality of each categorical column using get_embedding_sizes function above. We will use these cardinalities below in the movielens.json file as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9008f146-4baa-4536-9d0f-2f4ef96756cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /model/movielens_hugectr/1/movielens.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile '/model/movielens_hugectr/1/movielens.json'\n",
    "\n",
    "{\n",
    "   \"inference\": {\n",
    "    \"max_batchsize\": 64,\n",
    "    \"hit_rate_threshold\": 0.6,\n",
    "    \"dense_model_file\": \"/model/models/movielens/1/_dense_1900.model\",\n",
    "    \"sparse_model_file\": \"/model/models/movielens/1/0_sparse_1900.model\",\n",
    "    \"label\": 1,\n",
    "    \"input_key_type\": \"I64\"\n",
    "  },\n",
    "  \"layers\": [\n",
    "    {\n",
    "      \"name\": \"data\",\n",
    "      \"type\": \"Data\",\n",
    "      \"format\": \"Parquet\",\n",
    "      \"slot_size_array\": [56586, 162542],\n",
    "      \"source\": \"/model/data/train/_file_list.txt\",\n",
    "      \"eval_source\": \"/model/data/valid/_file_list.txt\",\n",
    "      \"check\": \"Sum\",\n",
    "      \"label\": {\n",
    "        \"top\": \"label\",\n",
    "        \"label_dim\": 1\n",
    "      },\n",
    "      \"dense\": {\n",
    "        \"top\": \"dense\",\n",
    "        \"dense_dim\": 0\n",
    "      },\n",
    "      \"sparse\": [\n",
    "        {\n",
    "          \"top\": \"data1\",\n",
    "          \"type\": \"DistributedSlot\",\n",
    "          \"max_feature_num_per_sample\": 3,\n",
    "          \"slot_num\": 2\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"sparse_embedding1\",\n",
    "      \"type\": \"DistributedSlotSparseEmbeddingHash\",\n",
    "      \"bottom\": \"data1\",\n",
    "      \"top\": \"sparse_embedding1\",\n",
    "      \"sparse_embedding_hparam\": {\n",
    "        \"max_vocabulary_size_per_gpu\": 219128,\n",
    "        \"embedding_vec_size\": 16,\n",
    "        \"combiner\": 0\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"reshape1\",\n",
    "      \"type\": \"Reshape\",\n",
    "      \"bottom\": \"sparse_embedding1\",\n",
    "      \"top\": \"reshape1\",\n",
    "      \"leading_dim\": 32\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc1\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"reshape1\",\n",
    "      \"top\": \"fc1\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 128\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"relu1\",\n",
    "      \"type\": \"ReLU\",\n",
    "      \"bottom\": \"fc1\",\n",
    "      \"top\": \"relu1\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc2\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"relu1\",\n",
    "      \"top\": \"fc2\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 128\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"relu2\",\n",
    "      \"type\": \"ReLU\",\n",
    "      \"bottom\": \"fc2\",\n",
    "      \"top\": \"relu2\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc3\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"relu2\",\n",
    "      \"top\": \"fc3\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 1\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"sigmoid\",\n",
    "      \"type\": \"Sigmoid\",\n",
    "      \"bottom\": \"fc3\",\n",
    "      \"top\": \"sigmoid\"\n",
    "    } \n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4048ea-b78b-4eb6-a1b2-572386b4794c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we can save our models to be deployed at the inference stage. To do so we will use `export_hugectr_ensemble` method below. With this method, we can generate the `config.pbtxt` files automatically for each model. In doing so, we should also create a `hugectr_params` dictionary, and define the parameters  like where the `movielens.json` file will be read, `slots` which corresponds to number of categorical features, `embedding_vector_size`, `max_nnz`, and `n_outputs` which is number of outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5ca601-ca12-42e4-aab5-a1c0973c5c5b",
   "metadata": {},
   "source": [
    "The script below creates an ensemble triton server model where  \n",
    "\n",
    "- `workflow` is the the nvtabular workflow used in preprocessing, \n",
    "- `hugectr_model_path` is the HugeCTR model that should be served. This path includes the `.model` files.\n",
    "- `name` is the base name of the various triton models\n",
    "- `output_path` is the path where is model will be saved to.\n",
    "- `cats` are the categorical column names\n",
    "- `label_columns` are the label column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd2d851f-5559-4eed-a875-e47cd938461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvtabular as nvt\n",
    "\n",
    "# path to store raw and preprocessed data\n",
    "INPUT_DATA_DIR = os.environ.get('INPUT_DATA_DIR', os.path.expanduser(\"~/nvt-examples/movielens/data/\"))\n",
    "# path to save models\n",
    "MODEL_BASE_DIR = os.environ.get('MODEL_BASE_DIR', '/model/')\n",
    "\n",
    "workflow = nvt.Workflow.load(os.path.join(INPUT_DATA_DIR, \"workflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2586843f-0d19-4c1b-ab25-ec8f2ed75580",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = ['userId', 'movieId']\n",
    "LABEL_COLUMNS = ['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffcb0de-1b3d-4e46-9e84-2b83bba03893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvtabular.inference.triton import export_hugectr_ensemble\n",
    "hugectr_params = dict()\n",
    "hugectr_params[\"config\"] = \"/model/models/movielens/1/movielens.json\"\n",
    "hugectr_params[\"slots\"] = 2\n",
    "hugectr_params[\"max_nnz\"] = 2\n",
    "hugectr_params[\"embedding_vector_size\"] = 16\n",
    "hugectr_params[\"n_outputs\"] = 1\n",
    "export_hugectr_ensemble(workflow=workflow, \n",
    "                        hugectr_model_path=\"/model/movielens_hugectr/1/\",\n",
    "                        hugectr_params=hugectr_params,\n",
    "                        name=\"movielens\", \n",
    "                        output_path=\"/model/models/\", \n",
    "                        label_columns=LABEL_COLUMNS, \n",
    "                        cats=CATEGORICAL_COLUMNS,\n",
    "                        max_batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f0a1fd-6f9f-4829-a1e2-f7849d5c2bc7",
   "metadata": {},
   "source": [
    "After we run the script above, we will have three model folders saved as `movielens_nvt`, `movielens` and `movielens_ens`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10779f2-ed40-4227-a618-2ae112d94703",
   "metadata": {},
   "source": [
    "## Load Models on Triton Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea6994-2cb2-47bf-990a-e0eb19d2bdd3",
   "metadata": {},
   "source": [
    "At this stage, you should have already started the `merlin-inference` container. After you started the container you can start triton server with the command below:\n",
    "\n",
    "```\n",
    "tritonserver --model-repository=path_to_models --backend-config=hugectr,movielens=path_to_json_file --backend-config=hugectr,supportlonglong=true --model-control-mode=explicit\n",
    "```\n",
    "\n",
    "Note: The model-repository path is `/model/models/`. The models haven't been loaded, yet. We can request triton server to load the saved ensemble. We initialize a triton client. The path for the json file is `/model/models/movielens/1/movielens.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33965417-bb6e-4399-876f-b7b49bf805df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "295e488c-9414-4ddd-804f-65f64f4b0c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/merlin/lib/python3.8/site-packages/tritonhttpclient/__init__.py:30: DeprecationWarning: The package `tritonhttpclient` is deprecated and will be removed in a future version. Please use instead `tritonclient.http`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client created.\n"
     ]
    }
   ],
   "source": [
    "import tritonhttpclient\n",
    "try:\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db4625-12c2-461d-8c8c-3a922ec31fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client.is_server_live()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ec4e8-e5ea-41fc-a1f5-0096d05f8958",
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client.get_model_repository_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706c1594-df0a-4423-9d89-021fed8a6972",
   "metadata": {},
   "source": [
    "We check the available models in the repositories:\n",
    "\n",
    "- criteo_ens: Ensemble\n",
    "- criteo_nvt: NVTabular\n",
    "- criteo: HugeCTR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaf6226-a10d-462d-8d81-fe5fdc89c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client.get_model_repository_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91add7ab-ef9e-4f14-8b62-75263df0a1cc",
   "metadata": {},
   "source": [
    "Let's load our models to Triton Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8a02e-1167-4495-808b-dad4968231e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "triton_client.load_model(model_name='movielens_nvt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dade770e-9796-4ff0-93e7-446c308c4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "triton_client.load_model(model_name='movielens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a08bcd-93c4-473c-9463-83b380771a5f",
   "metadata": {},
   "source": [
    "Finally, we load our ensemble model movielens_ens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fde03d-db13-4cf9-bc4c-78a5265dd59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "triton_client.load_model(model_name='movielens_ens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a48b2f1-9e94-41a7-9d72-550a88996ad8",
   "metadata": {},
   "source": [
    "Let's send a request to Inference Server and print out the response. Since in our example above we do not have continuous columns, below our only inputs are categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fee51-27de-4400-9ab9-056d419eee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as httpclient\n",
    "import nvtabular\n",
    "import cudf\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "model_name = 'movielens_ens'\n",
    "col_names = [\"movieId\", \"userId\"]\n",
    "# read in a batch of data to get transforms for\n",
    "batch = cudf.read_parquet('/model/data/valid.parquet', num_rows=3)[col_names]\n",
    "print(batch, \"\\n\")\n",
    "\n",
    "# convert the batch to a triton inputs\n",
    "columns = [(col, batch[col][0:3]) for col in col_names]\n",
    "inputs = []\n",
    "\n",
    "col_dtypes = [np.int64, np.int64]\n",
    "for i, (name, col) in enumerate(columns):\n",
    "    d = col.values_host.astype(col_dtypes[i])\n",
    "    d = d.reshape(len(d), 1)\n",
    "    inputs.append(httpclient.InferInput(name, d.shape, np_to_triton_dtype(col_dtypes[i])))\n",
    "    inputs[i].set_data_from_numpy(d)\n",
    "# placeholder variables for the output\n",
    "outputs = []\n",
    "outputs.append(httpclient.InferRequestedOutput(\"OUTPUT0\"))\n",
    "# make the request\n",
    "with httpclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer(model_name, inputs, request_id=str(1), outputs=outputs)\n",
    "# print(response.as_numpy('OUTPUT0'))\n",
    "print(\"predicted sigmoid result:\\n\", response.as_numpy(\"OUTPUT0\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
