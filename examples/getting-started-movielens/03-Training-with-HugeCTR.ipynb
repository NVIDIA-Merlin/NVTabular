{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bde644d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf5a92",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we want to provide an overview what HugeCTR framework is, its features and benefits. We will use HugeCTR to train a basic neural network architecture.\n",
    "\n",
    "<b>Learning Objectives</b>:\n",
    "* Adopt NVTabular workflow to provide input files to HugeCTR\n",
    "* Define HugeCTR neural network architecture\n",
    "* Train a deep learning model with HugeCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9ae9a9",
   "metadata": {},
   "source": [
    "### Why using HugeCTR?\n",
    "\n",
    "HugeCTR is a GPU-accelerated recommender framework designed to distribute training across multiple GPUs and nodes and estimate Click-Through Rates (CTRs).<br>\n",
    "\n",
    "HugeCTR offers multiple advantages to train deep learning recommender systems:\n",
    "1. **Speed**: HugeCTR is a highly efficient framework written C++. We experienced up to 10x speed up. HugeCTR on a NVIDIA DGX A100 system proved to be the fastest commercially available solution for training the architecture Deep Learning Recommender Model (DLRM) developed by Facebook.\n",
    "2. **Scale**: HugeCTR supports model parallel scaling. It distributes the large embedding tables over multiple GPUs or multiple nodes. \n",
    "3. **Easy-to-use**: Easy-to-use Python API similar to Keras. Examples for popular deep learning recommender systems architectures (Wide&Deep, DLRM, DCN, DeepFM) are available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5982a228",
   "metadata": {},
   "source": [
    "### Other Features of HugeCTR\n",
    "\n",
    "HugeCTR is designed to scale deep learning models for recommender systems. It provides a list of other important features:\n",
    "* Proficiency in oversubscribing models to train embedding tables with single nodes that donâ€™t fit within the GPU or CPU memory (only required embeddings are prefetched from a parameter server per batch)\n",
    "* Asynchronous and multithreaded data pipelines\n",
    "* A highly optimized data loader.\n",
    "* Supported data formats such as parquet and binary\n",
    "* Integration with Triton Inference Server for deployment to production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7db23a5",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c9d2e",
   "metadata": {},
   "source": [
    "In this example, we will train a neural network with HugeCTR. We will use preprocessed datasets generated via NVTabular in `02-ETL-with-NVTabular` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c961b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import os\n",
    "import nvtabular as nvt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563d8f08",
   "metadata": {},
   "source": [
    "We define our base directory, containing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24f8edeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to preprocessed data\n",
    "INPUT_DATA_DIR = os.environ.get(\n",
    "    \"INPUT_DATA_DIR\", os.path.expanduser(\"~/nvt-examples/movielens/data/\")\n",
    ")\n",
    "\n",
    "# path to save the models\n",
    "MODEL_BASE_DIR = os.environ.get(\"MODEL_BASE_DIR\", os.path.expanduser(\"~/nvt-examples/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dac2db",
   "metadata": {},
   "source": [
    "Let's load our saved workflow from the `02-ETL-with-NVTabular` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b5bf4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = nvt.Workflow.load(os.path.join(INPUT_DATA_DIR, \"workflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c135c2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'userId': dtype('int64'),\n",
       " 'movieId': dtype('int64'),\n",
       " 'genres': ListDtype(int64),\n",
       " 'rating': dtype('int8')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.output_dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa764881",
   "metadata": {},
   "source": [
    "Note: We do not have numerical output columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962d3b86",
   "metadata": {},
   "source": [
    "Let's clear existing directory and create the output folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ff07278",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = os.path.join(INPUT_DATA_DIR, \"model/movielens_hugectr/\")\n",
    "!rm -rf {MODEL_DIR}\n",
    "!mkdir -p {MODEL_DIR}\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5147aca3",
   "metadata": {},
   "source": [
    "## Scaling Accelerated training with HugeCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42971489",
   "metadata": {},
   "source": [
    "HugeCTR is a deep learning framework dedicated to recommendation systems. It is written in CUDA C++. As HugeCTR optimizes the training in CUDA++, we need to define the training pipeline and model architecture and execute it via the commandline. We will use the Python API, which is similar to Keras models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e9590f",
   "metadata": {},
   "source": [
    "HugeCTR has three main components:\n",
    "* Solver: Specifies various details such as active GPU list, batchsize, and model_file\n",
    "* Optimizer: Specifies the type of optimizer and its hyperparameters\n",
    "* DataReader: Specifies the training/evaludation data\n",
    "* Model: Specifies embeddings, and dense layers. Note that embeddings must precede the dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e042f7",
   "metadata": {},
   "source": [
    "**Solver**\n",
    "\n",
    "Let's take a look on the parameter for the `Solver`. We should be familiar from other frameworks for the hyperparameter.\n",
    "\n",
    "```\n",
    "solver = hugectr.CreateSolver(\n",
    "- vvgpu: GPU indices used in the training process, which has two levels. For example: [[0,1],[1,2]] indicates that two nodes are used in the first node. GPUs 0 and 1 are used while GPUs 1 and 2 are used for the second node. It is also possible to specify non-continuous GPU indices such as [0, 2, 4, 7]  \n",
    "- batchsize: Minibatch size used in training\n",
    "- max_eval_batches: Maximum number of batches used in evaluation. It is recommended that the number is equal to or bigger than the actual number of bathces in the evaluation dataset.\n",
    "If max_iter is used, the evaluation happens for max_eval_batches by repeating the evaluation dataset infinitely.\n",
    "On the other hand, with num_epochs, HugeCTR stops the evaluation if all the evaluation data is consumed    \n",
    "- batchsize_eval: Maximum number of batches used in evaluation. It is recommended that the number is equal to or\n",
    "  bigger than the actual number of bathces in the evaluation dataset\n",
    "- mixed_precision: Enables mixed precision training with the scaler specified here. Only 128,256, 512, and 1024 scalers are supported\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de604b1",
   "metadata": {},
   "source": [
    "**Optimizer**\n",
    "\n",
    "The optimizer is the algorithm to update the model parameters. HugeCTR supports the common algorithms.\n",
    "\n",
    "\n",
    "```\n",
    "optimizer = CreateOptimizer(\n",
    "- optimizer_type: Optimizer algorithm - Adam, MomentumSGD, Nesterov, and SGD \n",
    "- learning_rate: Learning Rate for optimizer\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e553eaa",
   "metadata": {},
   "source": [
    "**DataReader**\n",
    "\n",
    "The data reader defines the training and evaluation dataset.\n",
    "\n",
    "\n",
    "```\n",
    "reader = hugectr.DataReaderParams(\n",
    "- data_reader_type: Data format to read\n",
    "- source: The training dataset file list. IMPORTANT: This should be a list\n",
    "- eval_source: The evaluation dataset file list.\n",
    "- check_type: The data error detection mechanism (Sum: Checksum, None: no detection).\n",
    "- slot_size_array: The list of categorical feature cardinalities\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd98658",
   "metadata": {},
   "source": [
    "**Model**\n",
    "\n",
    "We initialize the model with the solver, optimizer and data reader:\n",
    "\n",
    "```\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "```\n",
    "\n",
    "We can add multiple layers to the model with `model.add` function. We will focus on:\n",
    "- `Input` defines the input data\n",
    "- `SparseEmbedding` defines the embedding layer\n",
    "- `DenseLayer` defines dense layers, such as fully connected, ReLU, BatchNorm, etc.\n",
    "\n",
    "**HugeCTR organizes the layers by names. For each layer, we define the input and output names.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f0f39d",
   "metadata": {},
   "source": [
    "Input layer:\n",
    "\n",
    "This layer is required to define the input data.\n",
    "\n",
    "```\n",
    "hugectr.Input(\n",
    "    label_dim: Number of label columns\n",
    "    label_name: Name of label columns in network architecture\n",
    "    dense_dim: Number of continuous columns\n",
    "    dense_name: Name of contiunous columns in network architecture\n",
    "    data_reader_sparse_param_array: Configuration how to read sparse data and its names\n",
    ")\n",
    "```\n",
    "\n",
    "SparseEmbedding:\n",
    "\n",
    "This layer defines embedding table\n",
    "\n",
    "```\n",
    "hugectr.SparseEmbedding(\n",
    "    embedding_type: Different embedding options to distribute embedding tables \n",
    "    workspace_size_per_gpu_in_mb: Maximum embedding table size in MB\n",
    "    embedding_vec_size: Embedding vector size\n",
    "    combiner: Intra-slot reduction op\n",
    "    sparse_embedding_name: Layer name\n",
    "    bottom_name: Input layer names\n",
    "    optimizer: Optimizer to use\n",
    ")\n",
    "```\n",
    "\n",
    "DenseLayer:\n",
    "\n",
    "This layer is copied to each GPU and is normally used for the MLP tower.\n",
    "\n",
    "```\n",
    "hugectr.DenseLayer(\n",
    "    layer_type: Layer type, such as FullyConnected, Reshape, Concat, Loss, BatchNorm, etc.\n",
    "    bottom_names: Input layer names\n",
    "    top_names: Layer name\n",
    "    ...: Depending on the layer type additional parameter can be defined\n",
    ")\n",
    "```\n",
    "\n",
    "This is only a short introduction in the API. You can read more in the official docs: [Python Interface](https://github.com/NVIDIA/HugeCTR/blob/master/docs/python_interface.md) and [Layer Book](https://github.com/NVIDIA/HugeCTR/blob/master/docs/hugectr_layer_book.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b99d7d",
   "metadata": {},
   "source": [
    "## Let's define our model\n",
    "\n",
    "We walked through the documentation, but it is useful to understand the API. Finally, we can define our model. We will write the model to `./model.py` and execute it afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd574a",
   "metadata": {},
   "source": [
    "We need the cardinalities of each categorical feature to assign as `slot_size_array` in the model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8fed2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'userId': (162542, 512), 'movieId': (56586, 512)}, {'genres': (21, 16)})\n"
     ]
    }
   ],
   "source": [
    "from nvtabular.ops import get_embedding_sizes\n",
    "\n",
    "embeddings = get_embedding_sizes(workflow)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55454c6f",
   "metadata": {},
   "source": [
    "We use `graph_to_json` to convert the model to a JSON configuration, required for the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb325fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, 'w') as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06cecd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate model.py\n",
    "\n",
    "import hugectr\n",
    "from mpi4py import MPI  # noqa\n",
    "\n",
    "solver = hugectr.CreateSolver(\n",
    "    vvgpu=[[0]],\n",
    "    batchsize=2048,\n",
    "    batchsize_eval=2048,\n",
    "    max_eval_batches=160,\n",
    "    i64_input_key=True,\n",
    "    use_mixed_precision=False,\n",
    "    repeat_dataset=True,\n",
    ")\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type=hugectr.Optimizer_t.Adam)\n",
    "reader = hugectr.DataReaderParams(\n",
    "    data_reader_type=hugectr.DataReaderType_t.Parquet,\n",
    "    source=[\"{INPUT_DATA_DIR}train/_file_list.txt\"],\n",
    "    eval_source=\"{INPUT_DATA_DIR}valid/_file_list.txt\",\n",
    "    check_type=hugectr.Check_t.Non,\n",
    "    slot_size_array=[162542, 56586, 21],\n",
    ")\n",
    "\n",
    "\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "\n",
    "model.add(\n",
    "    hugectr.Input(\n",
    "        label_dim=1,\n",
    "        label_name=\"label\",\n",
    "        dense_dim=0,\n",
    "        dense_name=\"dense\",\n",
    "        data_reader_sparse_param_array=[\n",
    "            hugectr.DataReaderSparseParam(\"data1\", nnz_per_slot=[1, 1, 2], is_fixed_length=False, slot_num=3)\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.SparseEmbedding(\n",
    "        embedding_type=hugectr.Embedding_t.LocalizedSlotSparseEmbeddingHash,\n",
    "        workspace_size_per_gpu_in_mb=200,\n",
    "        embedding_vec_size=16,\n",
    "        combiner=\"sum\",\n",
    "        sparse_embedding_name=\"sparse_embedding1\",\n",
    "        bottom_name=\"data1\",\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.Reshape,\n",
    "        bottom_names=[\"sparse_embedding1\"],\n",
    "        top_names=[\"reshape1\"],\n",
    "        leading_dim=48,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"reshape1\"],\n",
    "        top_names=[\"fc1\"],\n",
    "        num_output=128,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.ReLU,\n",
    "        bottom_names=[\"fc1\"],\n",
    "        top_names=[\"relu1\"],\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu1\"],\n",
    "        top_names=[\"fc2\"],\n",
    "        num_output=128,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.ReLU,\n",
    "        bottom_names=[\"fc2\"],\n",
    "        top_names=[\"relu2\"],\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu2\"],\n",
    "        top_names=[\"fc3\"],\n",
    "        num_output=1,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "        bottom_names=[\"fc3\", \"label\"],\n",
    "        top_names=[\"loss\"],\n",
    "    )\n",
    ")\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(max_iter=2000, display=100, eval_interval=200, snapshot=1900)\n",
    "model.graph_to_json(graph_config_file=\"{MODEL_DIR}1/movielens.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a417e565",
   "metadata": {},
   "source": [
    "We train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ec31508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HugeCTR Version: 3.2\n",
      "====================================================Model Init=====================================================\n",
      "[HUGECTR][20:42:39][INFO][RANK0]: Global seed is 4093587052\n",
      "[HUGECTR][20:42:39][INFO][RANK0]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[HUGECTR][20:42:40][WARNING][RANK0]: Peer-to-peer access cannot be fully enabled.\n",
      "[HUGECTR][20:42:40][INFO][RANK0]: Start all2all warmup\n",
      "[HUGECTR][20:42:40][INFO][RANK0]: End all2all warmup\n",
      "[HUGECTR][20:42:40][INFO][RANK0]: Using All-reduce algorithm: NCCL\n",
      "[HUGECTR][20:42:40][INFO][RANK0]: Device 0: Tesla V100-DGXS-16GB\n",
      "[HUGECTR][20:42:40][INFO][RANK0]: num of DataReader workers: 1\n",
      "[HUGECTR][20:42:41][INFO][RANK0]: Vocabulary size: 219149\n",
      "[HUGECTR][20:42:41][INFO][RANK0]: max_vocabulary_size_per_gpu_=3276800\n",
      "[HUGECTR][20:42:41][INFO][RANK0]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "[HUGECTR][20:42:43][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][20:42:43][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][20:42:43][INFO][RANK0]: Starting AUC NCCL warm-up\n",
      "[HUGECTR][20:42:43][INFO][RANK0]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1                         \n",
      "(None, 1)                               (None, 0)                               \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "LocalizedSlotSparseEmbeddingHash        data1                         sparse_embedding1             (None, 3, 16)                 \n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 48)                    \n",
      "InnerProduct                            reshape1                      fc1                           (None, 128)                   \n",
      "ReLU                                    fc1                           relu1                         (None, 128)                   \n",
      "InnerProduct                            relu1                         fc2                           (None, 128)                   \n",
      "ReLU                                    fc2                           relu2                         (None, 128)                   \n",
      "InnerProduct                            relu2                         fc3                           (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  fc3,label                     loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[HUGECTR][20:42:43][INFO][RANK0]: Use non-epoch mode with number of iterations: 2000\n",
      "[HUGECTR][20:42:43][INFO][RANK0]: Training batchsize: 2048, evaluation batchsize: 2048\n",
      "[HUGECTR][20:42:43][INFO][RANK0]: Evaluation interval: 200, snapshot interval: 1900\n",
      "[HUGECTR][20:42:43][INFO][RANK0]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[HUGECTR][20:42:43][INFO][RANK0]: Use mixed precision: 0, scaler: 1, use cuda graph: -891441590\n",
      "[HUGECTR][20:42:43][INFO][RANK0]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[HUGECTR][20:42:43][INFO][RANK0]: Training source file: /root/nvt-examples/movielens/data/train/_file_list.txt\n",
      "[HUGECTR][20:42:43][INFO][RANK0]: Evaluation source file: /root/nvt-examples/movielens/data/valid/_file_list.txt\n",
      "terminate called after throwing an instance of 'HugeCTR::internal_runtime_error'\n",
      "  what():  Runtime error: an illegal memory access was encountered /var/tmp/HugeCTR/HugeCTR/src/embeddings/localized_slot_sparse_embedding_hash.cu:141 \n",
      "\n",
      "[a2eb792b60ca:00602] *** Process received signal ***\n",
      "[a2eb792b60ca:00602] Signal: Aborted (6)\n",
      "[a2eb792b60ca:00602] Signal code:  (-6)\n",
      "Runtime error: an illegal memory access was encountered /var/tmp/HugeCTR/HugeCTR/src/data_readers/parquet_data_converter.cu:404 \n",
      "\n",
      "[a2eb792b60ca:00602] [ 0] /usr/lib/x86_64-linux-gnu/libc.so.6(+0x46210)[0x7f05cd915210]\n",
      "[a2eb792b60ca:00602] [ 1] /usr/lib/x86_64-linux-gnu/libc.so.6(gsignal+0xcb)[0x7f05cd91518b]\n",
      "[a2eb792b60ca:00602] [ 2] /usr/lib/x86_64-linux-gnu/libc.so.6(abort+0x12b)[0x7f05cd8f4859]\n",
      "[a2eb792b60ca:00602] [ 3] /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0x9e911)[0x7f05b376e911]\n",
      "[a2eb792b60ca:00602] [ 4] /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xaa38c)[0x7f05b377a38c]\n",
      "[a2eb792b60ca:00602] [ 5] /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xa9369)[0x7f05b3779369]\n",
      "[a2eb792b60ca:00602] [ 6] /usr/lib/x86_64-linux-gnu/libstdc++.so.6(__gxx_personality_v0+0x2a1)[0x7f05b3779d21]\n",
      "[a2eb792b60ca:00602] [ 7] /usr/lib/x86_64-linux-gnu/libgcc_s.so.1(+0x10bef)[0x7f05b3683bef]\n",
      "[a2eb792b60ca:00602] [ 8] /usr/lib/x86_64-linux-gnu/libgcc_s.so.1(_Unwind_Resume+0x12a)[0x7f05b36845aa]\n",
      "[a2eb792b60ca:00602] [ 9] /usr/local/hugectr/lib/libhuge_ctr_shared.so(_ZN7HugeCTR32LocalizedSlotSparseEmbeddingHashIxfE19filter_keys_per_gpuEbmmm+0xb94)[0x7f05ca9c0144]\n",
      "[a2eb792b60ca:00602] [10] /usr/local/hugectr/lib/libhuge_ctr_shared.so(+0x5946a2)[0x7f05ca9c06a2]\n",
      "[a2eb792b60ca:00602] [11] /usr/lib/x86_64-linux-gnu/libgomp.so.1(GOMP_parallel+0x46)[0x7f05b36a08e6]\n",
      "[a2eb792b60ca:00602] [12] /usr/local/hugectr/lib/libhuge_ctr_shared.so(_ZN7HugeCTR32LocalizedSlotSparseEmbeddingHashIxfE7forwardEbi+0x5a)[0x7f05ca9c084a]\n",
      "[a2eb792b60ca:00602] [13] /usr/local/hugectr/lib/libhuge_ctr_shared.so(_ZN7HugeCTR5Model5trainEv+0x2a3)[0x7f05cadce253]\n",
      "[a2eb792b60ca:00602] [14] /usr/local/hugectr/lib/libhuge_ctr_shared.so(_ZN7HugeCTR5Model3fitEiiiiiNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE+0xd16)[0x7f05caddb976]\n",
      "[a2eb792b60ca:00602] [15] /usr/local/hugectr/lib/hugectr.so(+0xd0543)[0x7f05cd331543]\n",
      "[a2eb792b60ca:00602] [16] /usr/local/hugectr/lib/hugectr.so(+0xcc498)[0x7f05cd32d498]\n",
      "[a2eb792b60ca:00602] [17] python(PyCFunction_Call+0x59)[0x5f5db9]\n",
      "[a2eb792b60ca:00602] [18] python(_PyObject_MakeTpCall+0x29e)[0x5f698e]\n",
      "[a2eb792b60ca:00602] [19] python[0x50b4c7]\n",
      "[a2eb792b60ca:00602] [20] python(_PyEval_EvalFrameDefault+0x186a)[0x56cf2a]\n",
      "[a2eb792b60ca:00602] [21] python(_PyEval_EvalCodeWithName+0x26a)[0x56a0ba]\n",
      "[a2eb792b60ca:00602] [22] python(PyEval_EvalCode+0x27)[0x68d5b7]\n",
      "[a2eb792b60ca:00602] [23] python[0x67cd01]\n",
      "[a2eb792b60ca:00602] [24] python[0x67cd7f]\n",
      "[a2eb792b60ca:00602] [25] python[0x67ce21]\n",
      "[a2eb792b60ca:00602] [26] python(PyRun_SimpleFileExFlags+0x197)[0x67ef47]\n",
      "[a2eb792b60ca:00602] [27] python(Py_RunMain+0x212)[0x6b7242]\n",
      "[a2eb792b60ca:00602] [28] python(Py_BytesMain+0x2d)[0x6b75cd]\n",
      "[a2eb792b60ca:00602] [29] /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3)[0x7f05cd8f60b3]\n",
      "[a2eb792b60ca:00602] *** End of error message ***\n",
      "Aborted (core dumped)\n"
     ]
    }
   ],
   "source": [
    "!python model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce4926f",
   "metadata": {},
   "source": [
    "After training terminates, we can see that multiple `.model` files and folders are generated. We need to move them inside `1` folder under the `movielens_hugectr` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c20a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv *.model MODEL_DIR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
