{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "artificial-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-valuation",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will show how we do inference with our trained deep learning recommender model using Triton Inference Server. In this example, we deploy the NVTabular workflow and HugeCTR model with Triton Inference Server. We deploy them as an ensemble. For each request, Triton Inference Server will feed the input data through the NVTabular workflow and its output through the HugeCR model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-poison",
   "metadata": {},
   "source": [
    "As we went through in the previous notebook, [movielens-HugeCTR](https://github.com/NVIDIA/NVTabular/blob/main/examples/inference_triton/inference-HugeCTR/movielens-HugeCTR.ipynb), NVTabular provides a function to save the NVTabular workflow via `export_hugectr_ensemble`. This function does not only save NVTabular workflow, but also saves the trained HugeCTR model and ensemble model to be served to Triton IS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-guinea",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d26d48",
   "metadata": {},
   "source": [
    "We need to write a configuration file with the stored model weights and model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05308361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /model/models/ps.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile '/model/models/ps.json'\n",
    "\n",
    "{\n",
    "    \"supportlonglong\": true,\n",
    "    \"models\": [\n",
    "        {\n",
    "            \"model\": \"movielens\",\n",
    "            \"sparse_files\": [\"/model/models/movielens/1/0_sparse_1900.model\"],\n",
    "            \"dense_file\": \"/model/models/movielens/1/_dense_1900.model\",\n",
    "            \"network_file\": \"/model/models/movielens/1/movielens.json\",\n",
    "        }\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-application",
   "metadata": {},
   "source": [
    "Let's import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vietnamese-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as httpclient\n",
    "\n",
    "import cudf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-discharge",
   "metadata": {},
   "source": [
    "### Load Models on Triton Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-render",
   "metadata": {},
   "source": [
    "At this stage, you should launch the Triton Inference Server docker container with the following script:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-tunisia",
   "metadata": {},
   "source": [
    "```\n",
    "docker run -it --gpus=all -p 8000:8000 -p 8001:8001 -p 8002:8002 -v ${PWD}:/model nvcr.io/nvidia/merlin/merlin-inference:0.6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-registrar",
   "metadata": {},
   "source": [
    "After you started the container you can start triton server with the command below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-reply",
   "metadata": {},
   "source": [
    "```\n",
    "tritonserver --model-repository=<path_to_models> --backend-config=hugectr,ps=<path_to_models>/ps.json --model-control-mode=explicit\n",
    "```\n",
    "\n",
    "In some cases, the Triton HugeCTR file has to be copied to the model folder.\n",
    "```\n",
    "cp /usr/local/hugectr/backends/hugectr/libtriton_hugectr.so <path_to_models>/movielens/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-round",
   "metadata": {},
   "source": [
    "Note: The model-repository path is `/model/models/`. The models haven't been loaded, yet. We can request triton server to load the saved ensemble.  We initialize a triton client. The path for the json file is `/model/models/movielens/1/movielens.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "third-ordinance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "outside-classics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tritonhttpclient/__init__.py:31: DeprecationWarning: The package `tritonhttpclient` is deprecated and will be removed in a future version. Please use instead `tritonclient.http`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tritonhttpclient\n",
    "\n",
    "try:\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "final-ratio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.is_server_live()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "diagnostic-volunteer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '72'}>\n",
      "bytearray(b'[{\"name\":\"movielens\"},{\"name\":\"movielens_ens\"},{\"name\":\"movielens_nvt\"}]')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'movielens'}, {'name': 'movielens_ens'}, {'name': 'movielens_nvt'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.get_model_repository_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-lender",
   "metadata": {},
   "source": [
    "Let's load our models to Triton Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "genetic-clinton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/models/movielens_nvt/load, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'movielens_nvt'\n",
      "CPU times: user 0 ns, sys: 3.97 ms, total: 3.97 ms\n",
      "Wall time: 3.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "triton_client.load_model(model_name=\"movielens_nvt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adolescent-sydney",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/models/movielens/load, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'movielens'\n",
      "CPU times: user 2.49 ms, sys: 968 Âµs, total: 3.46 ms\n",
      "Wall time: 5.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "triton_client.load_model(model_name=\"movielens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-reading",
   "metadata": {},
   "source": [
    "Finally, we load our ensemble model `movielens_ens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "square-newman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/models/movielens_ens/load, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'movielens_ens'\n",
      "CPU times: user 0 ns, sys: 2.45 ms, total: 2.45 ms\n",
      "Wall time: 104 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "triton_client.load_model(model_name=\"movielens_ens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-millennium",
   "metadata": {},
   "source": [
    "Let's send a request to Inference Server and print out the response. Since in our example above we do not have continuous columns, below our only inputs are categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tamil-purse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          userId  movieId\n",
      "15347762   99476   104374\n",
      "16647840  107979     2634\n",
      "23915192  155372     1614\n",
      "10052313   65225     7153\n",
      "12214125   79161      500\n",
      "...          ...      ...\n",
      "17138306  111072     1625\n",
      "21326655  138575    81591\n",
      "5664631    36671     8861\n",
      "217658      1535   111759\n",
      "11842246   76766   109487\n",
      "\n",
      "[64 rows x 2 columns] \n",
      "\n"
     ]
    },
    {
     "ename": "InferenceServerException",
     "evalue": "[StatusCode.UNAVAILABLE] Socket closed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInferenceServerException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_985/1747825886.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# make the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mhttpclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceServerClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"localhost:8001\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predicted sigmoid result:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OUTPUT0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tritonclient/grpc/__init__.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, model_name, inputs, model_version, outputs, request_id, sequence_id, sequence_start, sequence_end, priority, timeout, client_timeout, headers, compression_algorithm)\u001b[0m\n\u001b[1;32m   1084\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrpc_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m             \u001b[0mraise_error_grpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrpc_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m     def async_infer(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tritonclient/grpc/__init__.py\u001b[0m in \u001b[0;36mraise_error_grpc\u001b[0;34m(rpc_error)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_error_grpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrpc_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mget_error_grpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrpc_error\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInferenceServerException\u001b[0m: [StatusCode.UNAVAILABLE] Socket closed"
     ]
    }
   ],
   "source": [
    "from tritonclient.utils import np_to_triton_dtype\n",
    "\n",
    "model_name = \"movielens_ens\"\n",
    "col_names = [\"userId\", \"movieId\"]\n",
    "# read in a batch of data to get transforms for\n",
    "batch = cudf.read_parquet(\"/model/data/valid.parquet\", num_rows=64)[col_names]\n",
    "print(batch, \"\\n\")\n",
    "\n",
    "# convert the batch to a triton inputs\n",
    "columns = [(col, batch[col]) for col in col_names]\n",
    "inputs = []\n",
    "\n",
    "col_dtypes = [np.int64, np.int64]\n",
    "for i, (name, col) in enumerate(columns):\n",
    "    d = col.values_host.astype(col_dtypes[i])\n",
    "    d = d.reshape(len(d), 1)\n",
    "    inputs.append(httpclient.InferInput(name, d.shape, np_to_triton_dtype(col_dtypes[i])))\n",
    "    inputs[i].set_data_from_numpy(d)\n",
    "# placeholder variables for the output\n",
    "outputs = []\n",
    "outputs.append(httpclient.InferRequestedOutput(\"OUTPUT0\"))\n",
    "# make the request\n",
    "with httpclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer(model_name, inputs, request_id=str(1), outputs=outputs)\n",
    "\n",
    "print(\"predicted sigmoid result:\\n\", response.as_numpy(\"OUTPUT0\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
