{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Preprocessing the Criteo Dataset\n",
    "We are interested to benchmark the NVTabular data loader and compare its performance to the TensorFlow \"native\" data loader based on tf.records. First, we need to preprocess the dataset with NVTabular to normalize continuous features and categorify categorical ones. Afterwards, we transform the parquet files into tf.records.<br><br>\n",
    "The input for this notebook is based on [optimize_criteo.ipynb](https://github.com/NVIDIA/NVTabular/blob/main/examples/optimize_criteo.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "# tools for data preproc/loading\n",
    "import torch\n",
    "import rmm\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import Normalize,  Categorify,  LogOp, FillMissing, Clip, get_embedding_sizes\n",
    "from nvtabular.loader.torch import TorchAsyncItr, DLDataLoader\n",
    "from nvtabular.utils import device_mem_size\n",
    "\n",
    "import multiprocessing as mp\n",
    "from itertools import repeat\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define multiple helper functions.<br><br>\n",
    "*preproces_criteo* defines a NVTabular workflow to preprocess the data. It fills missing values, clip them, apply the logarithm function. Finally, the continuous features are normalized and categorical features are categorify. For more details, take a look on the [Criteo example](https://github.com/NVIDIA/NVTabular/blob/main/examples/criteo-example.ipynb).<br><br>\n",
    "*transform_tfrecords* loads the NVTabular output as parquet file and transforms them into tf.records format. The function organize the different components, such as TFRecordWriter, seralizing the data and using multi-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Function\n",
    "\n",
    "def preproces_criteo():\n",
    "    fname = 'day_{}.parquet'\n",
    "    num_days = len([i for i in os.listdir(INPUT_DATA_DIR) if re.match(fname.format('[0-9]{1,2}'), i) is not None])\n",
    "    train_paths = [os.path.join(INPUT_DATA_DIR, fname.format(day)) for day in range(1)]\n",
    "    valid_paths = [os.path.join(INPUT_DATA_DIR, fname.format(day)) for day in [2]]\n",
    "    train_paths, valid_paths\n",
    "    \n",
    "    proc = nvt.Workflow(\n",
    "        cat_names=CATEGORICAL_COLUMNS,\n",
    "        cont_names=CONTINUOUS_COLUMNS,\n",
    "        label_name=LABEL_COLUMNS\n",
    "    )\n",
    "    \n",
    "    proc.add_cont_feature([FillMissing(), Clip(min_value=0), LogOp()])\n",
    "    proc.add_cont_preprocess(Normalize())\n",
    "    proc.add_cat_preprocess(Categorify(freq_threshold=15, out_path=OUTPUT_DATA_DIR))\n",
    "    \n",
    "    train_dataset = nvt.Dataset(train_paths, engine='parquet', part_mem_fraction=0.15)\n",
    "    valid_dataset = nvt.Dataset(valid_paths, engine='parquet', part_mem_fraction=0.15)\n",
    "    \n",
    "    os.system('rm -r ' + OUTPUT_DATA_DIR)\n",
    "    os.system('mkdir -p ' + output_train_dir)\n",
    "    os.system('mkdir -p ' + output_valid_dir)\n",
    "    \n",
    "    proc.apply(train_dataset, \n",
    "               shuffle=nvt.io.Shuffle.PER_PARTITION, \n",
    "               output_path=output_train_dir, \n",
    "               out_files_per_proc=20\n",
    "              )\n",
    "    \n",
    "    proc.apply(valid_dataset, \n",
    "               record_stats=False, \n",
    "               shuffle=nvt.io.Shuffle.PER_PARTITION, \n",
    "               output_path=output_valid_dir, \n",
    "               out_files_per_proc=20\n",
    "              )\n",
    "    \n",
    "    proc.save_stats(OUTPUT_DATA_DIR + '/stats_and_workflow')\n",
    "\n",
    "def transform_tfrecords(parquet_files, TFRECORDS):\n",
    "    os.system('rm -r ' + TFRECORDS)\n",
    "    write_dir = os.path.dirname(TFRECORDS)\n",
    "    if not os.path.exists(write_dir):\n",
    "        os.makedirs(write_dir)\n",
    "    file_idx, example_idx = 0, 0\n",
    "    writer = get_writer(write_dir, file_idx)\n",
    "\n",
    "    do_break = False\n",
    "    column_names = [CONTINUOUS_COLUMNS, CATEGORICAL_COLUMNS+[LABEL_COLUMNS[0]]]\n",
    "    with mp.Pool(8, pool_initializer, column_names) as pool:\n",
    "        fnames = glob.glob(parquet_files)\n",
    "        ds_iterator = nvt.Dataset(fnames, part_mem_fraction=0.1)\n",
    "        pbar = trange(BATCH_SIZE*STEPS)\n",
    "\n",
    "        for df in ds_iterator.to_iter():\n",
    "            data = []\n",
    "            for col_names in column_names:\n",
    "                if len(col_names) == 0:\n",
    "                    data.append(repeat(None))\n",
    "                else:\n",
    "                    data.append(df[col_names].to_pandas().values)\n",
    "            data = zip(*data)\n",
    "\n",
    "            record_map = pool.imap(build_and_serialize_example, data, chunksize=200)\n",
    "            for record in record_map:\n",
    "                writer.write(record)\n",
    "                example_idx += 1\n",
    "\n",
    "                if example_idx == EXAMPLES_PER_RECORD:\n",
    "                    writer.close()\n",
    "                    file_idx += 1\n",
    "                    writer = get_writer(write_dir, file_idx)\n",
    "                    example_idx = 0\n",
    "                pbar.update(1)\n",
    "                if pbar.n == BATCH_SIZE*STEPS:\n",
    "                    do_break = True\n",
    "                    break\n",
    "            if do_break:\n",
    "                del df\n",
    "                break\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "def pool_initializer(num_cols, cat_cols):\n",
    "    global numeric_columns\n",
    "    global categorical_columns\n",
    "    numeric_columns = num_cols\n",
    "    categorical_columns = cat_cols\n",
    "\n",
    "def build_and_serialize_example(data):\n",
    "    numeric_values, categorical_values = data\n",
    "    feature = {}\n",
    "    if numeric_values is not None:\n",
    "        feature.update({\n",
    "            col: tf.train.Feature(float_list=tf.train.FloatList(value=[val]))\n",
    "                for col, val in zip(numeric_columns, numeric_values)\n",
    "    })\n",
    "    if categorical_values is not None:\n",
    "        feature.update({\n",
    "            col: tf.train.Feature(int64_list=tf.train.Int64List(value=[val]))\n",
    "                for col, val in zip(categorical_columns, categorical_values)\n",
    "    })\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
    "\n",
    "def get_writer(write_dir, file_idx):\n",
    "    filename = str(file_idx).zfill(5) + '.tfrecords'\n",
    "    return tf.io.TFRecordWriter(os.path.join(write_dir, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the directory stucture. The base directory and input and output directories for .parquet and tf.records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2288"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define some information about where to get our data\n",
    "INPUT_DIR = '/raid/data/criteo/input/'\n",
    "OUTPUT_DIR = '/raid/data/criteo/'\n",
    "INPUT_DATA_DIR = INPUT_DIR\n",
    "OUTPUT_DATA_DIR = os.environ.get('OUTPUT_DATA_DIR', OUTPUT_DIR + 'output') # where we'll save our procesed data to\n",
    "TFRECORD_DIR = os.environ.get(\"TFRECORD_DIR\", OUTPUT_DIR + 'tfrecords')\n",
    "TFRECORDS_TRAIN = os.path.join(TFRECORD_DIR, 'train', '*.tfrecords')\n",
    "TFRECORDS_VALID = os.path.join(TFRECORD_DIR, 'valid', '*.tfrecords')\n",
    "\n",
    "output_train_dir = os.path.join(OUTPUT_DATA_DIR, 'train/')\n",
    "output_valid_dir = os.path.join(OUTPUT_DATA_DIR, 'valid/')\n",
    "tf_input_train_dir = output_train_dir + '*.parquet'\n",
    "tf_input_valid_dir = output_valid_dir + '*.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the data schema, which column names are continouos, categorical and label columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our dataset schema\n",
    "CONTINUOUS_COLUMNS = ['I' + str(x) for x in range(1,14)]\n",
    "CATEGORICAL_COLUMNS =  ['C' + str(x) for x in range(1,27)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "COLUMNS = CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS + LABEL_COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tf.records, we need to pre-define some hyperparameters. How many examples per tf.record file should be stored and what is the batch-size, we use for training. Unfortunately, we cannot change this afterwards.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = int(os.environ.get('BATCH_SIZE', 1024*64))\n",
    "EXAMPLES_PER_RECORD = 20000000\n",
    "# Max. number of steps per epoch (tf.records allows only full batches)\n",
    "STEPS = int(150000000/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute the NVTabular workflow to preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 26s, sys: 1min 1s, total: 2min 28s\n",
      "Wall time: 3min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "preproces_criteo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the parquet files to tf.records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5910fc9d0dad441ba7939a2a61d53b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=149946368.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transform_tfrecords(tf_input_train_dir, TFRECORDS_TRAIN)\n",
    "transform_tfrecords(tf_input_valid_dir, TFRECORDS_VALID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
