{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/conda/envs/rapids/lib/python3.7/site-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_NVVM=/usr/local/cuda/nvvm/lib64/libnvvm.so.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n",
      "/conda/envs/rapids/lib/python3.7/site-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_LIBDEVICE=/usr/local/cuda/nvvm/libdevice/.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# External Dependencies\n",
    "import cupy as cp\n",
    "import cudf\n",
    "import dask_cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "from dask.utils import parse_bytes\n",
    "from dask.delayed import delayed\n",
    "import rmm\n",
    "\n",
    "from nvtabular.utils import device_mem_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/raid/'\n",
    "PREFIX_in = 'nv' # org, dask, nv\n",
    "PREFIX_out = 'nv3' # nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:34697</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>429.50 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:34697' processes=4 threads=4, memory=429.50 GB>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deploy a Single-Machine Multi-GPU Cluster\n",
    "protocol = \"tcp\"             # \"tcp\" or \"ucx\"\n",
    "visible_devices = \"0,1,2,3\"  # Delect devices to place workers\n",
    "device_spill_frac = 0.9      # Spill GPU-Worker memory to host at this limit.\n",
    "                             # Reduce if spilling fails to prevent\n",
    "                             # device memory errors.\n",
    "cluster = None               # (Optional) Specify existing scheduler port\n",
    "capacity = device_mem_size(kind=\"total\")\n",
    "if cluster is None:\n",
    "    cluster = LocalCUDACluster(\n",
    "        protocol = protocol,\n",
    "        CUDA_VISIBLE_DEVICES = visible_devices,\n",
    "        local_directory = BASE_DIR + 'dask/',\n",
    "        device_memory_limit = capacity * device_spill_frac,\n",
    "    )\n",
    "\n",
    "# Create the distributed client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:35195': None,\n",
       " 'tcp://127.0.0.1:39241': None,\n",
       " 'tcp://127.0.0.1:41755': None,\n",
       " 'tcp://127.0.0.1:44467': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize RMM pool on ALL workers\n",
    "def _rmm_pool():\n",
    "    rmm.reinitialize(\n",
    "        pool_allocator=True,\n",
    "        initial_pool_size=None, # Use default size\n",
    "    )\n",
    "    \n",
    "client.run(_rmm_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvtabular as nvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\n",
    "    'hashtags', 'tweet_id', 'media', 'links', 'domains', 'tweet_type', 'language', \n",
    "    'a_user_id', 'a_is_verified', \n",
    "    'b_user_id', 'b_is_verified', \n",
    "    'b_follows_a', 'dt_dow'\n",
    "]\n",
    "\n",
    "CONTINUOUS_COLUMNS = [\n",
    "    'timestamp', \n",
    "    'a_follower_count', 'a_following_count', \n",
    "    'b_follower_count', 'b_following_count',\n",
    "    'hashtags_count_t', 'domains_count_t', 'links_count_t'\n",
    "]\n",
    "LABEL_COLUMNS = [\n",
    "    'reply', 'retweet', 'retweet_comment', 'like'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = nvt.Workflow(\n",
    "    cat_names=CATEGORICAL_COLUMNS,\n",
    "    cont_names=CONTINUOUS_COLUMNS,\n",
    "    label_name=LABEL_COLUMNS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.add_feature([\n",
    "    nvt.ops.LambdaOp(\n",
    "        op_name = 'change',\n",
    "        f = lambda col, gdf: (col>0).astype('int8'),\n",
    "        columns = LABEL_COLUMNS,\n",
    "        replace=True\n",
    "    ),\n",
    "    nvt.ops.TargetEncoding(\n",
    "        cat_groups = ['media', \n",
    "                      'tweet_type', \n",
    "                      'language', \n",
    "                      'a_user_id', \n",
    "                      'b_user_id', \n",
    "                      ['domains','language','b_follows_a','tweet_type','media','a_is_verified']],\n",
    "        cont_target = LABEL_COLUMNS,\n",
    "        kfold = 5,\n",
    "        p_smooth = 20\n",
    "    ),\n",
    "    nvt.ops.JoinGroupby(\n",
    "        columns = [\n",
    "            'media', \n",
    "            'tweet_type', \n",
    "            'language', \n",
    "            'a_user_id', \n",
    "            'b_user_id'\n",
    "        ]\n",
    "    ),\n",
    "    nvt.ops.LambdaOp(\n",
    "        op_name = 'a_ff_rate',\n",
    "        f = lambda col, gdf: gdf['a_following_count']/gdf['a_follower_count'],\n",
    "        columns = ['a_following_count'],\n",
    "        replace=False\n",
    "    ),\n",
    "    nvt.ops.LambdaOp(\n",
    "        op_name = 'b_ff_rate',\n",
    "        f = lambda col, gdf: gdf['b_following_count']/gdf['b_follower_count'],\n",
    "        columns = ['b_following_count'],\n",
    "        replace=False\n",
    "    ),\n",
    "    nvt.ops.LambdaOp(\n",
    "        op_name = 'to_datetime',\n",
    "        f = lambda col, gdf: cudf.to_datetime(col, unit='s'),\n",
    "        columns = ['timestamp'],\n",
    "        replace=False\n",
    "    ),\n",
    "    nvt.ops.LambdaOp(\n",
    "        op_name = 'to_hour',\n",
    "        f = lambda col, gdf: col.dt.hour,\n",
    "        columns = ['timestamp_to_datetime'],\n",
    "        replace=False\n",
    "    ),\n",
    "    nvt.ops.LambdaOp(\n",
    "        op_name = 'to_minute',\n",
    "        f = lambda col, gdf: col.dt.minute,\n",
    "        columns = ['timestamp_to_datetime'],\n",
    "        replace=False\n",
    "    ),\n",
    "    nvt.ops.LambdaOp(\n",
    "        op_name = 'to_second',\n",
    "        f = lambda col, gdf: col.dt.second,\n",
    "        columns = ['timestamp_to_datetime'],\n",
    "        replace=False\n",
    "    ),\n",
    "    nvt.ops.LambdaOp(\n",
    "        op_name = 'asfloat',\n",
    "        f = lambda col, gdf: col.astype('float32'),\n",
    "        columns = ['b_follower_count','b_following_count','language'],\n",
    "        replace=True\n",
    "    ),\n",
    "    nvt.ops.DifferenceLag(\n",
    "        'b_user_id', \n",
    "        columns=['b_follower_count','b_following_count','language'],\n",
    "        shift = 1\n",
    "    ),\n",
    "    nvt.ops.FillMissing(fill_val=0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = nvt.Dataset(BASE_DIR + PREFIX_in + '_train-1-train.parquet', engine='parquet', part_size=\"200MB\")\n",
    "\n",
    "# get a nvt dataset and convert to a dask dataframe\n",
    "#ddf = train_dataset.to_ddf()\n",
    "# partition the dask dataframe by userid, then sort by userid/timestamp\n",
    "#ddf = ddf.shuffle(\"b_user_id\").sort_values([\"b_user_id\", \"timestamp\"]).reset_index()\n",
    "# create a new nvtabular dataset on the partitioned/sorted values\n",
    "#train_dataset = nvt.Dataset(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = nvt.Dataset(BASE_DIR + PREFIX_in + '_train-1-valid.parquet', engine='parquet', part_size=\"200MB\")\n",
    "#\n",
    "# get a nvt dataset and convert to a dask dataframe\n",
    "#ddf = valid_dataset.to_ddf()\n",
    "# partition the dask dataframe by userid, then sort by userid/timestamp\n",
    "#ddf = ddf.shuffle(\"b_user_id\").sort_values([\"b_user_id\", \"timestamp\"]).reset_index()\n",
    "# create a new nvtabular dataset on the partitioned/sorted values\n",
    "#valid_dataset = nvt.Dataset(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -r $BASE_DIR/${PREFIX_out}_out_train/\n",
    "#!rm -r $BASE_DIR/${PREFIX_out}_nv_out_valid/\n",
    "\n",
    "proc.apply(train_dataset, record_stats=True, output_path=BASE_DIR + PREFIX_in + '_' + PREFIX_out + '_out_train/')\n",
    "proc.apply(valid_dataset, record_stats=False, output_path=BASE_DIR + PREFIX_in + '_' + PREFIX_out + '_out_valid/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_metadata\t part.23.parquet  part.39.parquet  part.54.parquet\r\n",
      "part.0.parquet\t part.24.parquet  part.4.parquet   part.55.parquet\r\n",
      "part.1.parquet\t part.25.parquet  part.40.parquet  part.56.parquet\r\n",
      "part.10.parquet  part.26.parquet  part.41.parquet  part.57.parquet\r\n",
      "part.11.parquet  part.27.parquet  part.42.parquet  part.58.parquet\r\n",
      "part.12.parquet  part.28.parquet  part.43.parquet  part.59.parquet\r\n",
      "part.13.parquet  part.29.parquet  part.44.parquet  part.6.parquet\r\n",
      "part.14.parquet  part.3.parquet   part.45.parquet  part.60.parquet\r\n",
      "part.15.parquet  part.30.parquet  part.46.parquet  part.61.parquet\r\n",
      "part.16.parquet  part.31.parquet  part.47.parquet  part.62.parquet\r\n",
      "part.17.parquet  part.32.parquet  part.48.parquet  part.63.parquet\r\n",
      "part.18.parquet  part.33.parquet  part.49.parquet  part.64.parquet\r\n",
      "part.19.parquet  part.34.parquet  part.5.parquet   part.7.parquet\r\n",
      "part.2.parquet\t part.35.parquet  part.50.parquet  part.8.parquet\r\n",
      "part.20.parquet  part.36.parquet  part.51.parquet  part.9.parquet\r\n",
      "part.21.parquet  part.37.parquet  part.52.parquet\r\n",
      "part.22.parquet  part.38.parquet  part.53.parquet\r\n"
     ]
    }
   ],
   "source": [
    "#!ls $BASE_DIR/nv_out_train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_metadata\t part.14.parquet  part.20.parquet  part.5.parquet\r\n",
      "part.0.parquet\t part.15.parquet  part.21.parquet  part.6.parquet\r\n",
      "part.1.parquet\t part.16.parquet  part.22.parquet  part.7.parquet\r\n",
      "part.10.parquet  part.17.parquet  part.23.parquet  part.8.parquet\r\n",
      "part.11.parquet  part.18.parquet  part.24.parquet  part.9.parquet\r\n",
      "part.12.parquet  part.19.parquet  part.3.parquet\r\n",
      "part.13.parquet  part.2.parquet   part.4.parquet\r\n"
     ]
    }
   ],
   "source": [
    "#!ls $BASE_DIR/nv_out_valid/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "rapids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
