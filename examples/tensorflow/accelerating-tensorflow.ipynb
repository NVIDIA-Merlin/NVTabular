{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Tabular Deep Learning Models with Keras on GPU\n",
    "Deep learning has revolutionized the fields of computer vision (CV) and natural language processing (NLP) in the last few years, providing a fast and general framework for solving a host of difficult problems with unprecedented accuracy. Part and parcel of this revolution has been the development of APIs like [Keras](https://www.tensorflow.org/api_docs/python/tf/keras) for NVIDIA GPUs, allowing practitioners to quickly iterate on new and interesting ideas and receive feedback on their efficacy in shorter and shorter intervals.\n",
    "\n",
    "One class of problem which has remained largely immune to this revolution, however, is the class involving tabular data. Part of this difficulty is that, unlike CV or NLP, where different datasets are underlied by similar phenomena and therefore can be solved with similar mechanisms, \"tabular datasets\" span a vast array of phenomena, semantic meanings, and problem statements, from product and video recommendation to particle discovery and loan default prediction. This diversity makes universally useful components difficult to find or even define, and is only exacerbated by the notorious lack of standard, industrial-scale benchmark datasets in the tabular space. Accordingly, deep learning models are frequently bested by their machine learning analogues on these important tasks.\n",
    "\n",
    "Yet this diversity is also what makes tools like Keras all the more valuable. Architecture components can be quickly swapped in and out for different tasks like the implementation details they are, and new components can be built and tested with ease. Importantly, domain experts can interact with models at a high level and build their expertise into their design, without having to spend their time becoming Python programming wizrds. However, the other key ingredient of fast feedback, enabled by GPU acceleration and correlating with cost-efficiency for production pipelines, is lacking in most out-of-the-box APIs. In this example, we will walk through some recent advancements made by NVIDIA's [NVTabular](https://github.com/nvidia/nvtabular) library that can alleviate existing bottlenecks and bring to bear the full power of GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: include parquet -> tfrecord conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3105), started 0:17:49 ago. (Use '!kill 3105' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2b6b2f34fb09de05\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2b6b2f34fb09de05\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "! mkdir -p logs/native logs/accelerated\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /home/docker/tensorflow/logs --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use these and fix other paths\n",
    "DATA_DIR = \"/data\"\n",
    "LOG_DIR = \"logs/\"\n",
    "ACCELERATED = False\n",
    "\n",
    "# TODO: reimplement the preproc from criteo-example here?\n",
    "# Alternatively, make criteo its own folder, and split preproc\n",
    "# and training into separate notebooks, then execute the\n",
    "# preproc notebook from here?\n",
    "NUMERIC_FEATURE_NAMES = [f\"I{i}\" for i in range(1, 14)]\n",
    "CATEGORICAL_FEATURE_NAMES = [f\"C{i}\" for i in range(1, 27)]\n",
    "CATEGORY_COUNTS = [\n",
    "    7599500, 33521, 17022, 7339, 20046, 3, 7068, 1377, 63, 5345303,\n",
    "    561810, 242827, 11, 2209, 10616, 100, 4, 968, 14, 7838519,\n",
    "    2580502, 6878028, 298771, 11951, 97, 35\n",
    "]\n",
    "LABEL_NAME = \"label\"\n",
    "\n",
    "# optimization params\n",
    "BATCH_SIZE = 65536\n",
    "STEPS = 1000\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# architecture params\n",
    "EMBEDDING_DIM = 8\n",
    "TOP_MLP_HIDDEN_DIMS = [1024, 512, 256]\n",
    "BOTTOM_MLP_HIDDEN_DIMS = [1024, 1024, 512, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/conda/lib/python3.7/site-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_NVVM=/usr/local/cuda/nvvm/lib64/libnvvm.so.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n",
      "/conda/lib/python3.7/site-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_LIBDEVICE=/usr/local/cuda/nvvm/libdevice/.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import layers\n",
    "import os\n",
    "from functools import reduce\n",
    "from itertools import filterfalse\n",
    "os.environ[\"TF_MEMORY_ALLOCATION\"] = \"0.5\"\n",
    "\n",
    "from nvtabular.tf_dataloader import KerasSequenceDataset # for data loading\n",
    "from nvtabular import Workflow # for online preproc\n",
    "from nvtabular.ops import HashBucket # for doing hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns(accelerated=False):\n",
    "    numeric_columns = [tf.feature_column.numeric_column(name, (1,)) for name in NUMERIC_FEATURE_NAMES]\n",
    "    categorical_columns = []\n",
    "    for feature_name, count in zip(CATEGORICAL_FEATURE_NAME, CATEGORY_COUNTS):\n",
    "        num_buckets = int(0.75*count)\n",
    "\n",
    "        # when using nvtabular workflow, hashing gets done before tf, so\n",
    "        # we can just use an identity column on the number of hash buckets\n",
    "        if not accelerated:\n",
    "            column = tf.feature_column.categorical_column_with_hash_bucket(name, num_buckets, dtype=tf.int64)\n",
    "        else:\n",
    "            column = tf.feature_column.categorical_column_with_identity(name, num_buckets)\n",
    "        categorical_columns.append(column)\n",
    "    return numeric_columns, categorical_columns\n",
    "\n",
    "def get_dtype(column):\n",
    "    return getattr(column, 'dtype', tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(columns, accelerated=False):\n",
    "    # make a tfrecord features dataset\n",
    "    if not accelerated:\n",
    "        # feature spec tells us how to parse tfrecords\n",
    "        # using FixedLenFeatures keeps from using sparse machinery,\n",
    "        # but obviously wouldn't extend to multi-hot categoricals\n",
    "        feature_spec = {column.name: tf.io.FixedLenFeature((1,), get_dtype(column)) for column in columns}\n",
    "        feature_spec[LABEL_NAME] = tf.io.FixedLenFeature((1,), tf.int64)\n",
    "\n",
    "        return tf.data.experimental.make_batched_features_dataset(\n",
    "            \"/data/tfrecords/train/*.tfrecords\",\n",
    "            BATCH_SIZE,\n",
    "            feature_spec,\n",
    "            label_key=LABEL_NAME,\n",
    "            num_epochs=1,\n",
    "            shuffle=True,\n",
    "            shuffle_buffer_size=4*BATCH_SIZE,\n",
    "            )\n",
    "\n",
    "    # make an nvtabular KerasSequenceDataset and add\n",
    "    # a hash bucketing workflow for online preproc\n",
    "    else:\n",
    "        dataset = KerasSequenceDataset(\n",
    "            \"/data/stable/train/*.parquet\",\n",
    "            columns,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            label_name=LABEL_NAME,\n",
    "            shuffle=True,\n",
    "            buffer_size=0.2,\n",
    "            dataset_size=STEPS*BATCH_SIZE\n",
    "          )\n",
    "        workflow = Workflow(\n",
    "            cat_names=CATEGORICAL_FEATURE_NAMES,\n",
    "            cont_names=NUMERIC_FEATURE_NAMES,\n",
    "            label_name=[LABEL_NAME]\n",
    "        )\n",
    "        num_buckets = {\n",
    "            col.name: col.num_buckets for col in columns\n",
    "                if col.name in CATEGORICAL_FEATURE_NAMES\n",
    "        }\n",
    "        workflow.add_cat_preprocess(HashBucket(num_buckets))\n",
    "        workflow.finalize()\n",
    "        dataset.map(workflow)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_inputs(inputs, numeric_columns, embedding_columns, accelerated=False):\n",
    "    get_name = lambda x: x.name.split(\":\")[0]\n",
    "    categorical_inputs = {get_name(x): x for x in inputs if get_name(x) in CATEGORICAL_FEATURE_NAMES}\n",
    "    numeric_inputs = {get_name(x): x for x in inputs if get_name(x) in NUMERIC_FEATURE_NAMES}\n",
    "\n",
    "    # use vanilla Keras DenseFeatures layer\n",
    "    if not accelerated:\n",
    "        fm_x = tf.keras.layers.DenseFeatures(embedding_columns)(categorical_inputs)\n",
    "        fm_x = tf.keras.layers.Reshape((len(embedding_columns), EMBEDDING_DIM))(fm_x)\n",
    "        dense_x = tf.keras.layers.DenseFeatures(numeric_columns)(numeric_inputs)\n",
    "    # don't need to do feature transformation, so we can use custom,\n",
    "    # better optimized embedding layer\n",
    "    else:\n",
    "        fm_x = layers.ScalarDenseFeatures(embedding_columns, aggregation='stack')(categorical_inputs)\n",
    "        dense_x = layers.ScalarDenseFeatures(numeric_columns, aggregation='concat')(numeric_inputs)\n",
    "    return fm_x, dense_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_architecture():\n",
    "    # same either way: as it should be\n",
    "    dense_input = tf.keras.Input(name=\"dense_x\", shape=(len(numeric_columns),), dtype=tf.float32)\n",
    "    fm_input = tf.keras.Input(name=\"fm_x\", shape=(len(categorical_columns), EMBEDDING_DIM), dtype=tf.float32)\n",
    "\n",
    "    dense_x = dense_input\n",
    "    for dim in TOP_MLP_HIDDEN_DIMS:\n",
    "        dense_x = tf.keras.layers.Dense(dim, activation=\"relu\")(dense_x)\n",
    "    dense_x = tf.keras.layers.Dense(EMBEDDING_DIM, activation=\"linear\")(dense_x)\n",
    "    dense_x = tf.keras.layers.Reshape((1, EMBEDDING_DIM))(dense_x)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate(axis=1)([fm_input, dense_x])\n",
    "    x = layers.DotProductInteraction()(x)\n",
    "    for dim in BOTTOM_MLP_HIDDEN_DIMS:\n",
    "        x = tf.keras.layers.Dense(dim, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=[dense_input, fm_input], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_counts(model):\n",
    "    mult = lambda x, y: x*y\n",
    "    trainable_weights = model.trainable_weights\n",
    "    check_name = lambda x: (\n",
    "        x.name.split(':')[0].split('/')[-1] == \"embedding_weights\")\n",
    "\n",
    "    trainable_embedding_weights = filter(check_name, trainable_weights)\n",
    "    trainable_network_weights = filterfalse(check_name, trainable_weights)\n",
    "\n",
    "    num_embedding_params = sum(\n",
    "        [reduce(mult, x.shape) for x in trainable_embedding_weights]\n",
    "    )\n",
    "    num_network_params = sum(\n",
    "        [reduce(mult, x.shape) for x in trainable_network_weights]\n",
    "    )\n",
    "    print(\"Embedding parameter count: {}\".format(num_embedding_params))\n",
    "    print(\"Non-embedding parameter count: {}\".format(num_network_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding parameter count: 188746160\n",
      "Non-embedding parameter count: 2738953\n",
      "Train for 1000 steps\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layers with arguments in `__init__` must override `get_config`.\n",
      "1000/1000 [==============================] - 439s 439ms/step - loss: 0.1320 - auroc: 0.7467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0b2c6d02d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_columns, categorical_columns = get_columns(accelerated=ACCELERATED)\n",
    "embedding_columns = [tf.feature_column.embedding_column(column, EMBEDDING_DIM) for column in categorical_columns]\n",
    "columns = numeric_columns + categorical_columns\n",
    "\n",
    "train_dataset = make_dataset(columns, accelerated=ACCELERATED)\n",
    "\n",
    "make_input = lambda column: tf.keras.Input(name=column.name, shape=(1,), dtype=getattr(column, \"dtype\", tf.int64))\n",
    "inputs = list(map(make_input, columns))\n",
    "\n",
    "fm_x, dense_x = embed_inputs(inputs, numeric_columns, embedding_columns, accelerated=ACCELERATED)\n",
    "x = get_architecture()([dense_x, fm_x])\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "model.compile(\n",
    "    optimizer,\n",
    "    \"binary_crossentropy\",\n",
    "    metrics=[tf.keras.metrics.AUC(curve=\"ROC\", name=\"auroc\")]\n",
    ")\n",
    "print_param_counts(model)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        os.path.join(LOG_DIR, \"accelerated\" if ACCELERATED else \"native\"),\n",
    "        update_freq=10,\n",
    "        profile_batch=100)\n",
    "]\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1,\n",
    "    steps_per_epoch=STEPS,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
