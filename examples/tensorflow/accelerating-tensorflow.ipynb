{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Tabular Deep Learning Models with Keras on GPU\n",
    "Deep learning has revolutionized the fields of computer vision (CV) and natural language processing (NLP) in the last few years, providing a fast and general framework for solving a host of difficult problems with unprecedented accuracy. Part and parcel of this revolution has been the development of APIs like [Keras](https://www.tensorflow.org/api_docs/python/tf/keras) for NVIDIA GPUs, allowing practitioners to quickly iterate on new and interesting ideas and receive feedback on their efficacy in shorter and shorter intervals.\n",
    "\n",
    "One class of problem which has remained largely immune to this revolution, however, is the class involving tabular data. Part of this difficulty is that, unlike CV or NLP, where different datasets are underlied by similar phenomena and therefore can be solved with similar mechanisms, \"tabular datasets\" span a vast array of phenomena, semantic meanings, and problem statements, from product and video recommendation to particle discovery and loan default prediction. This diversity makes universally useful components difficult to find or even define, and is only exacerbated by the notorious lack of standard, industrial-scale benchmark datasets in the tabular space. As a result, deep learning models are frequently bested by their machine learning analogues on these important tasks, particularly on smaller scale datasets.\n",
    "\n",
    "Yet this diversity is also what makes tools like Keras all the more valuable. Architecture components can be quickly swapped in and out for different tasks like the implementation details they are, and new components can be built and tested with ease. Importantly, domain experts can interact with models at a high level and build *a priori* knowledge into model architectures, without having to spend their time becoming Python programming wizrds.\n",
    "\n",
    "However, most out-of-the-box APIs suffer from a lack of acceleration that reduces the rate at which new components can be tested and makes production deployment of deep learning systems cost-prohibitive. In this example, we will walk through some recent advancements made by NVIDIA's [NVTabular](https://github.com/nvidia/nvtabular) data loading library that can alleviate existing bottlenecks and bring to bear the full power of GPU acceleration.\n",
    "\n",
    "#### What to Keep an Eye Out For\n",
    "The point of this walkthrough will be to show how common components of existing TensorFlow tabular-learning pipelines can be drop-in replaced by NVTabular components for cheap-as-free acceleration with minimal overhead. To do this, we'll start by examining a pipeline for fitting the [DLRM](https://arxiv.org/abs/1906.00091) architecture on the [Criteo Terabyte Dataset](https://labs.criteo.com/2013/12/download-terabyte-click-logs/) using Keras/TensorFlow's native tools on both on CPU and GPU, and discuss why the acceleration we observe on GPU is not particularly impressive. Then we'll examine what an identical pipeline would look like using NVTabular and why it overcomes those bottlenecks.\n",
    "\n",
    "Since the Criteo Terabyte Dataset is large, and you and I both have better things to do than sit around for hours waiting to train a model we have no intention of ever using, I'll restrict the training to 1000 steps in order to illustrate the similarities in convergence and the expected acceleration. Of course, there may well exist alternative choices of architectures and hyperparameters that will lead to better or faster convergence, but I trust that you, clever data scientist that you are, are more than capable of finding these yourself should you wish. I intend only to demonstrate how NVTabular can help you achieve that convergence more quickly, in the hopes that you will find it easy to apply the same methods to the dataset that really matters: your own.\n",
    "\n",
    "I will assume at least some familiarity with the relevant tabular deep learning methods (in particular what I mean by \"tabular data\" and how it is distinct from, say, image data; continuous vs. categorical variables; learned categorical embeddings; and online vs. offline preprocessing) and a passing familiarity with TensorFlow and Keras. If you are green or rusty on any of this points, it won't make this discussion illegible, but I'll put links in the relevant places just in case.\n",
    "\n",
    "The structure will be building, step-by-step, the necessary functions that a dataset-agnostic pipeline might need in order to train a model in Keras. In each function, we'll include an `accelerated` kwarg that will be used to show the difference between what such a function might look like in native TensorFlow vs. using NVTabular. Let's start here by doing our imports and defining some hyperparameters for training (which won't change from one implementation to the next)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/conda/lib/python3.7/site-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_NVVM=/usr/local/cuda/nvvm/lib64/libnvvm.so.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n",
      "/conda/lib/python3.7/site-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_LIBDEVICE=/usr/local/cuda/nvvm/libdevice/.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n",
      "/nvtabular/nvtabular/tf_dataloader.py:36: UserWarning: Tensorflow 2.2.0 dlpack integration has known memory leak issues\n",
      "  warnings.warn(\"Tensorflow 2.2.0 dlpack integration has known memory leak issues\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from itertools import filterfalse\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "# this is a good habit to get in now: TensorFlow's default behavior\n",
    "# is to claim all of the GPU memory that it can for itself. This\n",
    "# is a problem when it needs to run alongside another GPU library\n",
    "# like NVTabular. To get around this, NVTabular will configure\n",
    "# TensorFlow to use this fraction of available GPU memory up front.\n",
    "# Make sure, however, that you do this before you do anything\n",
    "# with TensorFlow: as soon as it's initialized, that memory is gone\n",
    "# for good\n",
    "os.environ[\"TF_MEMORY_ALLOCATION\"] = \"0.5\"\n",
    "import nvtabular as nvt\n",
    "from nvtabular.tf_dataloader import KerasSequenceDataset\n",
    "\n",
    "# import some local, custom tools for training. There's no\n",
    "# real magic going on here, the biggest thing is the\n",
    "# `ScalarDenseFeatures` layer in `layers` that we'll discuss\n",
    "# in a bit\n",
    "import layers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.environ.get(\"DATA_DIR\", \"/data\")\n",
    "TFRECORD_DIR = os.environ.get(\"TFRECORD_DIR\", \"/tfrecords\")\n",
    "LOG_DIR = os.environ.get(\"LOG_DIR\", \"logs/\")\n",
    "\n",
    "TFRECORDS = os.path.join(TFRECORD_DIR, \"train\", \"*.tfrecords\")\n",
    "PARQUETS = os.path.join(DATA_DIR, \"train\", \"*.parquet\")\n",
    "\n",
    "# TODO: reimplement the preproc from criteo-example here?\n",
    "# Alternatively, make criteo its own folder, and split preproc\n",
    "# and training into separate notebooks, then execute the\n",
    "# preproc notebook from here?\n",
    "NUMERIC_FEATURE_NAMES = [f\"I{i}\" for i in range(1, 14)]\n",
    "CATEGORICAL_FEATURE_NAMES = [f\"C{i}\" for i in range(1, 27)]\n",
    "CATEGORY_COUNTS = [\n",
    "    7599500, 33521, 17022, 7339, 20046, 3, 7068, 1377, 63, 5345303,\n",
    "    561810, 242827, 11, 2209, 10616, 100, 4, 968, 14, 7838519,\n",
    "    2580502, 6878028, 298771, 11951, 97, 35\n",
    "]\n",
    "LABEL_NAME = \"label\"\n",
    "\n",
    "# optimization params\n",
    "BATCH_SIZE = 65536\n",
    "STEPS = 1000\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# architecture params\n",
    "EMBEDDING_DIM = 8\n",
    "TOP_MLP_HIDDEN_DIMS = [1024, 512, 256]\n",
    "BOTTOM_MLP_HIDDEN_DIMS = [1024, 1024, 512, 256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Does Your Data Look Like\n",
    "As we discussed before, \"tabular data\" is an umbrella term referring to data collected from a vast array of problems and phenomena. Perhaps Bob's dataset has 192 features, 54 of which are continuous variables recorded as 32 bit floating point numbers, and the remainder of which are categorical variables which he has encoded as strings. Alice, on the other hand, may have a dataset consisting of 3271 features, most of which are continuous, but a handful of which are integer IDs which can take on one of millions of possible values. We can't expect the same model to be able to handle this kind of variety unless we give it some description of what sorts of inputs to expect.\n",
    "\n",
    "Moreover, the format in which the data gets read from disk will rarely be the one the model finds useful. Bob's string categories will be of no use to a neural network which lives in the world of continuous functions of real numbers; they will need to be converted to integer lookup table indices before being ingested. For certain types of these **transformations**, Bob may want to do this conversion once, up front, before training begins, and then be done with it. However, this may not always be possible. Bob may wish to hyperparameter search over the parameters of such a transformation (if, for instance, he is using a hash function to map to indices and wants to play with the number of buckets to use). Or perhaps he wants to retain the pre-transformed values, but finds the cost of storing an entire second dataset of the transformed values prohibitive. In this case, he'll need to perform the transformations *online*, between when the data is read from disk and when it gets fed to the network.\n",
    "\n",
    "Finally, in the case of categorical variables, these lookup indices will need to, well, *look up* an embedding vector that finally puts us in the continuous space our network prefers. Therefore, we also need to define how large of an embedding vector we want to use for a given feature.\n",
    "\n",
    "TensorFlow provides a convenient module to record this information about the names of features to expect, their type (categorical or numeric), their data type, common transformations to perform on them, and the size of embedding table to use in the case of categorical variables: the [`feature_column` module](https://www.tensorflow.org/tutorials/structured_data/feature_columns). (Note: as of [TensorFlow 2.3](https://github.com/tensorflow/tensorflow/releases/tag/v2.3.0-rc0) these are being deprecated and replaced with Keras layers with similar functionality. Most of the arguments made here will still apply, the code will just look a bit different.) These objects provide both stateless representations of feature information, as well as the code that performs the transformations and embeddings at train time.\n",
    "\n",
    "While `feature_column`s are a handy and robust representation format, their transformation and embedding implementations are poorly suited for GPUs. We'll see how this looks in terms of TensorFlow profile traces later, but the upshot comes down to two basic points:\n",
    "- Many of the transformations involve ops that either don't have a GPU kernel, or have one which is unoptimized. The involvement of ops without GPU kernels means that you're spending a lot of your train step moving data around to the device which can run the current op. Many of the ops that *do* have a GPU kernel are small and don't involve much math, which drowns the math-hungry parallel computing model of GPUs in kernel launch overhead.\n",
    "- The embeddings use sparse tensor machinery that is unoptimized on GPUs and is unnecessary for one-hot categoricals, the only type we'll focus on here. This is a good time to mention that the techniques we'll cover today *do not generalize to multi-hot categorical data*, which isn't currently supported by NVTabular. However, there is active work to support this being done and we hope to have it seamlessly integrated in the near future.\n",
    "\n",
    "As we'll see later, one difficulty in addressing the second issue is that the same Keras layer which performs the embeddings *also* performs the transformations, so even if you know that all your categoricals are one-hot and want to build an accelerated embedding layer that leverages this information, you would be out of luck on a layer which can just perform whatever transformations you might need. One way to get around this is to move your transformations to NVTabular, which will do them all on the GPU at data-loading time, so that all Keras needs to handle is the embedding using a layer like the `ScalarDenseFeatures` layer I've build in `layers.py`.\n",
    "\n",
    "Let's see what this difference in feature columns looks like for a native pipeline vs. an NVTabular accelerated pipeline. In order to show what the difference in online preprocessing looks like, I'll hash all our categorical features to, say, 75% of their original size. This isn't standard practice for Criteo, and will probably affect learning, but since I'm sure you do hashing or at least something like it somewhere in your training pipeline, I think using it will help clarify where and how you can use NVTabular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_columns(accelerated=False):\n",
    "    # nothing changes in our numeric columns\n",
    "    numeric_columns = [tf.feature_column.numeric_column(name, (1,)) for name in NUMERIC_FEATURE_NAMES]\n",
    "\n",
    "    categorical_columns = []\n",
    "    for feature_name, count in zip(CATEGORICAL_FEATURE_NAMES, CATEGORY_COUNTS):\n",
    "        # when using nvtabular workflow, hashing gets done before tf, so\n",
    "        # we can just use an identity column on the number of hash buckets\n",
    "        if not accelerated:\n",
    "            column_type = tf.feature_column.categorical_column_with_hash_bucket\n",
    "            kwargs = {'dtype': tf.int64}\n",
    "        else:\n",
    "            column_type = tf.feature_column.categorical_column_with_identity\n",
    "            kwargs = {}\n",
    "\n",
    "        num_buckets = int(0.75*count) # arbitrary hashing reduction\n",
    "        column = column_type(feature_name, num_buckets, **kwargs)\n",
    "        categorical_columns.append(column)\n",
    "\n",
    "    embedding_columns = [tf.feature_column.embedding_column(column, EMBEDDING_DIM) for column in categorical_columns]\n",
    "    return numeric_columns + embedding_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the difference here just comes down to using a different type of `feature_column`, and illustrates the first critical point: if you're using NVTabular with TensorFlow `feature_column`s, you should *only* be using `categorical_column_with_identity` for categorical features, since any other transformation should be handled in NVTabular, on the GPU. The **cardinality** of the category doesn't change in this case, but at this point the `feature_column`s should be purely symbolic descriptors, and we'll use more optimized code to handle the work they used to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Data By Any Other Format: TFRecords and Tabular Representation\n",
    "By running the Criteo preprocessing example above, we generated a dataset in the parquet data format. Why Parquet? Well, besides the fact that NVTabular can read parquet files exceptionally quickly, parquet is a widely used tabular data format that can be read by libraries like Pandas or CuDF to quickly search, filter, and manipulate data using high level abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>...</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.262324</td>\n",
       "      <td>-0.382217</td>\n",
       "      <td>0.323255</td>\n",
       "      <td>-1.058641</td>\n",
       "      <td>-0.896547</td>\n",
       "      <td>-0.488617</td>\n",
       "      <td>-0.212545</td>\n",
       "      <td>1.569190</td>\n",
       "      <td>-0.247232</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>3688627</td>\n",
       "      <td>978346</td>\n",
       "      <td>2295585</td>\n",
       "      <td>205535</td>\n",
       "      <td>1588</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.386949</td>\n",
       "      <td>-0.371194</td>\n",
       "      <td>0.323255</td>\n",
       "      <td>-0.216371</td>\n",
       "      <td>-0.896547</td>\n",
       "      <td>1.302719</td>\n",
       "      <td>-0.212545</td>\n",
       "      <td>0.214639</td>\n",
       "      <td>0.082582</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>5958569</td>\n",
       "      <td>1597651</td>\n",
       "      <td>3349225</td>\n",
       "      <td>0</td>\n",
       "      <td>6710</td>\n",
       "      <td>61</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.311604</td>\n",
       "      <td>0.896629</td>\n",
       "      <td>-0.156177</td>\n",
       "      <td>-0.216371</td>\n",
       "      <td>-0.112056</td>\n",
       "      <td>0.930983</td>\n",
       "      <td>2.320010</td>\n",
       "      <td>1.156740</td>\n",
       "      <td>0.082582</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>3464388</td>\n",
       "      <td>1512195</td>\n",
       "      <td>327622</td>\n",
       "      <td>125816</td>\n",
       "      <td>10156</td>\n",
       "      <td>63</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.941259</td>\n",
       "      <td>1.079725</td>\n",
       "      <td>1.906012</td>\n",
       "      <td>0.584996</td>\n",
       "      <td>-0.896547</td>\n",
       "      <td>-0.488617</td>\n",
       "      <td>-0.212545</td>\n",
       "      <td>-0.112180</td>\n",
       "      <td>1.469257</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>228</td>\n",
       "      <td>9</td>\n",
       "      <td>18715</td>\n",
       "      <td>1630530</td>\n",
       "      <td>2939009</td>\n",
       "      <td>104676</td>\n",
       "      <td>10782</td>\n",
       "      <td>45</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.057393</td>\n",
       "      <td>0.068799</td>\n",
       "      <td>0.323255</td>\n",
       "      <td>1.574801</td>\n",
       "      <td>-0.896547</td>\n",
       "      <td>-0.488617</td>\n",
       "      <td>-0.212545</td>\n",
       "      <td>1.148680</td>\n",
       "      <td>-0.481238</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>228</td>\n",
       "      <td>2</td>\n",
       "      <td>4575297</td>\n",
       "      <td>231168</td>\n",
       "      <td>4013246</td>\n",
       "      <td>98040</td>\n",
       "      <td>10285</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.560650</td>\n",
       "      <td>-2.133078</td>\n",
       "      <td>0.639048</td>\n",
       "      <td>0.068561</td>\n",
       "      <td>-0.896547</td>\n",
       "      <td>2.350584</td>\n",
       "      <td>-0.212545</td>\n",
       "      <td>0.443555</td>\n",
       "      <td>0.412395</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>5</td>\n",
       "      <td>7783437</td>\n",
       "      <td>2579387</td>\n",
       "      <td>104363</td>\n",
       "      <td>0</td>\n",
       "      <td>1613</td>\n",
       "      <td>45</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.560650</td>\n",
       "      <td>-2.133078</td>\n",
       "      <td>-1.187271</td>\n",
       "      <td>-1.058641</td>\n",
       "      <td>-0.896547</td>\n",
       "      <td>0.407051</td>\n",
       "      <td>-0.212545</td>\n",
       "      <td>-1.092636</td>\n",
       "      <td>-1.374871</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>4742561</td>\n",
       "      <td>1545552</td>\n",
       "      <td>372202</td>\n",
       "      <td>283422</td>\n",
       "      <td>6718</td>\n",
       "      <td>61</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.837804</td>\n",
       "      <td>-0.371194</td>\n",
       "      <td>-1.187271</td>\n",
       "      <td>0.780321</td>\n",
       "      <td>-0.401588</td>\n",
       "      <td>-0.488617</td>\n",
       "      <td>-0.212545</td>\n",
       "      <td>-0.574641</td>\n",
       "      <td>-0.811051</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>6607477</td>\n",
       "      <td>1674069</td>\n",
       "      <td>6842640</td>\n",
       "      <td>0</td>\n",
       "      <td>9892</td>\n",
       "      <td>63</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>0</td>\n",
       "      <td>0.386949</td>\n",
       "      <td>1.333358</td>\n",
       "      <td>1.289596</td>\n",
       "      <td>1.875701</td>\n",
       "      <td>1.555578</td>\n",
       "      <td>-0.488617</td>\n",
       "      <td>-0.212545</td>\n",
       "      <td>-0.765817</td>\n",
       "      <td>0.771790</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>216</td>\n",
       "      <td>5</td>\n",
       "      <td>1719047</td>\n",
       "      <td>946656</td>\n",
       "      <td>1363097</td>\n",
       "      <td>165612</td>\n",
       "      <td>7440</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>0</td>\n",
       "      <td>1.098833</td>\n",
       "      <td>0.543444</td>\n",
       "      <td>-0.156177</td>\n",
       "      <td>1.361091</td>\n",
       "      <td>-0.112056</td>\n",
       "      <td>-0.488617</td>\n",
       "      <td>-0.212545</td>\n",
       "      <td>0.405815</td>\n",
       "      <td>0.412395</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>521</td>\n",
       "      <td>2</td>\n",
       "      <td>4575297</td>\n",
       "      <td>231168</td>\n",
       "      <td>4013246</td>\n",
       "      <td>98040</td>\n",
       "      <td>7002</td>\n",
       "      <td>61</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label        I1        I2        I3        I4        I5        I6  \\\n",
       "0           0  0.262324 -0.382217  0.323255 -1.058641 -0.896547 -0.488617   \n",
       "1           0  0.386949 -0.371194  0.323255 -0.216371 -0.896547  1.302719   \n",
       "2           0 -1.311604  0.896629 -0.156177 -0.216371 -0.112056  0.930983   \n",
       "3           0  0.941259  1.079725  1.906012  0.584996 -0.896547 -0.488617   \n",
       "4           0  1.057393  0.068799  0.323255  1.574801 -0.896547 -0.488617   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "999995      1 -0.560650 -2.133078  0.639048  0.068561 -0.896547  2.350584   \n",
       "999996      0 -0.560650 -2.133078 -1.187271 -1.058641 -0.896547  0.407051   \n",
       "999997      0 -0.837804 -0.371194 -1.187271  0.780321 -0.401588 -0.488617   \n",
       "999998      0  0.386949  1.333358  1.289596  1.875701  1.555578 -0.488617   \n",
       "999999      0  1.098833  0.543444 -0.156177  1.361091 -0.112056 -0.488617   \n",
       "\n",
       "              I7        I8        I9  ...  C17  C18  C19      C20      C21  \\\n",
       "0      -0.212545  1.569190 -0.247232  ...    3  137    0  3688627   978346   \n",
       "1      -0.212545  0.214639  0.082582  ...    0  228    0  5958569  1597651   \n",
       "2       2.320010  1.156740  0.082582  ...    1  137    0  3464388  1512195   \n",
       "3      -0.212545 -0.112180  1.469257  ...    2  228    9    18715  1630530   \n",
       "4      -0.212545  1.148680 -0.481238  ...    1  228    2  4575297   231168   \n",
       "...          ...       ...       ...  ...  ...  ...  ...      ...      ...   \n",
       "999995 -0.212545  0.443555  0.412395  ...    0  228    5  7783437  2579387   \n",
       "999996 -0.212545 -1.092636 -1.374871  ...    2  228    0  4742561  1545552   \n",
       "999997 -0.212545 -0.574641 -0.811051  ...    0  228    0  6607477  1674069   \n",
       "999998 -0.212545 -0.765817  0.771790  ...    1  216    5  1719047   946656   \n",
       "999999 -0.212545  0.405815  0.412395  ...    1  521    2  4575297   231168   \n",
       "\n",
       "            C22     C23    C24  C25  C26  \n",
       "0       2295585  205535   1588   24   23  \n",
       "1       3349225       0   6710   61   23  \n",
       "2        327622  125816  10156   63   25  \n",
       "3       2939009  104676  10782   45   23  \n",
       "4       4013246   98040  10285   24   12  \n",
       "...         ...     ...    ...  ...  ...  \n",
       "999995   104363       0   1613   45   34  \n",
       "999996   372202  283422   6718   61   25  \n",
       "999997  6842640       0   9892   63    6  \n",
       "999998  1363097  165612   7440    8    6  \n",
       "999999  4013246   98040   7002   61   12  \n",
       "\n",
       "[1000000 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cudf\n",
    "df = cudf.read_parquet(os.path.join(DATA_DIR, 'train', '0.parquet'), num_rows=1000000)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>...</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.386949</td>\n",
       "      <td>-0.371194</td>\n",
       "      <td>0.323255</td>\n",
       "      <td>-0.216371</td>\n",
       "      <td>-0.896547</td>\n",
       "      <td>1.302719</td>\n",
       "      <td>-0.212545</td>\n",
       "      <td>0.214639</td>\n",
       "      <td>0.082582</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>5958569</td>\n",
       "      <td>1597651</td>\n",
       "      <td>3349225</td>\n",
       "      <td>0</td>\n",
       "      <td>6710</td>\n",
       "      <td>61</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.941259</td>\n",
       "      <td>1.079725</td>\n",
       "      <td>1.906012</td>\n",
       "      <td>0.584996</td>\n",
       "      <td>-0.896547</td>\n",
       "      <td>-0.488617</td>\n",
       "      <td>-0.212545</td>\n",
       "      <td>-0.112180</td>\n",
       "      <td>1.469257</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>228</td>\n",
       "      <td>9</td>\n",
       "      <td>18715</td>\n",
       "      <td>1630530</td>\n",
       "      <td>2939009</td>\n",
       "      <td>104676</td>\n",
       "      <td>10782</td>\n",
       "      <td>45</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.057393</td>\n",
       "      <td>0.068799</td>\n",
       "      <td>0.323255</td>\n",
       "      <td>1.574801</td>\n",
       "      <td>-0.896547</td>\n",
       "      <td>-0.488617</td>\n",
       "      <td>-0.212545</td>\n",
       "      <td>1.148680</td>\n",
       "      <td>-0.481238</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>228</td>\n",
       "      <td>2</td>\n",
       "      <td>4575297</td>\n",
       "      <td>231168</td>\n",
       "      <td>4013246</td>\n",
       "      <td>98040</td>\n",
       "      <td>10285</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1.035691</td>\n",
       "      <td>-0.371194</td>\n",
       "      <td>-0.156177</td>\n",
       "      <td>-1.058641</td>\n",
       "      <td>0.747671</td>\n",
       "      <td>-0.488617</td>\n",
       "      <td>-0.212545</td>\n",
       "      <td>-1.092636</td>\n",
       "      <td>-0.065722</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>3669087</td>\n",
       "      <td>265283</td>\n",
       "      <td>6673100</td>\n",
       "      <td>186861</td>\n",
       "      <td>10431</td>\n",
       "      <td>64</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>-0.123270</td>\n",
       "      <td>0.639048</td>\n",
       "      <td>0.964612</td>\n",
       "      <td>0.588330</td>\n",
       "      <td>-0.488617</td>\n",
       "      <td>-0.212545</td>\n",
       "      <td>2.300952</td>\n",
       "      <td>0.646401</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>4374</td>\n",
       "      <td>1354949</td>\n",
       "      <td>3291829</td>\n",
       "      <td>0</td>\n",
       "      <td>5581</td>\n",
       "      <td>61</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999993</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.311604</td>\n",
       "      <td>-0.199777</td>\n",
       "      <td>-1.187271</td>\n",
       "      <td>-1.058641</td>\n",
       "      <td>0.252712</td>\n",
       "      <td>2.350584</td>\n",
       "      <td>2.320010</td>\n",
       "      <td>0.702204</td>\n",
       "      <td>-1.374871</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>2110464</td>\n",
       "      <td>2505660</td>\n",
       "      <td>4945751</td>\n",
       "      <td>0</td>\n",
       "      <td>11202</td>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999994</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.364005</td>\n",
       "      <td>1.209166</td>\n",
       "      <td>0.874918</td>\n",
       "      <td>0.858126</td>\n",
       "      <td>2.753796</td>\n",
       "      <td>1.826651</td>\n",
       "      <td>11.691580</td>\n",
       "      <td>-1.092636</td>\n",
       "      <td>1.175602</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>228</td>\n",
       "      <td>8</td>\n",
       "      <td>3341406</td>\n",
       "      <td>1056180</td>\n",
       "      <td>2126786</td>\n",
       "      <td>271285</td>\n",
       "      <td>10156</td>\n",
       "      <td>45</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.560650</td>\n",
       "      <td>-2.133078</td>\n",
       "      <td>0.639048</td>\n",
       "      <td>0.068561</td>\n",
       "      <td>-0.896547</td>\n",
       "      <td>2.350584</td>\n",
       "      <td>-0.212545</td>\n",
       "      <td>0.443555</td>\n",
       "      <td>0.412395</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>5</td>\n",
       "      <td>7783437</td>\n",
       "      <td>2579387</td>\n",
       "      <td>104363</td>\n",
       "      <td>0</td>\n",
       "      <td>1613</td>\n",
       "      <td>45</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.560650</td>\n",
       "      <td>-2.133078</td>\n",
       "      <td>-1.187271</td>\n",
       "      <td>-1.058641</td>\n",
       "      <td>-0.896547</td>\n",
       "      <td>0.407051</td>\n",
       "      <td>-0.212545</td>\n",
       "      <td>-1.092636</td>\n",
       "      <td>-1.374871</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>4742561</td>\n",
       "      <td>1545552</td>\n",
       "      <td>372202</td>\n",
       "      <td>283422</td>\n",
       "      <td>6718</td>\n",
       "      <td>61</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.837804</td>\n",
       "      <td>-0.371194</td>\n",
       "      <td>-1.187271</td>\n",
       "      <td>0.780321</td>\n",
       "      <td>-0.401588</td>\n",
       "      <td>-0.488617</td>\n",
       "      <td>-0.212545</td>\n",
       "      <td>-0.574641</td>\n",
       "      <td>-0.811051</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>6607477</td>\n",
       "      <td>1674069</td>\n",
       "      <td>6842640</td>\n",
       "      <td>0</td>\n",
       "      <td>9892</td>\n",
       "      <td>63</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499984 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label        I1        I2        I3        I4        I5        I6  \\\n",
       "1           0  0.386949 -0.371194  0.323255 -0.216371 -0.896547  1.302719   \n",
       "3           0  0.941259  1.079725  1.906012  0.584996 -0.896547 -0.488617   \n",
       "4           0  1.057393  0.068799  0.323255  1.574801 -0.896547 -0.488617   \n",
       "6           0  1.035691 -0.371194 -0.156177 -1.058641  0.747671 -0.488617   \n",
       "7           0  0.018519 -0.123270  0.639048  0.964612  0.588330 -0.488617   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "999993      0 -1.311604 -0.199777 -1.187271 -1.058641  0.252712  2.350584   \n",
       "999994      0 -0.364005  1.209166  0.874918  0.858126  2.753796  1.826651   \n",
       "999995      1 -0.560650 -2.133078  0.639048  0.068561 -0.896547  2.350584   \n",
       "999996      0 -0.560650 -2.133078 -1.187271 -1.058641 -0.896547  0.407051   \n",
       "999997      0 -0.837804 -0.371194 -1.187271  0.780321 -0.401588 -0.488617   \n",
       "\n",
       "               I7        I8        I9  ...  C17  C18  C19      C20      C21  \\\n",
       "1       -0.212545  0.214639  0.082582  ...    0  228    0  5958569  1597651   \n",
       "3       -0.212545 -0.112180  1.469257  ...    2  228    9    18715  1630530   \n",
       "4       -0.212545  1.148680 -0.481238  ...    1  228    2  4575297   231168   \n",
       "6       -0.212545 -1.092636 -0.065722  ...    2  228    0  3669087   265283   \n",
       "7       -0.212545  2.300952  0.646401  ...    0  228    0     4374  1354949   \n",
       "...           ...       ...       ...  ...  ...  ...  ...      ...      ...   \n",
       "999993   2.320010  0.702204 -1.374871  ...    0  228    0  2110464  2505660   \n",
       "999994  11.691580 -1.092636  1.175602  ...    1  228    8  3341406  1056180   \n",
       "999995  -0.212545  0.443555  0.412395  ...    0  228    5  7783437  2579387   \n",
       "999996  -0.212545 -1.092636 -1.374871  ...    2  228    0  4742561  1545552   \n",
       "999997  -0.212545 -0.574641 -0.811051  ...    0  228    0  6607477  1674069   \n",
       "\n",
       "            C22     C23    C24  C25  C26  \n",
       "1       3349225       0   6710   61   23  \n",
       "3       2939009  104676  10782   45   23  \n",
       "4       4013246   98040  10285   24   12  \n",
       "6       6673100  186861  10431   64   23  \n",
       "7       3291829       0   5581   61    6  \n",
       "...         ...     ...    ...  ...  ...  \n",
       "999993  4945751       0  11202   36   13  \n",
       "999994  2126786  271285  10156   45   23  \n",
       "999995   104363       0   1613   45   34  \n",
       "999996   372202  283422   6718   61   25  \n",
       "999997  6842640       0   9892   63    6  \n",
       "\n",
       "[499984 rows x 40 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do some filtering or whatever\n",
    "df[df['C18'] == 228]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great news for data scientists: formats like parquet are the bread and butter of any sort of data exploration. You almost certainly want to keep at least *one* version of your dataset in a format like this. If your dataset is large enough, and storage gets expensive, it's probably the *only* format you want to keep your dataset in.\n",
    "\n",
    "Unfortunately, TensorFlow does not have fast native readers for formats like this that can read larger-than-memory datasets in an online fashion. TensorFlow's preferred, and fastest, data format is the [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord), a binary format which associates all field names and their values with every example in your datset. For tabular data, where small float or int features have a smaller memory footprint than string field names, the memory footprint of such a representation can get really big, really fast.\n",
    "\n",
    "More importantly, TFRecords require reading and parsing in batches using user-provided data schema descriptions. This makes doing the sorts of manipulations described above difficult, if not near impossible, and requires an enormous amount of work to change the values corresponding to a single field in your dataset. For this reason, you almost never want to use TFRecords as the *only* means of representing your data, which means you have generate and store an entire copy of your dataset every time it needs to update. This can take an enormous amount of time and resources that prolong the time from the conception of a feature to testing it in a model.\n",
    "\n",
    "The main advantage of TFRecords is the speed with which TensorFlow can read them (and its APIs for doing this online), and their support for multi-hot categorical features. While NVTabular is still working on addressing the latter, we'll show below that reading parquet files in batch using NVTabular is substantially faster than the existing TFRecord readers. In order to do this, we'll need to generate a TFRecord version of the parquet dataset we generated before. I'm going to restrict this to generating just the 1000 steps we'll need to do our training demo, but if you have a few days and a couple terabytes of storage lying around feel free to run the whole thing.\n",
    "\n",
    "Don't worry too much about the code below: it's a bit dense (and frankly still isn't fully robust to string features) and doesn't have much to do with what follows. I'm sure there are ways to make it cleaner/faster/etc., but If anything, it should make clear how nontrivial the process of building and writing TFRecords is.\n",
    "\n",
    "The last thing I'll note is that the astute and experienced TensorFlow user will at this point object that there exist ways to make reading TFRecords for tabular data faster than what I'm about to present. Among these are pre-batching examples (which, I would point out, more or less enforces a fixed valency for all categorical features) and combining all fixed valency categorical and continuous features into vectorized fields in records which can all be parsed at once. And while it's true that methods like this will accelerate TFRecord reading, they still fail to overtake NVTabular's parquet reader. Perhaps more importantly (at least from my workflow-centric view), they only compound the problems I've outlined so far of the difficulty of doing data analysis with TFRecords, and would almost certainly require the code below to be even more brittle and complicated. And this is actually a point worth emphasizing: with NVTabular data loading, you're getting better performance *and* less programming overhead, the holy grail of GPU-based DL software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing as mp\n",
    "# from glob import glob\n",
    "# from itertools import repeat\n",
    "# from tqdm.notebook import trange\n",
    "\n",
    "# def pool_initializer(num_cols, cat_cols):\n",
    "#     global numeric_columns\n",
    "#     global categorical_columns\n",
    "#     numeric_columns = num_cols\n",
    "#     categorical_columns = cat_cols\n",
    "\n",
    "# def build_and_serialize_example(data):\n",
    "#     numeric_values, categorical_values = data\n",
    "#     feature = {}\n",
    "#     if numeric_values is not None:\n",
    "#         feature.update({\n",
    "#             col: tf.train.Feature(float_list=tf.train.FloatList(value=[val]))\n",
    "#                 for col, val in zip(numeric_columns, numeric_values)\n",
    "#     })\n",
    "#     if categorical_values is not None:\n",
    "#         feature.update({\n",
    "#             col: tf.train.Feature(int64_list=tf.train.Int64List(value=[val]))\n",
    "#                 for col, val in zip(categorical_columns, categorical_values)\n",
    "#     })\n",
    "#     return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
    "\n",
    "# def get_writer(write_dir, file_idx):\n",
    "#     filename = str(file_idx).zfill(5) + '.tfrecords'\n",
    "#     return tf.io.TFRecordWriter(os.path.join(write_dir, filename))\n",
    "\n",
    "\n",
    "# _EXAMPLES_PER_RECORD = 20000000\n",
    "# write_dir = os.path.dirname(TFRECORDS)\n",
    "# if not os.path.exists(write_dir):\n",
    "#     os.makedirs(write_dir)\n",
    "# file_idx, example_idx = 0, 0\n",
    "# writer = get_writer(write_dir, file_idx)\n",
    "\n",
    "# do_break = False\n",
    "# column_names = [NUMERIC_FEATURE_NAMES, CATEGORICAL_FEATURE_NAMES+[LABEL_NAME]]\n",
    "# with mp.Pool(8, pool_initializer, column_names) as pool:\n",
    "#     fnames = glob(PARQUETS)\n",
    "#     ds_iterator = nvt.dataset(fnames, gpu_memory_frac=0.1)\n",
    "#     pbar = trange(BATCH_SIZE*STEPS)\n",
    "\n",
    "#     for df in ds_iterator:\n",
    "#         data = []\n",
    "#         for col_names in column_names:\n",
    "#             if len(col_names) == 0:\n",
    "#                 data.append(repeat(None))\n",
    "#             else:\n",
    "#                 data.append(df[col_names].to_pandas().values)\n",
    "#         data = zip(*data)\n",
    "\n",
    "#         record_map = pool.imap(build_and_serialize_example, data, chunksize=200)\n",
    "#         for record in record_map:\n",
    "#             writer.write(record)\n",
    "#             example_idx += 1\n",
    "\n",
    "#             if example_idx == _EXAMPLES_PER_RECORD:\n",
    "#                 writer.close()\n",
    "#                 file_idx += 1\n",
    "#                 writer = get_writer(file_idx)\n",
    "#                 example_idx = 0\n",
    "#             pbar.update(1)\n",
    "#             if pbar.n == BATCH_SIZE*STEPS:\n",
    "#                 do_break = True\n",
    "#                 break\n",
    "#         if do_break:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have our data set up the way that we need it, we're ready to get training! TensorFlow provides a handy utility for building an online dataloader that we'll use to parse the tfrecords. Meanwhile, on the NVTablar side, we'll use the `KerasSequenceDataset` for reading chunks of parquet files. We'll also use a `Workflow` with a `HashBucket` preprocessing op in order to perform the requisite hashing online. Take a look below to see the similarities in the API. What's great about using NVTabular `Workflow`s for online preprocessing is that it makes doing arbitrary preprocessing reasonably simple by using `DFlambda` ops, and the `Op` class API allows for extension to more complicated, stat-driven preprocessing as well.\n",
    "\n",
    "One potentially important difference between these dataset classes is the way in which shuffling is handled. The TensorFlow data loader maintains a buffer of size `shuffle_buffer_size` from which batch elements are randomly selected, with the buffer then sequentially replenished by the next `batch_size` elements in the TFRecord. Large shuffle buffers, while allowing for better epoch-to-epoch randomness and hence generalization, can be hard to maintain given the slow read times. This isn't as big a deal for datasets which are uniformly shuffled in the TFRecord and only require one or two epochs to converge, but many datasets are ordered by some feature (whether it's time or some categorical groupby), and in this case the windowed shuffle buffer can lead to biased sampling and hence poorer quality gradients.\n",
    "\n",
    "On the other hand, the `KerasSequenceDataset` manages shuffling by loading in very large chunks of data, shuffling it, than reading through them sequentially in batches. While this limits the amount of randomness to the size of the chunks, the size of chunks that CuDF can handle is much larger than the effective chunk size created by the windowed buffer method (since older elements slowly trickle out of the buffer), leading to better randomness and hence better gradient estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(file_pattern, columns, accelerated=False):\n",
    "    # get rid of embeddings for \"raw\" columns\n",
    "    columns = [getattr(col, \"categorical_column\", col) for col in columns]\n",
    "\n",
    "    # make a tfrecord features dataset\n",
    "    if not accelerated:\n",
    "        # feature spec tells us how to parse tfrecords\n",
    "        # using FixedLenFeatures keeps from using sparse machinery,\n",
    "        # but obviously wouldn't extend to multi-hot categoricals\n",
    "        get_dtype = lambda col: getattr(col, \"dtype\", tf.int64)\n",
    "        feature_spec = {column.name: tf.io.FixedLenFeature((1,), get_dtype(column)) for column in columns}\n",
    "        feature_spec[LABEL_NAME] = tf.io.FixedLenFeature((1,), tf.int64)\n",
    "\n",
    "        dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "            file_pattern,\n",
    "            BATCH_SIZE,\n",
    "            feature_spec,\n",
    "            label_key=LABEL_NAME,\n",
    "            num_epochs=1,\n",
    "            shuffle=True,\n",
    "            shuffle_buffer_size=4*BATCH_SIZE,\n",
    "        )\n",
    "\n",
    "    # make an nvtabular KerasSequenceDataset and add\n",
    "    # a hash bucketing workflow for online preproc\n",
    "    else:\n",
    "        dataset = KerasSequenceDataset(\n",
    "            file_pattern,\n",
    "            columns,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            label_name=LABEL_NAME,\n",
    "            shuffle=True,\n",
    "            buffer_size=0.2,\n",
    "            dataset_size=STEPS*BATCH_SIZE\n",
    "        )\n",
    "        workflow = nvt.Workflow(\n",
    "            cat_names=CATEGORICAL_FEATURE_NAMES,\n",
    "            cont_names=NUMERIC_FEATURE_NAMES,\n",
    "            label_name=[LABEL_NAME]\n",
    "        )\n",
    "        num_buckets = {\n",
    "            col.name: col.num_buckets for col in columns\n",
    "                if col.name in CATEGORICAL_FEATURE_NAMES\n",
    "        }\n",
    "        workflow.add_cat_preprocess(nvt.ops.HashBucket(num_buckets))\n",
    "        workflow.finalize() # always make sure you finalize your workflow before mapping\n",
    "\n",
    "        # map the workflow onto the dataset in-place\n",
    "        dataset.map(workflow)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Living In The Continuous World\n",
    "So at this point, we have a description of our dataset schema contained in our `feature_column`s, and we have a `dataset` object which can load some particular materialization of this schema (our dataset) in an online fashion (with the bytes encoding that materialization organized according to either the TFRecord or Parquet standard).\n",
    "\n",
    "Once the data is loaded, it needs to get run through a neural network, which will use them to produce predictions of interaction likelihoods, compare its predictions to the labelled answers, and improve its future guesses using this comparison through the magic of backpropogation. Easy as pie.\n",
    "\n",
    "Unfortunately, the magic of backpropogation relies on a trick of calculus which, by its nature, requires that the functions represented by the neural network are *continuous*. Whether or not you fully understand exactly what that means, you can probably imagine that this is incongrous with the *categorical* features our dataset contains. Less fundamentally, but from an equally practical standpoint, much of the algebra that our network will perform on our tabular features goes much (read: *MUCH*) faster if we do it in parallel as matrix algebra.\n",
    "\n",
    "For these reasons, we'll want to convert our tabular continous and categorical features into purely continuous vectors that can be consumed by the network and processed efficiently. For categorical features, this means using the categorical index to lookup a (typically learned) vector from some lower-dimensional space to pass to the network. The exact mechanism by which your network embeds and combines these values will depend on your choice of architecture. But the fundamental operation of looking up and concatenating (or stacking) is ubiquitous across almost all tabular deep learning architectures.\n",
    "\n",
    "The go-to Keras layer for doing this sort of operation is the `DenseFeatures` layer, which will also perform any transformations defined by your `feature_column`s. The downside of using the `DenseFeatures` layer, as we'll investigate more fully in a bit, is that its GPU performance is handicapped by the use of lots of small ops for doing things that aren't necessarily worth doing on an accelerator like a GPU e.g. checking for in-range values. This drowns the compute itself in kernel launch overhead. Moreover, `DenseFeatures` has no mechanism for identifying one-hot categorical features, instead using `SparseTensor` machinery for all categorical columns for the sake of robustness. Many sparse TensorFlow ops aren't optimized for GPU, further reducing GPU performance.\n",
    "\n",
    "Because we're now doing all our transformations in NVTabular, and we *know* all of our categorical features are one-hot, we can build a better-optimized layer with this functionality like the one I've build in `layers.py`. Below, we'll see how we can wrap such a layer into an embedding layer built specifically for the input pattern leveraged by the DLRM architecture. Note how the numeric and categorical features are handled entirely separately: this is a peculiarity of DLRM and shouldn't be treated leveraged as an assumption of a robust embedding layer (and our `ScalarDenseFeatures` layer makes no such assumption). As a helpful exercise, I would encourage the reader to think of *other* input ingestion patterns that might capture information the DLRM one does not, and use these same building blocks to mock up an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLRMEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, columns, accelerated=False, **kwargs):\n",
    "        is_cat = lambda col: hasattr(col, \"categorical_column\")\n",
    "        embedding_columns = list(filter(is_cat, columns))\n",
    "        numeric_columns = list(filterfalse(is_cat, columns))\n",
    "\n",
    "        self.categorical_feature_names = [col.categorical_column.name for col in embedding_columns]\n",
    "        self.numeric_feature_names = [col.name for col in numeric_columns]\n",
    "\n",
    "        if not accelerated:\n",
    "            # need DenseFeatures layer to perform transformations,\n",
    "            # so we're stuck with the whole thing\n",
    "            self.categorical_densifier = tf.keras.layers.DenseFeatures(embedding_columns)\n",
    "            self.categorical_reshape = tf.keras.layers.Reshape((len(embedding_columns), -1))\n",
    "            self.numeric_densifier = tf.keras.layers.DenseFeatures(numeric_columns)\n",
    "        else:\n",
    "            # otherwise we can do a much faster embedding that\n",
    "            # doesn't break out the SparseTensor machinery\n",
    "            self.categorical_densifier = layers.ScalarDenseFeatures(embedding_columns, aggregation=\"stack\")\n",
    "            self.categorical_reshape = None\n",
    "            self.numeric_densifier = layers.ScalarDenseFeatures(numeric_columns, aggregation=\"concat\")\n",
    "        super(DLRMEmbedding, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if not isinstance(inputs, dict):\n",
    "            raise TypeError(\"Expected a dict!\")\n",
    "\n",
    "        categorical_inputs = {name: inputs[name] for name in self.categorical_feature_names}\n",
    "        numeric_inputs = {name: inputs[name] for name in self.numeric_feature_names}\n",
    "\n",
    "        fm_x = self.categorical_densifier(categorical_inputs)\n",
    "        dense_x = self.numeric_densifier(numeric_inputs)\n",
    "        if self.categorical_reshape is not None:\n",
    "            fm_x = self.categorical_reshape(fm_x)\n",
    "        return fm_x, dense_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Our Differences Aside\n",
    "As a practical matter, that *does it* for the differences between a typical TensorFlow pipeline and an NVTabular accelerated pipeline. Let's review where they've diverged so far:\n",
    "- We needed different feature columns because we're no longer using TensorFlow's transformation code for the hash bucketing\n",
    "- We needed a different data loader because we're reading parquet files instead of tfrecords (and using NVTabular to hash that data online)\n",
    "- We needed a different embedding layer because the existing one is suboptimal and we don't need most of its functionality\n",
    "\n",
    "Once the data is ready to be consumed by the network, we really *shouldn't* be doing anything different. So from here on out we'll just define the DLRM architecture using Keras, and then define a training function which uses the components we've built so far to string together a functional training run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUMLP(tf.keras.layers.Layer):\n",
    "    def __init__(self, dims, output_activation, **kwargs):\n",
    "        self.layers = []\n",
    "        for dim in dims[:-1]:\n",
    "            self.layers.append(tf.keras.layers.Dense(dim, activation=\"relu\"))\n",
    "        self.layers.append(tf.keras.layers.Dense(dims[-1], activation=output_activation))\n",
    "        super(ReLUMLP, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class DLRM(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, top_mlp_hidden_dims, bottom_mlp_hidden_dims, **kwargs):\n",
    "        self.top_mlp = ReLUMLP(top_mlp_hidden_dims + [embedding_dim], \"linear\", name=\"top_mlp\")\n",
    "        self.bottom_mlp = ReLUMLP(bottom_mlp_hidden_dims + [1], \"sigmoid\", name=\"bottom_mlp\")\n",
    "        self.interaction = layers.DotProductInteraction()\n",
    "        super(DLRM, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        dense_x, fm_x = inputs\n",
    "        dense_x = self.top_mlp(dense_x)\n",
    "        dense_x_expanded = tf.expand_dims(dense_x, axis=1)\n",
    "\n",
    "        x = tf.concat([fm_x, dense_x_expanded], axis=1)\n",
    "        x = self.interaction(x)\n",
    "        x = tf.concat([x, dense_x], axis=1)\n",
    "        x = self.bottom_mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an ugly little function I have for giving a more useful reporting of the model parameter count, since the embedding parameters will dominate the total count yet account for very little of the actual learning capacity. Unless you're curious, just execute the cell and keep moving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_counts(model):\n",
    "    from functools import reduce\n",
    "    mult = lambda x, y: x*y\n",
    "    trainable_weights = model.trainable_weights\n",
    "    check_name = lambda x: (\n",
    "        x.name.split(\":\")[0].split(\"/\")[-1] == \"embedding_weights\")\n",
    "\n",
    "    trainable_embedding_weights = filter(check_name, trainable_weights)\n",
    "    trainable_network_weights = filterfalse(check_name, trainable_weights)\n",
    "\n",
    "    num_embedding_params = sum(\n",
    "        [reduce(mult, x.shape) for x in trainable_embedding_weights]\n",
    "    )\n",
    "    num_network_params = sum(\n",
    "        [reduce(mult, x.shape) for x in trainable_network_weights]\n",
    "    )\n",
    "    print(\"Embedding parameter count: {}\".format(num_embedding_params))\n",
    "    print(\"Non-embedding parameter count: {}\".format(num_network_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, finally, below we will define our training pipeline from end to end. Take a look at the comments to see how each component we've built so far plugs in. What's great about such a pipeline is that it's more or less agnostic to what the schema returned by `get_feature_columns` looks like (subject of course to the constraint that there are no multi-hot categorical or vectorized continuous features, which aren't supported yet). In fact, from a certain point of view it would make sense to make the columns and filenames an *input* to this function (and possibly even the architecture itself as well). But I'll leave that level of robustness to you for when you build your own pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_a_model(device, accelerated=False):\n",
    "    # get our columns to describe our dataset\n",
    "    columns = get_feature_columns(accelerated=accelerated)\n",
    "\n",
    "    # build a dataset from those descriptions\n",
    "    file_pattern = PARQUETS if accelerated else TFRECORDS\n",
    "    train_dataset = make_dataset(file_pattern, columns, accelerated=accelerated)\n",
    "\n",
    "    # build our Keras model, using column descriptions to build input tensors\n",
    "    inputs = {}\n",
    "    for column in columns:\n",
    "        column = getattr(column, \"categorical_column\", column)\n",
    "        dtype = getattr(column, \"dtype\", tf.int64)\n",
    "        inputs[column.name] = tf.keras.Input(name=column.name, shape=(1,), dtype=dtype)\n",
    "    fm_x, dense_x = DLRMEmbedding(columns, accelerated=accelerated)(inputs)\n",
    "    x = DLRM(EMBEDDING_DIM, TOP_MLP_HIDDEN_DIMS, BOTTOM_MLP_HIDDEN_DIMS)([dense_x, fm_x])\n",
    "    model = tf.keras.Model(inputs=list(inputs.values()), outputs=x)\n",
    "\n",
    "    # compile our Keras model with our desired loss, optimizer, and metrics\n",
    "    optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "    metrics = [tf.keras.metrics.AUC(curve=\"ROC\", name=\"auroc\")]\n",
    "    model.compile(optimizer, \"binary_crossentropy\", metrics=metrics)\n",
    "    print_param_counts(model)\n",
    "\n",
    "    # create a file writer for our custom throughput logger\n",
    "    run_name = device + \"_\" + (\"accelerated\" if accelerated else \"native\")\n",
    "    if mixed_precision.global_policy().name == \"mixed_float16\":\n",
    "        run_name += \"_mixed-precision\"\n",
    "    log_dir = os.path.join(LOG_DIR, run_name)\n",
    "    file_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"metrics\"))\n",
    "    file_writer.set_as_default()\n",
    "\n",
    "    # now fit the model on the desired device\n",
    "    with tf.device(\"/{}:0\".format(device)):\n",
    "        model.fit(\n",
    "            train_dataset,\n",
    "            epochs=1,\n",
    "            steps_per_epoch=STEPS,\n",
    "            callbacks=[\n",
    "                callbacks.ThroughputLogger(BATCH_SIZE),\n",
    "                tf.keras.callbacks.TensorBoard(log_dir, update_freq=20, profile_batch=100)\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One particularly cool feature of TensorFlow's TensorBoard tool is that we can embed it directly into this notebook. This way, we can monitor training metrics, including throughput, as well as take a look at the in-depth profiles the most recent versions of TensorBoard can generate, without every having to leave the comfort of this browser tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 158), started 1:18:24 ago. (Use '!kill 158' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-259e0dd6f53ba0d8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-259e0dd6f53ba0d8\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not os.path.exists(LOG_DIR):\n",
    "    os.mkdir(LOG_DIR)\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /home/docker/tensorflow/logs --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by doing a training run on CPU using all the default TensorFlow tools. Since I'm less concerned about profiling this run, we'll just note the throughput and then move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding parameter count: 188746160\n",
      "Non-embedding parameter count: 2747145\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer DLRMEmbedding has arguments in `__init__` and therefore must override `get_config`.\n",
      "  99/1000 [=>............................] - ETA: 20:49 - loss: 0.1568 - auroc: 0.6552WARNING:tensorflow:From /conda/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "1000/1000 [==============================] - 1384s 1s/step - loss: 0.1338 - auroc: 0.7441\n"
     ]
    }
   ],
   "source": [
    "fit_a_model(\"cpu\", accelerated=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's do the exact same run, but this time on GPU. This will give us some indication of the \"out-of-the-box\" acceleration generated by GPU-based training. We'll see that it's not particularly impressive (around 4x or so), and we'll start to get an indication of *why* that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding parameter count: 188746160\n",
      "Non-embedding parameter count: 2747145\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer DLRMEmbedding has arguments in `__init__` and therefore must override `get_config`.\n",
      "1000/1000 [==============================] - 354s 354ms/step - loss: 0.1325 - auroc: 0.7459\n"
     ]
    }
   ],
   "source": [
    "fit_a_model(\"gpu\", accelerated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding parameter count: 0\n",
      "Non-embedding parameter count: 2747145\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer DLRMEmbedding has arguments in `__init__` and therefore must override `get_config`.\n",
      "  99/1000 [=>............................] - ETA: 1:41 - loss: 0.1483 - auroc: 0.6561WARNING:tensorflow:From /conda/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "1000/1000 [==============================] - 122s 122ms/step - loss: 0.1346 - auroc: 0.7204\n"
     ]
    }
   ],
   "source": [
    "fit_a_model(\"gpu\", accelerated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla V100-SXM2-32GB, compute capability 7.0\n"
     ]
    }
   ],
   "source": [
    "# update our precision policy to use mixed\n",
    "policy = mixed_precision.Policy(\"mixed_float16\")\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding parameter count: 188746160\n",
      "Non-embedding parameter count: 2747145\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer DLRMEmbedding has arguments in `__init__` and therefore must override `get_config`.\n",
      "1000/1000 [==============================] - 373s 373ms/step - loss: 0.2263 - auroc: 0.4926\n"
     ]
    }
   ],
   "source": [
    "fit_a_model(\"gpu\", accelerated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding parameter count: 0\n",
      "Non-embedding parameter count: 2747145\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer DLRMEmbedding has arguments in `__init__` and therefore must override `get_config`.\n",
      "1000/1000 [==============================] - 63s 63ms/step - loss: 0.2165 - auroc: 0.5829\n"
     ]
    }
   ],
   "source": [
    "fit_a_model(\"gpu\", accelerated=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
