{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Tabular Deep Learning Models with Keras on GPU\n",
    "Deep learning has revolutionized the fields of computer vision (CV) and natural language processing (NLP) in the last few years, providing a fast and general framework for solving a host of difficult problems with unprecedented accuracy. Part and parcel of this revolution has been the development of APIs like [Keras](https://www.tensorflow.org/api_docs/python/tf/keras) for NVIDIA GPUs, allowing practitioners to quickly iterate on new and interesting ideas and receive feedback on their efficacy in shorter and shorter intervals.\n",
    "\n",
    "One class of problem which has remained largely immune to this revolution, however, is the class involving tabular data. Part of this difficulty is that, unlike CV or NLP, where different datasets are underlied by similar phenomena and therefore can be solved with similar mechanisms, \"tabular datasets\" span a vast array of phenomena, semantic meanings, and problem statements, from product and video recommendation to particle discovery and loan default prediction. This diversity makes universally useful components difficult to find or even define, and is only exacerbated by the notorious lack of standard, industrial-scale benchmark datasets in the tabular space. Accordingly, deep learning models are frequently bested by their machine learning analogues on these important tasks.\n",
    "\n",
    "Yet this diversity is also what makes tools like Keras all the more valuable. Architecture components can be quickly swapped in and out for different tasks like the implementation details they are, and new components can be built and tested with ease. Importantly, domain experts can interact with models at a high level and build their expertise into their design, without having to spend their time becoming Python programming wizrds. However, the other key ingredient of fast feedback, enabled by GPU acceleration and correlating with cost-efficiency for production pipelines, is lacking in most out-of-the-box APIs. In this example, we will walk through some recent advancements made by NVIDIA's [NVTabular](https://github.com/nvidia/nvtabular) library that can alleviate existing bottlenecks and bring to bear the full power of GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b8c0b846824a9145\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b8c0b846824a9145\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "! mkdir -p logs/native logs/accelerated\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /home/docker/tensorflow/logs --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use these and fix other paths\n",
    "DATA_DIR = '/data'\n",
    "LOG_DIR = 'logs/'\n",
    "ACCELERATED = True\n",
    "\n",
    "# TODO: reimplement the preproc from criteo-example here?\n",
    "# Alternatively, make criteo its own folder, and split preproc\n",
    "# and training into separate notebooks, then execute the\n",
    "# preproc notebook from here?\n",
    "NUMERIC_FEATURE_NAMES = [f'I{i}' for i in range(1, 14)]\n",
    "CATEGORICAL_FEATURE_NAMES = [f'C{i}' for i in range(1, 27)]\n",
    "CATEGORY_COUNTS = [\n",
    "    7599500, 33521, 17022, 7339, 20046, 3, 7068, 1377, 63, 5345303,\n",
    "    561810, 242827, 11, 2209, 10616, 100, 4, 968, 14, 7838519,\n",
    "    2580502, 6878028, 298771, 11951, 97, 35\n",
    "]\n",
    "LABEL_NAME = 'label'\n",
    "\n",
    "# optimization params\n",
    "BATCH_SIZE = 65536\n",
    "STEPS = 1000\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# architecture params\n",
    "EMBEDDING_DIM = 8\n",
    "TOP_MLP_HIDDEN_DIMS = [1024, 512, 256]\n",
    "BOTTOM_MLP_HIDDEN_DIMS = [1024, 1024, 512, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import layers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns(accelerated=False):\n",
    "    numeric_columns = [tf.feature_column.numeric_column(name, (1,)) for name in NUMERIC_FEATURE_NAMES]\n",
    "    if not accelerated:\n",
    "        categorical_columns = [\n",
    "            tf.feature_column.categorical_column_with_hash_bucket(\n",
    "                name, int(0.75*count), dtype=tf.int64) for\n",
    "            name, count in zip(CATEGORICAL_FEATURE_NAMES, CATEGORY_COUNTS)\n",
    "        ]\n",
    "    else:\n",
    "        categorical_columns = [\n",
    "            tf.feature_column.categorical_column_with_identity(\n",
    "                name, int(0.75*count)) for\n",
    "            name, count in zip(CATEGORICAL_FEATURE_NAMES, CATEGORY_COUNTS)\n",
    "        ]\n",
    "    return numeric_columns, categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(columns, accelerated=False):\n",
    "    if not accelerated:\n",
    "        # feature spec tells us how to parse tfrecords\n",
    "        # using FixedLenFeatures keeps from using sparse machinery,\n",
    "        # but obviously wouldn't extend to multi-hot categoricals\n",
    "        # TODO: this is being generous, since the typical tensorflow user\n",
    "        # will use tf.feature_column.make_parse_example_spec which will\n",
    "        # use varlenfeature by default for categorical columns, and the\n",
    "        # sparse machinery will definitely hurt GPU perf. Should we just\n",
    "        # use this, even though our API is leveraging the fact that all\n",
    "        # categoricals are single hot?\n",
    "        feature_spec = {\n",
    "            column.name: tf.io.FixedLenFeature((1,), getattr(column, 'dtype', tf.int64)) for column in columns\n",
    "        }\n",
    "        feature_spec[LABEL_NAME] = tf.io.FixedLenFeature((1,), tf.int64)\n",
    "        return tf.data.experimental.make_batched_features_dataset(\n",
    "            '/data/tfrecords/train/*.tfrecords',\n",
    "            BATCH_SIZE,\n",
    "            feature_spec,\n",
    "            label_key=LABEL_NAME,\n",
    "            num_epochs=1,\n",
    "            shuffle=True,\n",
    "            shuffle_buffer_size=4*BATCH_SIZE,\n",
    "            )\n",
    "    else:\n",
    "        from nvtabular import Workflow\n",
    "        from nvtabular.tf_dataloader import KerasSequenceDataset\n",
    "        from nvtabular.ops import HashBucket\n",
    "        dataset = KerasSequenceDataset(\n",
    "            '/data/stable/train/*.parquet',\n",
    "            columns,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            label_name=LABEL_NAME,\n",
    "            shuffle=True,\n",
    "            buffer_size=0.2,\n",
    "            dataset_size=STEPS*BATCH_SIZE\n",
    "          )\n",
    "        workflow = Workflow(\n",
    "            cat_names=CATEGORICAL_FEATURE_NAMES,\n",
    "            cont_names=CONTINUOUS_FEATURE_NAMES,\n",
    "            label_name=LABEL_NAME\n",
    "        )\n",
    "        num_buckets = {\n",
    "            col.name: col.num_buckets for col in columns\n",
    "                if col.name in CATEGORICAL_FEATURE_NAMES\n",
    "        }\n",
    "        workflow.add_cont_preprocess(HashBucket(num_buckets))\n",
    "        workflow.finalize()\n",
    "        dataset.map(workflow)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_inputs(numeric_inputs, categorical_inputs, accelerated=False):\n",
    "    numeric_columns, categorical_columns = get_columns(accelerated=accelerated)\n",
    "\n",
    "    fm_x = tf.keras.layers.DenseFeatures(embedding_columns)(categorical_inputs)\n",
    "    fm_x = tf.keras.layers.Reshape((len(categorical_columns), EMBEDDING_DIM))(fm_x)\n",
    "    dense_x = tf.keras.layers.DenseFeatures(numeric_columns)(numeric_inputs)\n",
    "    return fm_x, dense_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_architecture():\n",
    "    dense_input = tf.keras.Input(name='dense_x', shape=(len(numeric_columns),), dtype=tf.float32)\n",
    "    fm_input = tf.keras.Input(name='fm_x', shape=(len(categorical_columns), EMBEDDING_DIM), dtype=tf.float32)\n",
    "\n",
    "    dense_x = dense_input\n",
    "    for dim in TOP_MLP_HIDDEN_DIMS:\n",
    "        dense_x = tf.keras.layers.Dense(dim, activation='relu')(dense_x)\n",
    "    dense_x = tf.keras.layers.Dense(EMBEDDING_DIM, activation='linear')(dense_x)\n",
    "    dense_x = tf.keras.layers.Reshape((1, EMBEDDING_DIM))(dense_x)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate(axis=1)([fm_input, dense_x])\n",
    "    x = layers.DotProductInteraction()(x)\n",
    "    for dim in BOTTOM_MLP_HIDDEN_DIMS:\n",
    "        x = tf.keras.layers.Dense(dim, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=[dense_input, fm_input], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/conda/lib/python3.7/site-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_NVVM=/usr/local/cuda/nvvm/lib64/libnvvm.so.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n",
      "/conda/lib/python3.7/site-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_LIBDEVICE=/usr/local/cuda/nvvm/libdevice/.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-79c18220efe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membedding_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategorical_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mACCELERATED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-209b50dcd69e>\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(columns, accelerated)\u001b[0m\n\u001b[1;32m     24\u001b[0m             )\n\u001b[1;32m     25\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mnvtabular\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWorkflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnvtabular\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_dataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasSequenceDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnvtabular\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHashBucket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nvtabular/nvtabular/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdask\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDaskDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nvtabular/nvtabular/workflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnvtx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mannotate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhighlevelgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHighLevelGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dask'"
     ]
    }
   ],
   "source": [
    "numeric_columns, categorical_columns = get_columns(accelerated=ACCELERATED)\n",
    "columns = numeric_columns + categorical_columns\n",
    "\n",
    "# wrap categorical columns with an embedding to feed to DenseFeatures\n",
    "embedding_columns = [tf.feature_column.embedding_column(column, EMBEDDING_DIM) for column in categorical_columns]\n",
    "\n",
    "train_dataset = make_dataset(columns, accelerated=ACCELERATED)\n",
    "\n",
    "\n",
    "# now we build the model\n",
    "categorical_inputs = {\n",
    "    column.name: tf.keras.Input(name=column.name, shape=(1,), dtype=getattr(column, 'dtype', tf.int64)) for\n",
    "        column in categorical_columns\n",
    "}\n",
    "numeric_inputs = {\n",
    "    column.name: tf.keras.Input(name=column.name, shape=(1,), dtype=getattr(column, 'dtype', tf.int64)) for\n",
    "        column in numeric_columns\n",
    "}\n",
    "fm_x, dense_x = embed_inputs(numeric_inputs, categorical_inputs, accelerated=ACCELERATED)\n",
    "x = get_architecture()([dense_x, fm_x])\n",
    "\n",
    "inputs = list(categorical_inputs.values()) + list(numeric_inputs.values())\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "model.compile(optimizer, 'binary_crossentropy', metrics=[tf.keras.metrics.AUC(curve='ROC', name='auroc')])\n",
    "print(model.summary())\n",
    "\n",
    "run_name = 'accelerated' if ACCELERATED else 'native'\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        os.path.join(LOG_DIR, run_name),\n",
    "        update_freq=10,\n",
    "        profile_batch=100)\n",
    "]\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1,\n",
    "    steps_per_epoch=STEPS,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 821), started 0:23:50 ago. (Use '!kill 821' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3b3c30b5ebedd9b1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3b3c30b5ebedd9b1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "! mkdir -p logs/native logs/accelerated\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /home/docker/tensorflow/logs --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
