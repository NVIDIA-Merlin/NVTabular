{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = False\n",
    "evaluation_verbose = False\n",
    "OUTPUT_BUCKET_FOLDER = \"./preprocessed/\"\n",
    "DATA_BUCKET_FOLDER = \"/dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/conda/envs/rapids/lib/python3.7/site-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_NVVM=/usr/local/cuda/nvvm/lib64/libnvvm.so.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n",
      "/conda/envs/rapids/lib/python3.7/site-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_LIBDEVICE=/usr/local/cuda/nvvm/libdevice/.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import datetime\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import cudf\n",
    "import cupy\n",
    "from numba import cuda\n",
    "import rmm\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular import io as nvt_io\n",
    "from nvtabular import ops as ops\n",
    "from nvtabular.ops import Normalize, FillMissing, Categorify, LogOp, JoinExternal, Dropna, LambdaOp, JoinGroupby, Filter, HashBucket, FillMedian\n",
    "from nvtabular.column_similarity import ColumnSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38 µs, sys: 55 µs, total: 93 µs\n",
      "Wall time: 106 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import cupy\n",
    "from cudf import read_csv\n",
    "import rmm\n",
    "rmm.reinitialize(managed_memory=True)  \n",
    "# Merge all the CSV files together\n",
    "documents_meta = read_csv(DATA_BUCKET_FOLDER + 'documents_meta.csv')\n",
    "merged = (read_csv(DATA_BUCKET_FOLDER+'clicks_train.csv')\n",
    "             .merge(read_csv(DATA_BUCKET_FOLDER + 'events.csv'), on=\"display_id\", how=\"left\", suffixes=('', '_event'))\n",
    "             .merge(read_csv(DATA_BUCKET_FOLDER+'promoted_content.csv'), on=\"ad_id\", how=\"left\", suffixes=('', '_promo'))\n",
    "             .merge(documents_meta, on=\"document_id\", how=\"left\")\n",
    "             .merge(documents_meta, left_on=\"document_id_promo\", right_on=\"document_id\", how=\"left\", suffixes=('', \"_promo\"))\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a stratified split of the merged dataset into a training/validation dataset\n",
    "merged['day_event'] = (merged['timestamp'] / 1000 / 60 / 60 / 24).astype(int)\n",
    "random_state = cudf.Series(cupy.random.uniform(size=len(merged)))\n",
    "valid_set, train_set = merged.scatter_by_map(((merged.day_event <= 10) & (random_state > 0.2)).astype(int)) \n",
    "train_set.to_parquet(OUTPUT_BUCKET_FOLDER+\"train_gdf.parquet\", compression=None)\n",
    "valid_set.to_parquet(OUTPUT_BUCKET_FOLDER+\"valid_gdf.parquet\", compression=None)\n",
    "merged = train_set = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmm.reinitialize(managed_memory=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_categories_cudf = cudf.read_csv(DATA_BUCKET_FOLDER + 'documents_categories.csv')\n",
    "documents_topics_cudf = cudf.read_csv(DATA_BUCKET_FOLDER + 'documents_topics.csv')\n",
    "documents_entities_cudf = cudf.read_csv(DATA_BUCKET_FOLDER + 'documents_entities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in document categories/topics/entities as cupy sparse matrices\n",
    "def df_to_coo(df, row=\"document_id\", col=None, data=\"confidence_level\"):\n",
    "    return cupy.sparse.coo_matrix((df[data].values, (df[row].values, df[col].values)))\n",
    "\n",
    "categories = df_to_coo(documents_categories_cudf, col=\"category_id\")\n",
    "topics = df_to_coo(documents_topics_cudf, col=\"topic_id\")\n",
    "documents_entities_cudf['entity_id'] = documents_entities_cudf['entity_id'].astype(\"category\").cat.codes\n",
    "entities = df_to_coo(documents_entities_cudf, col=\"entity_id\")\n",
    "\n",
    "documents_categories_cudf=None\n",
    "documents_topics_cudf =None\n",
    "documents_entities_cudf=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "HASH_BUCKET_SIZES = {\n",
    "    'document_id': 300000,\n",
    "    'ad_id': 250000,\n",
    "    'document_id_promo': 100000,\n",
    "    'source_id_promo': 4000,\n",
    "    'source_id': 4000,\n",
    "    'advertiser_id': 2500,\n",
    "    'publisher_id_promo': 1000,\n",
    "    'publisher_id': 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP_DELTA = 1465876799998\n",
    "def calculate_delta(col,gdf):\n",
    "    delta = ((gdf['timestamp']+TIMESTAMP_DELTA).astype('datetime64[ms]') - col.astype('datetime64[ns]')).dt.days\n",
    "    delta = delta * (delta >=0) * (delta<=10*365)\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#The smoothed ctr function we adapt with the LambdaOP\n",
    "def smoothed_ctr(clicks, displays, prior_ctr=0.1, prior_weight=10):\n",
    "    return (clicks + prior_ctr * prior_weight) / (displays + prior_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "CATEGORICAL_COLUMNS =['display_id', 'ad_id', 'uuid', 'document_id','platform', 'geo_location', 'document_id_promo', 'campaign_id',\n",
    "       'advertiser_id', 'source_id', 'publisher_id', 'publish_time','source_id_promo', 'publisher_id_promo', 'publish_time_promo']\n",
    "CONTINUOUS_COLUMNS = ['timestamp']\n",
    "\n",
    "workflow = nvt.Workflow(\n",
    "    cat_names=CATEGORICAL_COLUMNS,\n",
    "    cont_names= CONTINUOUS_COLUMNS,\n",
    "    label_name=['clicked'])\n",
    "\n",
    "workflow.add_feature([\n",
    "    LambdaOp(\n",
    "        op_name='country',\n",
    "        f=lambda col, gdf: col.str.slice(0,2),\n",
    "        columns=['geo_location'], replace=False),\n",
    "    LambdaOp(\n",
    "        op_name='state',\n",
    "        f=lambda col, gdf: col.str.slice(0,5),\n",
    "        columns=['geo_location'],replace=False),\n",
    "    LambdaOp(\n",
    "        op_name='days_since_published',\n",
    "        f=calculate_delta,\n",
    "        columns=['publish_time','publish_time_promo'], replace=False),\n",
    "    \n",
    "    FillMedian(columns=['publish_time_days_since_published','publish_time_promo_days_since_published']),\n",
    "    \n",
    "    Dropna(columns=['geo_location', 'platform']),\n",
    "    \n",
    "    JoinGroupby(columns=['ad_id', 'source_id', 'document_id_promo', 'publisher_id', 'advertiser_id', 'campaign_id'], \n",
    "        cont_names=['clicked'],stats=['sum','count']),\n",
    "    \n",
    "    #calculate the smoothed ctr\n",
    "    LambdaOp(\n",
    "         op_name='ctr',\n",
    "         f=lambda col, gdf: (col + 0.1 * 10)/(gdf['ad_id_count']+10),\n",
    "         columns=['ad_id_clicked_sum'],replace=False),\n",
    "     LambdaOp(\n",
    "         op_name='ctr',\n",
    "         f=lambda col, gdf: (col + 0.1 * 10)/(gdf['source_id_count']+10),\n",
    "         columns=['source_id_clicked_sum'],replace=False),\n",
    "      LambdaOp(\n",
    "         op_name='ctr',\n",
    "         f=lambda col, gdf: (col + 0.1 * 10)/(gdf['document_id_promo_count']+10),\n",
    "         columns=['document_id_promo_clicked_sum'], replace=False),\n",
    "     LambdaOp(\n",
    "        op_name='ctr',\n",
    "        f=lambda col, gdf: (col + 0.1 * 10)/(gdf['publisher_id_count']+10),\n",
    "        columns=['publisher_id_clicked_sum'], replace=False),\n",
    "    LambdaOp(\n",
    "        op_name='ctr',\n",
    "        f=lambda col, gdf: (col + 0.1 * 10)/(gdf['advertiser_id_count']+10),\n",
    "        columns=['advertiser_id_clicked_sum'], replace=False),\n",
    "     LambdaOp(\n",
    "        op_name='ctr',\n",
    "        f=lambda col, gdf: (col + 0.1 * 10)/(gdf['campaign_id_count']+10),\n",
    "        columns=['campaign_id_clicked_sum'], replace=False),\n",
    "    \n",
    "    #take the log of the views and clicks\n",
    "    LogOp(columns=['ad_id_count', 'ad_id_clicked_sum','source_id_count', 'source_id_clicked_sum', \n",
    "       'document_id_promo_count', 'document_id_promo_clicked_sum','publisher_id_count','publisher_id_clicked_sum', \n",
    "       'advertiser_id_count', 'advertiser_id_clicked_sum', 'campaign_id_count', 'campaign_id_clicked_sum']),\n",
    "    \n",
    "    #normalize the views and clicks\n",
    "    Normalize(columns=['ad_id_count', 'ad_id_clicked_sum','source_id_count', 'source_id_clicked_sum', \n",
    "           'document_id_promo_count', 'document_id_promo_clicked_sum','publisher_id_count','publisher_id_clicked_sum', \n",
    "           'advertiser_id_count', 'advertiser_id_clicked_sum', 'campaign_id_count', 'campaign_id_clicked_sum']),\n",
    "     \n",
    "    Dropna(columns=['ad_id', 'source_id', 'document_id_promo', 'publisher_id', 'advertiser_id', 'campaign_id'])\n",
    "    ])\n",
    "\n",
    "workflow.add_preprocess([\n",
    "    Categorify(columns=['geo_location_country','geo_location','geo_location_state', 'uuid'],freq_threshold=10),\n",
    "    \n",
    "    HashBucket(HASH_BUCKET_SIZES)])\n",
    "\n",
    "op = ColumnSimilarity(\"doc_event_doc_ad_sim_categories\", \"document_id\", categories, \"document_id_promo\", metric='tfidf', on_device=False)\n",
    "workflow.add_feature(op)\n",
    "\n",
    "op = ColumnSimilarity(\"doc_event_doc_ad_sim_topics\", \"document_id\", topics, \"document_id_promo\", metric='tfidf', on_device=False)\n",
    "workflow.add_feature(op)\n",
    "\n",
    "op = ColumnSimilarity(\"doc_event_doc_ad_sim_entities\", \"document_id\", entities, \"document_id_promo\", metric='tfidf', on_device=False)\n",
    "workflow.add_feature(op)\n",
    "\n",
    "workflow.finalize()\n",
    "\n",
    "train_dataset = nvt.Dataset(OUTPUT_BUCKET_FOLDER+'train_gdf.parquet', part_mem_fraction=0.12)\n",
    "valid_dataset = nvt.Dataset(OUTPUT_BUCKET_FOLDER+'valid_gdf.parquet', part_mem_fraction=0.12)\n",
    "\n",
    "workflow.apply(train_dataset, record_stats=True, output_path='./preprocessed/train/', shuffle=True, out_files_per_proc=1)\n",
    "workflow.apply(valid_dataset, record_stats=False, output_path='./preprocessed/valid/', shuffle=False, out_files_per_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df= cudf.read_parquet('./preprocessed/train/*.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "rapids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
