{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVTabular demo on Outbrain Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we train TF Wide & Deep Learning framework using [Kaggle Outbrain dataset](https://www.kaggle.com/c/outbrain-click-prediction). In that competition, ‘Kagglers’ were challenged to predict on which ads and other forms of sponsored content its global users would click. One of  the top finishers' preprocessing and feature engineering pipeline is taken into consideration here, and this pipeline was restructured using NVTabular and cuDF. The Kaggle Outbrain click prediction challenge datasets can be downloaded from [here](https://www.kaggle.com/c/outbrain-click-prediction/data).\n",
    "\n",
    "[Wide & Deep Learning](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html) refers to a class of networks that use the output of two parts working in parallel - wide model and deep model - to make predictions using categorical and continuous inputs. The wide model is a generalized linear model of features together with their transforms. The deep model in this notebook is a series of 5 hidden MLP layers of 1024 neurons each beginning with a dense embedding of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import graphviz\n",
    "\n",
    "import cupy\n",
    "import cudf\n",
    "import rmm\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import Normalize, FillMedian, FillMissing, Categorify, LogOp, JoinExternal, Dropna, LambdaOp, JoinGroupby, HashBucket, TargetEncoding, get_embedding_sizes, Rename\n",
    "from nvtabular.ops.column_similarity import ColumnSimilarity\n",
    "from nvtabular import ColumnGroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set where the dataset should be saved once processed (OUTPUT_BUCKET_FOLDER), as well as where the dataset originally resides (DATA_BUCKET_FOLDER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_BUCKET_FOLDER = os.environ.get(\"OUTPUT_DATA_DIR\", \"./preprocessed/\")\n",
    "DATA_BUCKET_FOLDER = os.environ.get(\"INPUT_DATA_DIR\", \"./dataset/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we merge the component tables of our dataset into a single data frame, using [cuDF](https://github.com/rapidsai/cudf), which is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data. We do this because NVTabular applies a workflow to a single table. We also re-initialize managed memory. `rmm.reinitialize()` provides an easy way to initialize RMM (RAPIDS Memory Manager) with specific memory resource options across multiple devices. The reason we re-initialize managed memory here is to allow us to perform memory intensive merge operation. Note that dask-cudf can also be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from cudf import read_csv\n",
    "\n",
    "# use managed memory for device memory allocation\n",
    "rmm.reinitialize(managed_memory=True)  \n",
    "\n",
    "# Merge all the CSV files together\n",
    "documents_meta = read_csv(DATA_BUCKET_FOLDER + 'documents_meta.csv', na_values=['\\\\N', ''])\n",
    "merged = (read_csv(DATA_BUCKET_FOLDER+'clicks_train.csv', na_values=['\\\\N', ''])\n",
    "             .merge(read_csv(DATA_BUCKET_FOLDER + 'events.csv', na_values=['\\\\N', '']), on=\"display_id\", how=\"left\", suffixes=('', '_event'))\n",
    "             .merge(read_csv(DATA_BUCKET_FOLDER+'promoted_content.csv', na_values=['\\\\N', '']), on=\"ad_id\", how=\"left\", suffixes=('', '_promo'))\n",
    "             .merge(documents_meta, on=\"document_id\", how=\"left\")\n",
    "             .merge(documents_meta, left_on=\"document_id_promo\", right_on=\"document_id\", how=\"left\", suffixes=('', \"_promo\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the output directories to store the preprocessed parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_dir = os.path.join(OUTPUT_BUCKET_FOLDER, 'train/')\n",
    "output_valid_dir = os.path.join(OUTPUT_BUCKET_FOLDER, 'valid/')\n",
    "! mkdir -p $output_train_dir\n",
    "! mkdir -p $output_valid_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a time-stratified sample to create a validation set that is more recent, and save both our train and validation sets to parquet files to be read by NVTabular. Note that you should run the cell below only once, then save your `train` and `valid` data frames as parquet files. If you want to rerun the entire notebook, do not run this cell again to get consistent results, otherwise you might end up with a different train-validation split each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a stratified split of the merged dataset into a training/validation dataset\n",
    "merged['day_event'] = (merged['timestamp'] / 1000 / 60 / 60 / 24).astype(int)\n",
    "random_state = cudf.Series(cupy.random.uniform(size=len(merged)))\n",
    "valid_set, train_set = merged.scatter_by_map(((merged.day_event <= 10) & (random_state > 0.2)).astype(int)) \n",
    "train_set.to_parquet(OUTPUT_BUCKET_FOLDER+\"train_gdf.parquet\", compression=None)\n",
    "valid_set.to_parquet(OUTPUT_BUCKET_FOLDER+\"valid_gdf.parquet\", compression=None)\n",
    "merged = train_set = valid_set= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmm.reinitialize(managed_memory=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in three more cudf data frames, <i>documents categories</i>, <i>topics</i>, and <i>entities</i>, and use them to create sparse matricies in cupy. We will use these later to calculate cosine similarity between event document (landing page context) and ad document profile vectors (TF-IDF), i.e., how close in profile an ad is to the page that it is being displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_categories_cudf = cudf.read_csv(DATA_BUCKET_FOLDER + 'documents_categories.csv')\n",
    "documents_topics_cudf = cudf.read_csv(DATA_BUCKET_FOLDER + 'documents_topics.csv')\n",
    "documents_entities_cudf = cudf.read_csv(DATA_BUCKET_FOLDER + 'documents_entities.csv')\n",
    "# read in document categories/topics/entities as cupy sparse matrices\n",
    "def df_to_coo(df, row=\"document_id\", col=None, data=\"confidence_level\"):\n",
    "    return cupy.sparse.coo_matrix((df[data].values, (df[row].values, df[col].values)))\n",
    "\n",
    "categories = df_to_coo(documents_categories_cudf, col=\"category_id\")\n",
    "topics = df_to_coo(documents_topics_cudf, col=\"topic_id\")\n",
    "documents_entities_cudf['entity_id'] = documents_entities_cudf['entity_id'].astype(\"category\").cat.codes\n",
    "entities = df_to_coo(documents_entities_cudf, col=\"entity_id\")\n",
    "\n",
    "documents_categories_cudf=None\n",
    "documents_topics_cudf =None\n",
    "documents_entities_cudf=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we create a function that calculates the time difference between a specified time column (either publish_time or publish_time_promo) and timestamp. This is used to calculate <i>time elapsed since publication</i> between the landing page and the ad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save disk space, the timestamps in the entire dataset are relative to the first time in the dataset. \n",
    "#To recover the actual epoch time of the visit, we add 1465876799998 to the timestamp.\n",
    "TIMESTAMP_DELTA = 1465876799998\n",
    "\n",
    "def calculate_delta(col,gdf):\n",
    "    col.loc[col == \"\"] = None\n",
    "    col = col.astype('datetime64[ns]')\n",
    "    timestamp = (gdf['timestamp']+TIMESTAMP_DELTA).astype('datetime64[ms]')\n",
    "    delta = (timestamp - col).dt.days\n",
    "    delta = delta * (delta >=0) * (delta<=10*365)\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our datasets, sparse matrices and udf are created, we can begin laying the groundwork for NVTabular. NVTabular requires input features to be specified as either categorical or continuous upon workflow instantiation, so we define our continuous features and categorical features at this step. In this case, categorical columns are treated as integers, and numerical columns are treated as floats.\n",
    "\n",
    "Note that we specify our continuous and categorical columns using lists. Feature engineering and preprocessing often happens to sets of columns, so we adopt that method and require the user to specify continuous and categoricals along with the target as lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate NVTabular Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the groundwork laid, we can now initiate our workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our data still isn’t in a form that’s ideal for consumption by our W&D model. There are missing values, and our categorical variables are still represented by random, discrete identifiers, and need to be transformed into contiguous indices for embedding lookups. The distributions of our continuous variables are uncentered. We also would like to create new features that will help to increase the model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin to create and process features using NVTabular ops:\n",
    " * <i>geo_location_state</i> and <i>geo_location_country</i> are created by stripping geo_location using the `LambdaOp`\n",
    " * <i>publish_time_days_since_published</i> and <i>publish_time_promo_days_since_published</i> features are created using the `calculate_delta` function in a `LambdaOp`\n",
    " * Missing values are filled using median value depending on the feature using `FillMedian()`op\n",
    " * Continuous features are log transformed with the `LogOp()`.\n",
    " \n",
    "`Categorify` op is used for categorification, i.e. encoding of categorical features. Categorify op takes a param called `freq_threshold` which is used for frequency capping. This handy functionality will map all categories which occur in the dataset with some threshold level of infrequency to the _same_ index, keeping the model from overfitting to sparse signals. Below we set all frequency thresholds to 0, but one can easily create a frequency threshold dictionary, assign a custom threshold value for each categorical feature, and feed that dictionary into the `Categorify` op as `freq_threshold` param."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the important part of building recommender systems is to do feature engineering. As a very promising feature engineering technique, `Target Encoding` processes the categorical features and makes them easier accessible to the model during training and validation. *Target Encoding (TE)* has emerged as being both effective and efficient in many data science projects. For example, it is the major component of Nvidia Kaggle Grandmasters team’s [winning solution](https://medium.com/rapids-ai/winning-solution-of-recsys2020-challenge-gpu-accelerated-feature-engineering-and-training-for-cd67c5a87b1f) of [Recsys Challenge 2020](http://www.recsyschallenge.com/2020/). TE calculates the statistics from a target variable grouped by the unique values of one or more categorical features. For example in a binary classification problem, it calculates the probability that the target is true for each category value - a simple mean. In other words, for each distinct element in feature <b>$x$</b> we are going to compute the average of the corresponding values in target <i>y</i>. Then we are going to replace each $x_{i}$ with the corresponding mean value. For more details on TargetEncoding please visit [here](https://medium.com/rapids-ai/target-encoding-with-rapids-cuml-do-more-with-your-categorical-data-8c762c79e784) and [here](https://github.com/rapidsai/deeplearning/blob/main/RecSys2020Tutorial/03_3_TargetEncoding.ipynb).\n",
    "\n",
    "Here, we apply Target Encoding to certain categorical features with *kfold* of 5 and *smoothing* of 20 to avoid overfitting using [TargetEncoding op](https://github.com/NVIDIA/NVTabular/blob/a0141d0a710698470160bc2cbc42b18ce2d49133/nvtabular/ops/target_encoding.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo processing, apply two different lambda operators to the ‘geo_location’ column, and \n",
    "# extract the country/state from the geo_location value\n",
    "geo_location = ColumnGroup([\"geo_location\"])\n",
    "country = geo_location >> (lambda col: col.str.slice(0, 2)) >> Rename(postfix=\"_country\")\n",
    "state = geo_location >> (lambda col: col.str.slice(0, 5)) >> Rename(postfix=\"_state\")\n",
    " \n",
    "# process dates using the ‘calculate_delta’ function\n",
    "dates = ColumnGroup([\"publish_time\", \"publish_time_promo\"])\n",
    "date_features = dates >> LambdaOp(calculate_delta, dependency=[\"timestamp\"]) >> Rename(postfix=\"_since_published\") >> FillMedian >> LogOp\n",
    "\n",
    "# categoricals processing: categorify certain input columns as well as the geo features\n",
    "cats = ColumnGroup(['ad_id', 'document_id', 'platform', 'document_id_promo', 'campaign_id', 'advertiser_id', 'source_id', \n",
    "                    'publisher_id', 'source_id_promo', 'publisher_id_promo'])\n",
    "cat_features = geo_location + country + state + cats >> Categorify\n",
    "\n",
    " #Apply TargetEncoding to certain categoricals with kfold of 1 and smoothing of 20\n",
    "cats_te = ColumnGroup(['ad_id', 'document_id_promo', 'campaign_id', 'advertiser_id', 'source_id', 'publisher_id'])\n",
    "te_features = cats_te >> TargetEncoding(\"clicked\", kfold = 5, p_smooth = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our calculation graph wth the column groups we used and created so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"1121pt\" height=\"476pt\"\n",
       " viewBox=\"0.00 0.00 1121.28 476.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 472)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-472 1117.2826,-472 1117.2826,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"912.2416\" cy=\"-306\" rx=\"40.0939\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"912.2416\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Rename</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>13</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"962.2416\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"962.2416\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">+</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;13 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>0&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M924.3452,-288.5708C930.4525,-279.7763 937.9738,-268.9457 944.7016,-259.2577\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"947.7646,-260.983 950.5938,-250.7729 942.015,-256.9902 947.7646,-260.983\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>8</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"892.2416\" cy=\"-378\" rx=\"50.8918\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"892.2416\" y=\"-374.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">LambdaOp</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>8&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M897.2885,-359.8314C899.4843,-351.9266 902.1063,-342.4872 904.5371,-333.7365\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"907.9615,-334.4854 907.2657,-323.9134 901.2169,-332.6118 907.9615,-334.4854\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"94.2416\" cy=\"-450\" rx=\"94.4839\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"94.2416\" y=\"-446.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">input cols=[timestamp]</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>14</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"241.2416\" cy=\"-378\" rx=\"50.8918\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"241.2416\" y=\"-374.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">LambdaOp</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;14 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>1&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M128.7066,-433.1192C150.7837,-422.3059 179.48,-408.2506 202.3688,-397.0397\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"204.0253,-400.1258 211.4663,-392.5838 200.9461,-393.8393 204.0253,-400.1258\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"342.2416\" cy=\"-162\" rx=\"81.7856\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"342.2416\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">input cols=[clicked]</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>18</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"538.2416\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"538.2416\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">+</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;18 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>2&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M384.7707,-146.3771C420.7884,-133.1461 471.7116,-114.4396 504.8528,-102.2653\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"506.351,-105.4437 514.5309,-98.7101 503.9373,-98.873 506.351,-105.4437\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"241.2416\" cy=\"-306\" rx=\"40.0939\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"241.2416\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Rename</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>11</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"241.2416\" cy=\"-234\" rx=\"50.0912\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"241.2416\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">FillMedian</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;11 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>3&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M241.2416,-287.8314C241.2416,-280.131 241.2416,-270.9743 241.2416,-262.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"244.7417,-262.4132 241.2416,-252.4133 237.7417,-262.4133 244.7417,-262.4132\"/>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;3 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>14&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M241.2416,-359.8314C241.2416,-352.131 241.2416,-342.9743 241.2416,-334.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"244.7417,-334.4132 241.2416,-324.4133 237.7417,-334.4133 244.7417,-334.4132\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"522.2416\" cy=\"-234\" rx=\"213.3623\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"522.2416\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">input cols=[ad_id, document_id_promo, campaign_id...]</text>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>15</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"599.2416\" cy=\"-162\" rx=\"67.6881\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"599.2416\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">TargetEncoding</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;15 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>4&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M541.672,-215.8314C551.1866,-206.9346 562.7801,-196.0939 573.0628,-186.4789\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"575.7006,-188.8042 580.6144,-179.4177 570.9196,-183.6912 575.7006,-188.8042\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"388.2416\" cy=\"-450\" rx=\"181.9677\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"388.2416\" y=\"-446.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">input cols=[publish_time, publish_time_promo]</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;14 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>5&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M351.9046,-432.2022C330.1816,-421.5624 302.5814,-408.044 280.3793,-397.1695\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"281.7476,-393.9424 271.2275,-392.6869 278.6685,-400.2289 281.7476,-393.9424\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>6</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"683.2416\" cy=\"-306\" rx=\"170.0701\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"683.2416\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">input cols=[ad_id, document_id, platform...]</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;13 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>6&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M776.1858,-290.8847C822.0977,-282.0307 877.8796,-269.1705 926.2416,-252 928.1184,-251.3337 930.0234,-250.5926 931.9272,-249.8018\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"933.64,-252.868 941.2989,-245.5473 930.7464,-246.4941 933.64,-252.868\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>7</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"835.2416\" cy=\"-234\" rx=\"81.7856\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"835.2416\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">input cols=[clicked]</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;15 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>7&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M787.1991,-219.3429C748.4778,-207.5297 694.0272,-190.9176 653.6934,-178.6124\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"654.6073,-175.232 644.0212,-175.6616 652.5647,-181.9274 654.6073,-175.232\"/>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>17</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1011.2416\" cy=\"-450\" rx=\"102.0819\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1011.2416\" y=\"-446.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">input cols=[geo_location]</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;8 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>17&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M982.4351,-432.5708C965.6825,-422.4348 944.4596,-409.594 926.8098,-398.9152\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"928.3161,-395.7358 917.9484,-393.5537 924.6924,-401.7249 928.3161,-395.7358\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>10</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1011.2416\" cy=\"-378\" rx=\"50.8918\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1011.2416\" y=\"-374.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">LambdaOp</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;10 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>17&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1011.2416,-431.8314C1011.2416,-424.131 1011.2416,-414.9743 1011.2416,-406.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1014.7417,-406.4132 1011.2416,-396.4133 1007.7417,-406.4133 1014.7417,-406.4132\"/>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;13 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>17&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1039.5141,-432.5382C1051.6885,-423.3299 1064.6279,-410.8503 1071.2416,-396 1081.3561,-373.2894 1063.6222,-292.8431 1060.2416,-288 1045.2002,-266.4508 1018.6892,-252.5971 997.0309,-244.3232\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"998.0746,-240.9794 987.48,-240.9067 995.7169,-247.5704 998.0746,-240.9794\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>9</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"478.2416\" cy=\"-162\" rx=\"35.9954\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"478.2416\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">LogOp</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;18 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>9&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M492.1577,-145.3008C499.9154,-135.9915 509.7104,-124.2374 518.2733,-113.9621\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"521.1007,-116.0363 524.8138,-106.1134 515.7232,-111.555 521.1007,-116.0363\"/>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;9 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>11&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M279.2745,-222.2028C286.2377,-220.0981 293.4493,-217.9548 300.2416,-216 359.0911,-199.063 375.159,-199.4044 433.2416,-180 435.9351,-179.1002 438.7006,-178.1247 441.4721,-177.1093\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"442.7198,-180.3794 450.8235,-173.5543 440.2324,-173.8362 442.7198,-180.3794\"/>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1011.2416\" cy=\"-306\" rx=\"40.0939\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1011.2416\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Rename</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>10&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1011.2416,-359.8314C1011.2416,-352.131 1011.2416,-342.9743 1011.2416,-334.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1014.7417,-334.4132 1011.2416,-324.4133 1007.7417,-334.4133 1014.7417,-334.4132\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>12</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"886.2416\" cy=\"-162\" rx=\"48.1917\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"886.2416\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Categorify</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;18 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>12&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M843.9179,-153.2434C774.9405,-138.9722 639.4774,-110.9453 574.5822,-97.5187\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"574.9256,-94.0157 564.4238,-95.417 573.5073,-100.8705 574.9256,-94.0157\"/>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;12 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>13&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M946.4944,-219.0816C936.3653,-209.4856 922.9793,-196.8041 911.3974,-185.8317\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"913.7659,-183.2544 904.0993,-178.9178 908.9517,-188.3361 913.7659,-183.2544\"/>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;13 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>16&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M999.3801,-288.5708C993.395,-279.7763 986.0241,-268.9457 979.4309,-259.2577\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"982.1763,-257.0709 973.6565,-250.7729 976.3893,-261.0093 982.1763,-257.0709\"/>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;18 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>15&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M584.163,-144.2022C576.4453,-135.0928 566.9398,-123.8733 558.5831,-114.0096\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"561.0327,-111.4864 551.898,-106.119 555.6917,-116.0113 561.0327,-111.4864\"/>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>19</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"538.2416\" cy=\"-18\" rx=\"365.8351\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"538.2416\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">output cols=[publish_time_since_published, publish_time_promo_since_published, geo_location...]</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;19 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>18&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M538.2416,-71.8314C538.2416,-64.131 538.2416,-54.9743 538.2416,-46.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"541.7417,-46.4132 538.2416,-36.4133 534.7417,-46.4133 541.7417,-46.4132\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fd2652e0b50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = date_features + cat_features + te_features + \"clicked\"\n",
    "(output).graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A user might sometimes be interested to continue reading about the same topics of the current page. Computing the similarity between the textual content of the current page and the pages linked to the displayed ads, can be a relevant feature for a model that predicts which ad the user would click next. A simple, yet effective way to compute the similarity between documents is generating the TF-IDF vectors for each of them, which captures their most relevant terms, and then computing the cosine similarity between those vectors.\n",
    " \n",
    "Below, we calculate <i>doc_event_doc_ad_sim_categories</i>, <i>topics</i>, and <i>entities</i> using the `ColumnSimilarity` op, which utilizes the sparse categories, topics, and entities matrices that were created above to calculate landing page similarity for categories, topics, and entities. We calculate Cosine similarity between event doc (landing page) and ad doc aspects vectors (TF-IDF). Creating these extra features help to improve model accuracy and predictability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ronayak/ronaya/NVTabular/nvtabular/ops/column_similarity.py:226: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(N / np.bincount(X.col))\n"
     ]
    }
   ],
   "source": [
    "sim_features_categ = [[\"document_id\", \"document_id_promo\"]] >> ColumnSimilarity(categories, metric='tfidf', on_device=False) >> Rename(postfix=\"_categories\")\n",
    "sim_features_topics= [[\"document_id\", \"document_id_promo\"]] >> ColumnSimilarity(topics, metric='tfidf', on_device=False) >> Rename(postfix=\"_topics\")\n",
    "sim_features_entities= [[\"document_id\", \"document_id_promo\"]] >> ColumnSimilarity(entities, metric='tfidf', on_device=False) >> Rename(postfix=\"_entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = nvt.Workflow(date_features + cat_features + te_features + sim_features_categ + sim_features_topics +  sim_features_entities + \"clicked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['publish_time_since_published',\n",
       " 'publish_time_promo_since_published',\n",
       " 'geo_location',\n",
       " 'geo_location_country',\n",
       " 'geo_location_state',\n",
       " 'ad_id',\n",
       " 'document_id',\n",
       " 'platform',\n",
       " 'document_id_promo',\n",
       " 'campaign_id',\n",
       " 'advertiser_id',\n",
       " 'source_id',\n",
       " 'publisher_id',\n",
       " 'source_id_promo',\n",
       " 'publisher_id_promo',\n",
       " 'TE_ad_id_clicked',\n",
       " 'TE_document_id_promo_clicked',\n",
       " 'TE_campaign_id_clicked',\n",
       " 'TE_advertiser_id_clicked',\n",
       " 'TE_source_id_clicked',\n",
       " 'TE_publisher_id_clicked',\n",
       " 'document_id_document_id_promo_sim_categories',\n",
       " 'document_id_document_id_promo_sim_topics',\n",
       " 'document_id_document_id_promo_sim_entities',\n",
       " 'clicked']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.column_group.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create an NVTabular Dataset object both for train and validation sets. We calculate statistics for this workflow on the input dataset, i.e. on our training set, using the `workflow.fit()` method so that our <i>Workflow</i> can use these stats to transform any given input. When our <i>Workflow</i> transorms our datasets and, we also save the results out to parquet files for fast reading at train time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ad_id': './/categories/cat_stats.ad_id.parquet', 'document_id_promo': './/categories/cat_stats.document_id_promo.parquet', 'campaign_id': './/categories/cat_stats.campaign_id.parquet', 'advertiser_id': './/categories/cat_stats.advertiser_id.parquet', 'source_id': './/categories/cat_stats.source_id.parquet', 'publisher_id': './/categories/cat_stats.publisher_id.parquet', '__fold___ad_id': './/categories/cat_stats.__fold___ad_id.parquet', '__fold___document_id_promo': './/categories/cat_stats.__fold___document_id_promo.parquet', '__fold___campaign_id': './/categories/cat_stats.__fold___campaign_id.parquet', '__fold___advertiser_id': './/categories/cat_stats.__fold___advertiser_id.parquet', '__fold___source_id': './/categories/cat_stats.__fold___source_id.parquet', '__fold___publisher_id': './/categories/cat_stats.__fold___publisher_id.parquet'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ronayak/miniconda3/envs/rapids-0.16/lib/python3.7/site-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\u001b[0m\n",
      "  warnings.warn(problem)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = nvt.Dataset(OUTPUT_BUCKET_FOLDER+'train_gdf.parquet', part_mem_fraction=0.12)\n",
    "valid_dataset = nvt.Dataset(OUTPUT_BUCKET_FOLDER+'valid_gdf.parquet', part_mem_fraction=0.12)\n",
    "\n",
    "# Calculate statistics on the training set\n",
    "workflow.fit(train_dataset)\n",
    " \n",
    "# use the calculated statistics to transform the train/valid datasets and write out each as \n",
    "# parquet\n",
    "workflow.transform(train_dataset).to_parquet(output_path=output_train_dir, shuffle=nvt.io.Shuffle.PER_PARTITION, out_files_per_proc=5)\n",
    "workflow.transform(valid_dataset).to_parquet(output_path=output_valid_dir, shuffle=None, out_files_per_proc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the stats from the workflow and load it anytime, so we can run training without doing preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMBEDDING_TABLE_SHAPES defines the size of the embedding tables that our model will use to map categorical outputs from NVTabular into numeric dense inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ad_id': (418402, 16),\n",
       " 'advertiser_id': (4060, 16),\n",
       " 'campaign_id': (31390, 16),\n",
       " 'document_id': (693454, 16),\n",
       " 'document_id_promo': (143973, 16),\n",
       " 'geo_location': (2886, 16),\n",
       " 'geo_location_country': (232, 16),\n",
       " 'geo_location_state': (2486, 16),\n",
       " 'platform': (5, 4),\n",
       " 'publisher_id': (483, 16),\n",
       " 'publisher_id_promo': (804, 16),\n",
       " 'source_id': (4740, 16),\n",
       " 'source_id_promo': (6825, 16)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we use default embedding size rule defined within NVTabular.\n",
    "#EMBEDDING_TABLE_SHAPES = nvt.ops.get_embedding_sizes(workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_TABLE_SHAPES = {'ad_id': (418402, 16),\n",
    " 'advertiser_id': (4060, 16),\n",
    " 'campaign_id': (31390, 16),\n",
    " 'document_id': (693454, 16),\n",
    " 'document_id_promo': (143973, 16),\n",
    " 'geo_location': (2886, 16),\n",
    " 'geo_location_country': (232, 16),\n",
    " 'geo_location_state': (2486, 16),\n",
    " 'platform': (5, 4),\n",
    " 'publisher_id': (483, 16),\n",
    " 'publisher_id_promo': (804, 16),\n",
    " 'source_id': (4740, 16),\n",
    " 'source_id_promo': (6825, 16)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS =['geo_location', 'geo_location_country', 'geo_location_state', 'ad_id','document_id', 'platform', 'document_id_promo', \n",
    "                      'campaign_id','advertiser_id', 'source_id', 'publisher_id', 'source_id_promo', 'publisher_id_promo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select certain categorical and numerical features that are processed and generated via the NVTabular workflow to train our W&D TF model. Below, we'll create NUMERIC_COLUMNS list to feed our W&D model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLUMNS = ['document_id_document_id_promo_sim_categories',\n",
    "'document_id_document_id_promo_sim_topics',\n",
    "'document_id_document_id_promo_sim_entities',\n",
    "'publish_time_since_published', \n",
    "'publish_time_promo_since_published',\n",
    "'TE_ad_id_clicked',\n",
    "'TE_document_id_promo_clicked', \n",
    "'TE_campaign_id_clicked',\n",
    "'TE_advertiser_id_clicked', \n",
    "'TE_source_id_clicked',\n",
    "'TE_publisher_id_clicked',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a TF W&D Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# we can control how much memory to give tensorflow with this environment variable\n",
    "# IMPORTANT: make sure you do this before you initialize TF's runtime, otherwise\n",
    "# TF will have claimed all free GPU memory\n",
    "#os.environ['TF_MEMORY_ALLOCATION'] = \"0.8\" # fraction of free memory\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "from tensorflow.python.feature_column import feature_column_v2 as fc\n",
    "\n",
    "from nvtabular.loader.tensorflow import KerasSequenceLoader, KerasSequenceValidater\n",
    "from nvtabular.framework_utils.tensorflow import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create tensorflow feature columns corresponding to each feature of the model input. If you're using NVTabular with TensorFlow feature_columns, you should only be using `tf.feature_column.categorical_column_with_identity` for categorical features, since any other transformation (categorification and/or hashing) should be handled in NVTabular on the GPU. This feature column is passed to the wide portion of the model. If a categorical column corresponds to an embedding table, it is wrapped with an embedding_column feature_column, if it does not correspond to an embedding table, it is wrapped as an indicator column. The wrapped column is passed to the deep portion of the model. Continuous columns are passed to both the wide and deep portions of the model after being encapsulated as a `numeric_column`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_columns():\n",
    "    wide_columns, deep_columns = [], []\n",
    "\n",
    "    for column_name in CATEGORICAL_COLUMNS:\n",
    "        if column_name in EMBEDDING_TABLE_SHAPES: # Changing hashing to identity + adding modulo to dataloader\n",
    "            categorical_column = tf.feature_column.categorical_column_with_identity(\n",
    "                column_name, num_buckets=EMBEDDING_TABLE_SHAPES[column_name][0])\n",
    "        else:\n",
    "            raise ValueError(f'Unexpected categorical column found {column_name}')\n",
    "\n",
    "        if column_name in EMBEDDING_TABLE_SHAPES:\n",
    "            wrapped_column = tf.feature_column.embedding_column(\n",
    "                categorical_column,\n",
    "                dimension=EMBEDDING_TABLE_SHAPES[column_name][1],\n",
    "                combiner='mean')\n",
    "        else:\n",
    "            wrapped_column = tf.feature_column.indicator_column(categorical_column)\n",
    "            \n",
    "        wide_columns.append(categorical_column)\n",
    "        deep_columns.append(wrapped_column)\n",
    "    \n",
    "    numerics = [tf.feature_column.numeric_column(column_name, shape=(1,),dtype=tf.float32) \n",
    "                for column_name in NUMERIC_COLUMNS]\n",
    "    \n",
    "    wide_columns.extend(numerics)\n",
    "    deep_columns.extend(numerics)\n",
    "       \n",
    "    return wide_columns, deep_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the layer shape and dropout probability for the deep portion of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_hidden_units=[1024, 512, 256]\n",
    "deep_dropout=.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An input is created for each feature column, with a datatype of either tf.float32 for continuous values, or tf.int32 for categorical values. To implement the wide model, for categorical inputs, we embed them to a dimension of one, and sum them with the results of applying a dense layer with output dimension one, effectively weighting and summing each of the inputs. For the deep model, we embed our categorical columns according to the feature columns we defined earlier, and concatenate the newly dense features with our dense continuous features, which we pass to our deep model, which by default is a 5 layer MLP with internal dimension of 1024 neurons for each layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_columns, deep_columns = get_feature_columns()\n",
    "\n",
    "wide_weighted_outputs = []  # a list of (batch_size, 1) contributors to the linear weighted sum\n",
    "numeric_dense_inputs = []  # NumericColumn inputs; to be concatenated and then fed to a dense layer\n",
    "wide_columns_dict = {}  # key : column\n",
    "deep_columns_dict = {}  # key : column\n",
    "features = {}  # tf.keras.Input placeholders for each feature to be used\n",
    "\n",
    "# construct input placeholders for wide features\n",
    "for col in wide_columns:\n",
    "    features[col.key] = tf.keras.Input(shape=(1,),\n",
    "                                       batch_size=None, \n",
    "                                       name=col.key,\n",
    "                                       dtype=tf.float32 if col.key in NUMERIC_COLUMNS else tf.int32,\n",
    "                                       sparse=False)\n",
    "    wide_columns_dict[col.key] = col\n",
    "for col in deep_columns:\n",
    "    is_embedding_column = ('key' not in dir(col))\n",
    "    key = col.categorical_column.key if is_embedding_column else col.key\n",
    "\n",
    "    if key not in features:\n",
    "        features[key] = tf.keras.Input(shape=(1,), \n",
    "                                       batch_size=None, \n",
    "                                       name=key, \n",
    "                                       dtype=tf.float32 if col.key in NUMERIC_COLUMNS else tf.int32,\n",
    "                                       sparse=False)\n",
    "    deep_columns_dict[key] = col\n",
    "\n",
    "for key in wide_columns_dict:\n",
    "    if key in EMBEDDING_TABLE_SHAPES: \n",
    "        wide_weighted_outputs.append(tf.keras.layers.Flatten()(tf.keras.layers.Embedding(\n",
    "            EMBEDDING_TABLE_SHAPES[key][0], 1, input_length=1)(features[key])))\n",
    "    else:\n",
    "        numeric_dense_inputs.append(features[key])\n",
    "\n",
    "categorical_output_contrib = tf.keras.layers.add(wide_weighted_outputs,\n",
    "                                                 name='categorical_output')\n",
    "numeric_dense_tensor = tf.keras.layers.concatenate(\n",
    "    numeric_dense_inputs, name='numeric_dense')\n",
    "deep_columns = list(deep_columns_dict.values())\n",
    "\n",
    "dnn = layers.DenseFeatures(deep_columns, name='deep_embedded')(features)\n",
    "for unit_size in deep_hidden_units:\n",
    "    dnn = tf.keras.layers.Dense(units=unit_size,activation='relu')(dnn)\n",
    "    dnn = tf.keras.layers.Dropout(rate=deep_dropout)(dnn)\n",
    "    dnn = tf.keras.layers.BatchNormalization(momentum=.999)(dnn)\n",
    "dnn = tf.keras.layers.Dense(units=1)(dnn)\n",
    "dnn_model = tf.keras.Model(inputs=features,\n",
    "                           outputs=dnn)\n",
    "linear_output = categorical_output_contrib + tf.keras.layers.Dense(1)(numeric_dense_tensor)\n",
    "\n",
    "linear_model = tf.keras.Model(inputs=features,\n",
    "                              outputs=linear_output)\n",
    "\n",
    "wide_and_deep_model = tf.keras.experimental.WideDeepModel(\n",
    "    linear_model, dnn_model, activation='sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the datasets that will be used to ingest data into our model. In this case, the NVTabular dataloaders take a set of parquet files generated by NVTabular as input, and are capable of accelerated throughput. The [KerasSequenceLoader](https://github.com/NVIDIA/NVTabular/blob/9aa70caa1dfb5d2fd694cad535def1e470d37b29/nvtabular/loader/tensorflow.py#L89) manages shuffling by loading in chunks of data from different parts of the full dataset, concatenating them and then shuffling, then iterating through this super-chunk sequentially in batches. The number of \"parts\" of the dataset that get sample, or \"partitions\", is controlled by the <i>parts_per_chunk</i> parameter, while the size of each one of these parts is controlled by the <i>buffer_size</i> parameter, which refers to a fraction of available GPU memory. Using more chunks leads to better randomness, especially at the epoch level where physically disparate samples can be brought into the same batch, but can impact throughput if you use too many.\n",
    "\n",
    "The validation process gets slightly complicated by the fact that <i>model.fit</i> doesn't accept Keras Sequence objects as validation data. To support this, we also define a [KerasSequenceValidater](https://github.com/NVIDIA/NVTabular/blob/9aa70caa1dfb5d2fd694cad535def1e470d37b29/nvtabular/loader/tensorflow.py#L351), a lightweight Keras callback to handle validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is preprocessed and saved out, we can leverage datasets to read through the preprocessed parquet files in an online fashion to train neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATHS = sorted(glob.glob('./preprocessed/train/*.parquet'))\n",
    "VALID_PATHS = sorted(glob.glob('./preprocessed/valid/*.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_tf = KerasSequenceLoader(\n",
    "    TRAIN_PATHS, # you could also use a glob pattern\n",
    "    batch_size=131072,\n",
    "    label_names=['clicked'],\n",
    "    cat_names=CATEGORICAL_COLUMNS,\n",
    "    cont_names=NUMERIC_COLUMNS,\n",
    "    engine='parquet',\n",
    "    shuffle=True,\n",
    "    buffer_size=0.06, # how many batches to load at once\n",
    "    parts_per_chunk=1\n",
    ")\n",
    "\n",
    "valid_dataset_tf = KerasSequenceLoader(\n",
    "    VALID_PATHS, # you could also use a glob pattern\n",
    "    batch_size=131072,\n",
    "    label_names=['clicked'],\n",
    "    cat_names=CATEGORICAL_COLUMNS,\n",
    "    cont_names=NUMERIC_COLUMNS,\n",
    "    engine='parquet',\n",
    "    shuffle=False,\n",
    "    buffer_size=0.06,\n",
    "    parts_per_chunk=1\n",
    ")\n",
    "validation_callback = KerasSequenceValidater(valid_dataset_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wide portion of the model is optimized using the <i>Follow The Regularized Leader (FTRL)</i> algorithm, while the deep portion of the model is optimized using <i>Adam</i> optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_optimizer = tf.keras.optimizers.Ftrl(\n",
    "        learning_rate=0.1,\n",
    ")\n",
    "\n",
    "deep_optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.2\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compile our model with our dual optimizers and binary cross-entropy loss, and train our model for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_and_deep_model.compile(\n",
    "    optimizer=[wide_optimizer, deep_optimizer],\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()],\n",
    "    experimental_run_tf_function=False\n",
    ")\n",
    "history = wide_and_deep_model.fit(train_dataset_tf, callbacks=[validation_callback], epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
