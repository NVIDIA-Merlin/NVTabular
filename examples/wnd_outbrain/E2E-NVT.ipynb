{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import cupy\n",
    "from numba import cuda\n",
    "import rmm\n",
    "\n",
    "import nvtabular as nvt\n",
    "\n",
    "from nvtabular.ops import Normalize, FillMedian,FillMissing, Categorify, LogOp, JoinExternal, Dropna, LambdaOp, JoinGroupby, HashBucket\n",
    "from nvtabular.column_similarity import ColumnSimilarity\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.python.feature_column import feature_column_v2 as fc\n",
    "from nvtabular.tf_dataloader import KerasSequenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation = False\n",
    "# evaluation_verbose = False\n",
    "OUTPUT_BUCKET_FOLDER = \"./preprocessed/\"\n",
    "DATA_BUCKET_FOLDER = \"/dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "from cudf import read_csv\n",
    "rmm.reinitialize(managed_memory=True)  \n",
    "# Merge all the CSV files together\n",
    "documents_meta = read_csv(DATA_BUCKET_FOLDER + 'documents_meta.csv', na_values=['\\\\N', ''])\n",
    "merged = (read_csv(DATA_BUCKET_FOLDER+'clicks_train.csv', na_values=['\\\\N', ''])\n",
    "             .merge(read_csv(DATA_BUCKET_FOLDER + 'events.csv', na_values=['\\\\N', '']), on=\"display_id\", how=\"left\", suffixes=('', '_event'))\n",
    "             .merge(read_csv(DATA_BUCKET_FOLDER+'promoted_content.csv', na_values=['\\\\N', '']), on=\"ad_id\", how=\"left\", suffixes=('', '_promo'))\n",
    "             .merge(documents_meta, on=\"document_id\", how=\"left\")\n",
    "             .merge(documents_meta, left_on=\"document_id_promo\", right_on=\"document_id\", how=\"left\", suffixes=('', \"_promo\"))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_dir = os.path.join(OUTPUT_BUCKET_FOLDER, 'train/')\n",
    "output_valid_dir = os.path.join(OUTPUT_BUCKET_FOLDER, 'valid/')\n",
    "! mkdir -p $output_train_dir\n",
    "! mkdir -p $output_valid_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do a stratified split of the merged dataset into a training/validation dataset\n",
    "merged['day_event'] = (merged['timestamp'] / 1000 / 60 / 60 / 24).astype(int)\n",
    "random_state = cudf.Series(cupy.random.uniform(size=len(merged)))\n",
    "valid_set, train_set = merged.scatter_by_map(((merged.day_event <= 10) & (random_state > 0.2)).astype(int)) \n",
    "train_set.to_parquet(OUTPUT_BUCKET_FOLDER+\"train_gdf.parquet\", compression=None)\n",
    "valid_set.to_parquet(OUTPUT_BUCKET_FOLDER+\"valid_gdf.parquet\", compression=None)\n",
    "merged = train_set = valid_set= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmm.reinitialize(managed_memory=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_categories_cudf = cudf.read_csv(DATA_BUCKET_FOLDER + 'documents_categories.csv')\n",
    "documents_topics_cudf = cudf.read_csv(DATA_BUCKET_FOLDER + 'documents_topics.csv')\n",
    "documents_entities_cudf = cudf.read_csv(DATA_BUCKET_FOLDER + 'documents_entities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in document categories/topics/entities as cupy sparse matrices\n",
    "def df_to_coo(df, row=\"document_id\", col=None, data=\"confidence_level\"):\n",
    "    return cupy.sparse.coo_matrix((df[data].values, (df[row].values, df[col].values)))\n",
    "\n",
    "categories = df_to_coo(documents_categories_cudf, col=\"category_id\")\n",
    "topics = df_to_coo(documents_topics_cudf, col=\"topic_id\")\n",
    "documents_entities_cudf['entity_id'] = documents_entities_cudf['entity_id'].astype(\"category\").cat.codes\n",
    "entities = df_to_coo(documents_entities_cudf, col=\"entity_id\")\n",
    "\n",
    "documents_categories_cudf=None\n",
    "documents_topics_cudf =None\n",
    "documents_entities_cudf=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "HASH_BUCKET_SIZES = {\n",
    "    'document_id': 300000,\n",
    "    'ad_id': 250000,\n",
    "    'document_id_promo': 100000,\n",
    "    'source_id_promo': 4000,\n",
    "    'source_id': 4000,\n",
    "    'advertiser_id': 2500,\n",
    "    'publisher_id_promo': 1000,\n",
    "    'publisher_id': 1000\n",
    "}\n",
    "IDENTITY_NUM_BUCKETS = {\n",
    "    'platform': 5,\n",
    "    'geo_location':2988,\n",
    "    'geo_location_country':230,\n",
    "    'geo_location_state': 2500\n",
    "}\n",
    "\n",
    "EMBEDDING_DIMENSIONS = {\n",
    "    'document_id': 128,\n",
    "    'ad_id': 128,\n",
    "    'document_id_promo': 128,\n",
    "    'source_id': 64,\n",
    "    'source_id_promo': 64,\n",
    "    'geo_location': 64,\n",
    "    'advertiser_id': 64,\n",
    "    'geo_location_state': 64,\n",
    "    'publisher_id_promo': 64,\n",
    "    'publisher_id': 64,\n",
    "    'geo_location_country': 64,\n",
    "}\n",
    "dtypes = {\n",
    "    'document_id':np.int32,\n",
    "    'document_id_promo':np.int32,\n",
    "    'source_id':np.int32,\n",
    "    'source_id_promo':np.int32,\n",
    "    'geo_location':np.int32,\n",
    "    'geo_location_country':np.int32,\n",
    "    'geo_location_state':np.int32,\n",
    "    'publisher_id':np.int32,\n",
    "    'platform':np.int32,\n",
    "    'document_id_promo_clicked_sum_ctr':np.float32,\n",
    "    'publisher_id_clicked_sum_ctr':np.float32,\n",
    "    'source_id_clicked_sum_ctr':np.float32,\n",
    "    'document_id_promo_count':np.float32,\n",
    "    'publish_time_days_since_published':np.float32,\n",
    "    'ad_id':np.int32,\n",
    "    'source_id_promo':np.int32,\n",
    "    'advertiser_id':np.int32,\n",
    "    'publisher_id_promo':np.int32,\n",
    "    'ad_id_clicked_sum_ctr':np.float32,\n",
    "    'advertiser_id_clicked_sum_ctr':np.float32,\n",
    "    'campaign_id_clicked_sum_ctr':np.float32,\n",
    "    'ad_id_count':np.float32,\n",
    "    'publish_time_promo_days_since_published':np.float32,\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#The smoothed ctr function we adapt with the LambdaOP\n",
    "def smoothed_ctr(clicks, displays, prior_ctr=0.1, prior_weight=10):\n",
    "    return (clicks + prior_ctr * prior_weight) / (displays + prior_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS =['display_id', 'ad_id', 'uuid', 'document_id','platform', 'geo_location', 'document_id_promo', 'campaign_id','advertiser_id', \n",
    "                      'source_id', 'publisher_id', 'publish_time','source_id_promo', 'publisher_id_promo', 'publish_time_promo', 'day_event']\n",
    "CONTINUOUS_COLUMNS = ['timestamp']\n",
    "\n",
    "TIMESTAMP_DELTA = 1465876799998\n",
    "\n",
    "def calculate_delta(col,gdf):\n",
    "    col.loc[col == \"\"] = None\n",
    "    col = col.astype('datetime64[ns]')\n",
    "    timestamp = (gdf['timestamp']+TIMESTAMP_DELTA).astype('datetime64[ms]')\n",
    "    delta = (timestamp - col).dt.days\n",
    "    delta = delta * (delta >=0) * (delta<=10*365)\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nvtabular/nvtabular/column_similarity.py:243: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(N / np.bincount(X.col))\n"
     ]
    }
   ],
   "source": [
    "workflow = nvt.Workflow(\n",
    "    cat_names=CATEGORICAL_COLUMNS,\n",
    "    cont_names= CONTINUOUS_COLUMNS,\n",
    "    label_name=['clicked'])\n",
    "\n",
    "workflow.add_feature([\n",
    "    LambdaOp(\n",
    "        op_name='country',\n",
    "        f=lambda col, gdf: col.str.slice(0,2),\n",
    "        columns=['geo_location'], replace=False),\n",
    "    LambdaOp(\n",
    "        op_name='state',\n",
    "        f=lambda col, gdf: col.str.slice(0,5),\n",
    "        columns=['geo_location'],replace=False),\n",
    "    LambdaOp(\n",
    "        op_name='days_since_published',\n",
    "        f=calculate_delta,\n",
    "        columns=['publish_time','publish_time_promo'], replace=False),\n",
    "    \n",
    "    FillMedian(columns=['publish_time_days_since_published','publish_time_promo_days_since_published']),\n",
    "    \n",
    "    JoinGroupby(columns=['ad_id', 'source_id', 'document_id_promo', 'publisher_id', 'advertiser_id', 'campaign_id'], \n",
    "        cont_names=['clicked'],stats=['sum','count']),\n",
    "    \n",
    "    #calculate the smoothed ctr\n",
    "    LambdaOp(\n",
    "         op_name='ctr',\n",
    "         f=lambda col, gdf: (col + 0.1 * 10)/(gdf['ad_id_count']+10),\n",
    "         columns=['ad_id_clicked_sum'],replace=False),\n",
    "     LambdaOp(\n",
    "         op_name='ctr',\n",
    "         f=lambda col, gdf: (col + 0.1 * 10)/(gdf['source_id_count']+10),\n",
    "         columns=['source_id_clicked_sum'],replace=False),\n",
    "      LambdaOp(\n",
    "         op_name='ctr',\n",
    "         f=lambda col, gdf: (col + 0.1 * 10)/(gdf['document_id_promo_count']+10),\n",
    "         columns=['document_id_promo_clicked_sum'], replace=False),\n",
    "     LambdaOp(\n",
    "        op_name='ctr',\n",
    "        f=lambda col, gdf: (col + 0.1 * 10)/(gdf['publisher_id_count']+10),\n",
    "        columns=['publisher_id_clicked_sum'], replace=False),\n",
    "    LambdaOp(\n",
    "        op_name='ctr',\n",
    "        f=lambda col, gdf: (col + 0.1 * 10)/(gdf['advertiser_id_count']+10),\n",
    "        columns=['advertiser_id_clicked_sum'], replace=False),\n",
    "     LambdaOp(\n",
    "        op_name='ctr',\n",
    "        f=lambda col, gdf: (col + 0.1 * 10)/(gdf['campaign_id_count']+10),\n",
    "        columns=['campaign_id_clicked_sum'], replace=False), \n",
    "    \n",
    "    #take the log of the views and clicks\n",
    "    LogOp(columns=['ad_id_count', 'ad_id_clicked_sum','source_id_count', 'source_id_clicked_sum', 'document_id_promo_count', 'document_id_promo_clicked_sum',\n",
    "                   'publisher_id_count','publisher_id_clicked_sum', 'advertiser_id_count', 'advertiser_id_clicked_sum', 'campaign_id_count', 'campaign_id_clicked_sum']),\n",
    "    \n",
    "    #normalize the views and clicks\n",
    "    Normalize(columns=['ad_id_count', 'ad_id_clicked_sum','source_id_count', 'source_id_clicked_sum', 'document_id_promo_count', 'document_id_promo_clicked_sum',\n",
    "                       'publisher_id_count','publisher_id_clicked_sum', 'advertiser_id_count', 'advertiser_id_clicked_sum', 'campaign_id_count', 'campaign_id_clicked_sum']),\n",
    "    #fill missing values\n",
    "    FillMissing(columns= ['ad_id_count', 'ad_id_clicked_sum','source_id_count', 'source_id_clicked_sum', 'document_id_promo_count', \n",
    "        'document_id_promo_clicked_sum','publisher_id_count','publisher_id_clicked_sum', 'advertiser_id_count', 'advertiser_id_clicked_sum', \n",
    "        'campaign_id_count', 'campaign_id_clicked_sum', 'advertiser_id_clicked_sum_ctr','document_id_promo_clicked_sum_ctr', \n",
    "        'publisher_id_clicked_sum_ctr', 'source_id_clicked_sum_ctr', 'ad_id_clicked_sum_ctr', 'campaign_id_clicked_sum_ctr']),\n",
    "    \n",
    "    Dropna(columns=['ad_id', 'source_id', 'document_id_promo', 'publisher_id', 'advertiser_id', 'campaign_id'])])\n",
    "\n",
    "workflow.add_preprocess([\n",
    "    Categorify(columns=['geo_location_country','geo_location','geo_location_state', 'uuid', 'platform'],freq_threshold=10),\n",
    "    \n",
    "    HashBucket(HASH_BUCKET_SIZES)])\n",
    "\n",
    "op = ColumnSimilarity(\"doc_event_doc_ad_sim_categories\", \"document_id\", categories, \"document_id_promo\", metric='tfidf', on_device=False)\n",
    "workflow.add_feature(op)\n",
    "op = ColumnSimilarity(\"doc_event_doc_ad_sim_topics\", \"document_id\", topics, \"document_id_promo\", metric='tfidf', on_device=False)\n",
    "workflow.add_feature(op)\n",
    "op = ColumnSimilarity(\"doc_event_doc_ad_sim_entities\", \"document_id\", entities, \"document_id_promo\", metric='tfidf', on_device=False)\n",
    "workflow.add_feature(op)\n",
    "\n",
    "workflow.finalize()\n",
    "\n",
    "train_dataset = nvt.Dataset(OUTPUT_BUCKET_FOLDER+'train_gdf.parquet', part_mem_fraction=0.12)\n",
    "valid_dataset = nvt.Dataset(OUTPUT_BUCKET_FOLDER+'valid_gdf.parquet', part_mem_fraction=0.12)\n",
    "\n",
    "workflow.apply(train_dataset, record_stats=True, output_path=output_train_dir, shuffle=True, out_files_per_proc=5)\n",
    "workflow.apply(valid_dataset, record_stats=False, output_path=output_valid_dir, shuffle=False, out_files_per_proc=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = glob.glob(os.path.join(output_train_dir, \"*.parquet\"))\n",
    "valid_paths = glob.glob(os.path.join(output_valid_dir, \"*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [col for col in dtypes if dtypes[col] == np.int32]\n",
    "NUMERIC_COLUMNS = [col for col in dtypes if dtypes[col] == np.float32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_columns():\n",
    "    wide_columns, deep_columns = [], []\n",
    "\n",
    "    for column_name in CATEGORICAL_COLUMNS:\n",
    "        if column_name in HASH_BUCKET_SIZES: # Changing hashing to identity + adding modulo to dataloader\n",
    "            categorical_column = tf.feature_column.categorical_column_with_identity(\n",
    "                column_name, num_buckets=HASH_BUCKET_SIZES[column_name])\n",
    "\n",
    "        elif column_name in IDENTITY_NUM_BUCKETS:\n",
    "            categorical_column = tf.feature_column.categorical_column_with_identity(\n",
    "                column_name, num_buckets=IDENTITY_NUM_BUCKETS[column_name])\n",
    "        else:\n",
    "            raise ValueError(f'Unexpected categorical column found {column_name}')\n",
    "\n",
    "        if column_name in EMBEDDING_DIMENSIONS:\n",
    "            print(column_name)\n",
    "            wrapped_column = tf.feature_column.embedding_column(\n",
    "                categorical_column,\n",
    "                dimension=EMBEDDING_DIMENSIONS[column_name],\n",
    "                combiner='mean')\n",
    "        else:\n",
    "            wrapped_column = tf.feature_column.indicator_column(categorical_column)\n",
    "            \n",
    "        wide_columns.append(categorical_column)\n",
    "        deep_columns.append(wrapped_column)\n",
    "    \n",
    "    numerics = [tf.feature_column.numeric_column(column_name, shape=(1,),dtype=tf.float32) \n",
    "                for column_name in NUMERIC_COLUMNS]\n",
    "    \n",
    "    wide_columns.extend(numerics)\n",
    "    deep_columns.extend(numerics)\n",
    "       \n",
    "    return wide_columns, deep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_hidden_units=[1024,1024,1024,1024,1024]\n",
    "deep_dropout=.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document_id\n",
      "document_id_promo\n",
      "source_id\n",
      "source_id_promo\n",
      "geo_location\n",
      "geo_location_country\n",
      "geo_location_state\n",
      "publisher_id\n",
      "ad_id\n",
      "advertiser_id\n",
      "publisher_id_promo\n",
      "document_id\n",
      "document_id_promo\n",
      "source_id\n",
      "source_id_promo\n",
      "geo_location\n",
      "geo_location_country\n",
      "geo_location_state\n",
      "publisher_id\n",
      "platform\n",
      "ad_id\n",
      "advertiser_id\n",
      "publisher_id_promo\n",
      "document_id_promo_clicked_sum_ctr\n",
      "publisher_id_clicked_sum_ctr\n",
      "source_id_clicked_sum_ctr\n",
      "document_id_promo_count\n",
      "publish_time_days_since_published\n",
      "ad_id_clicked_sum_ctr\n",
      "advertiser_id_clicked_sum_ctr\n",
      "campaign_id_clicked_sum_ctr\n",
      "ad_id_count\n",
      "publish_time_promo_days_since_published\n"
     ]
    }
   ],
   "source": [
    "wide_columns, deep_columns = get_feature_columns()\n",
    "\n",
    "wide_weighted_outputs = []  # a list of (batch_size, 1) contributors to the linear weighted sum\n",
    "numeric_dense_inputs = []  # NumericColumn inputs; to be concatenated and then fed to a dense layer\n",
    "wide_columns_dict = {}  # key : column\n",
    "deep_columns_dict = {}  # key : column\n",
    "features = {}  # tf.keras.Input placeholders for each feature to be used\n",
    "\n",
    "# construct input placeholders for wide features\n",
    "for col in wide_columns:\n",
    "    print(col.key)\n",
    "    features[col.key] = tf.keras.Input(shape=(1,),\n",
    "                                       batch_size=None, \n",
    "                                       name=col.key,\n",
    "                                       dtype=dtypes[col.key],\n",
    "                                       sparse=False)\n",
    "    wide_columns_dict[col.key] = col\n",
    "for col in deep_columns:\n",
    "    is_embedding_column = ('key' not in dir(col))\n",
    "    key = col.categorical_column.key if is_embedding_column else col.key\n",
    "\n",
    "    if key not in features:\n",
    "        features[key] = tf.keras.Input(shape=(1,), \n",
    "                                       batch_size=None, \n",
    "                                       name=key, \n",
    "                                       dtype=dtypes[col.key], \n",
    "                                       sparse=False)\n",
    "    deep_columns_dict[key] = col\n",
    "\n",
    "for key in wide_columns_dict:\n",
    "    if key in HASH_BUCKET_SIZES: # For Kyle, features are already hashed and no multivalued, I've removed it \n",
    "        wide_weighted_outputs.append(tf.keras.layers.Flatten()(tf.keras.layers.Embedding(\n",
    "            HASH_BUCKET_SIZES[key], 1, input_length=1)(features[key])))\n",
    "\n",
    "    elif key in IDENTITY_NUM_BUCKETS:\n",
    "        wide_weighted_outputs.append(tf.keras.layers.Flatten()(tf.keras.layers.Embedding(\n",
    "            IDENTITY_NUM_BUCKETS[key], 1, input_length=1)(features[key])))\n",
    "    else:\n",
    "        numeric_dense_inputs.append(features[key])\n",
    "\n",
    "categorical_output_contrib = tf.keras.layers.add(wide_weighted_outputs,\n",
    "                                                 name='categorical_output')\n",
    "numeric_dense_tensor = tf.keras.layers.concatenate(\n",
    "    numeric_dense_inputs, name='numeric_dense')\n",
    "deep_columns = list(deep_columns_dict.values())\n",
    "\n",
    "dnn = tf.keras.layers.DenseFeatures(deep_columns, name='deep_embedded')(features)\n",
    "for unit_size in deep_hidden_units:\n",
    "    dnn = tf.keras.layers.Dense(units=unit_size)(dnn)\n",
    "    dnn = tf.keras.layers.Dropout(rate=deep_dropout)(dnn)\n",
    "    dnn = tf.keras.layers.BatchNormalization(momentum=.999)(dnn)\n",
    "dnn = tf.keras.layers.Dense(units=1)(dnn)\n",
    "dnn_model = tf.keras.Model(inputs=features,\n",
    "                           outputs=dnn)\n",
    "linear_output = categorical_output_contrib + tf.keras.layers.Dense(1)(numeric_dense_tensor)\n",
    "\n",
    "linear_model = tf.keras.Model(inputs=features,\n",
    "                              outputs=linear_output)\n",
    "\n",
    "wide_and_deep_model = tf.keras.experimental.WideDeepModel(\n",
    "    linear_model, dnn_model, activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(deep_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATHS = sorted(glob.glob('./preprocessed/train/*.parquet'))\n",
    "VALID_PATHS = sorted(glob.glob('./preprocessed/valid/*.parquet'))\n",
    "train_dataset_tf = KerasSequenceDataset(\n",
    "    TRAIN_PATHS, # you could also use a glob pattern\n",
    "    CATEGORICAL_COLUMNS+NUMERIC_COLUMNS,\n",
    "    batch_size=131072,\n",
    "    label_name='clicked',\n",
    "    shuffle=True,\n",
    "    buffer_size=1 # how many batches to load at once\n",
    ")\n",
    "\n",
    "valid_dataset_tf = KerasSequenceDataset(\n",
    "    VALID_PATHS, # you could also use a glob pattern\n",
    "    CATEGORICAL_COLUMNS+NUMERIC_COLUMNS,\n",
    "    batch_size=131072,\n",
    "    label_name='clicked',\n",
    "    shuffle=False,\n",
    "    buffer_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_optimizer = tf.keras.optimizers.Ftrl(\n",
    "        learning_rate=.1,\n",
    ")\n",
    "\n",
    "deep_optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=.2\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_and_deep_model.compile(\n",
    "    optimizer=[wide_optimizer, deep_optimizer],\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()],\n",
    "    experimental_run_tf_function=False\n",
    ")\n",
    "wide_and_deep_model.fit(train_dataset_tf, validation_data=valid_dataset_tf, epochs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "rapids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
