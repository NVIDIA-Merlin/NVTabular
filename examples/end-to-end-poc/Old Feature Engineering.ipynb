{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped = ratings[ratings[\"interaction\"]].sort_values(\"timestamp\").groupby(\n",
    "#     [\"day\", \"userId\"]\n",
    "# ).agg({\"movieId\": \"collect\"})\n",
    "# grouped = grouped.reset_index()\n",
    "# grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import os\n",
    "import cudf  # cuDF is an implementation of Pandas-like Dataframe on GPU\n",
    "\n",
    "import nvtabular as nvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = os.environ.get(\n",
    "    \"INPUT_DATA_DIR\", os.path.expanduser(\"~/nvt-examples/end-to-end-poc/data/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings = movie_features_ddf.merge(train, on=\"movieId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-louisville",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings.to_parquet(os.path.join(INPUT_DATA_DIR, \"train_ratings.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sample a tag from the list for each interaction\n",
    "\n",
    "sampled_tag = nvt.ColumnGroup([\"tags_unique\"]) >> (lambda col: len(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-borough",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = day_number + sampled_tag\n",
    "(output).graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = nvt.Workflow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings_ds = nvt.Dataset(train_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.fit(train_ratings_ds)\n",
    "workflow.transform(train_ratings_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Group by user id and day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Count number of positives per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dtypes = {}\n",
    "\n",
    "for col in CATEGORICAL_COLUMNS:\n",
    "    dict_dtypes[col] = np.int64\n",
    "\n",
    "for col in LABEL_COLUMNS:\n",
    "    dict_dtypes[col] = np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = nvt.Dataset([os.path.join(INPUT_DATA_DIR, \"train.parquet\")], part_size=\"100MB\")\n",
    "valid_dataset = nvt.Dataset([os.path.join(INPUT_DATA_DIR, \"valid.parquet\")], part_size=\"100MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "workflow.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we have a clean output path\n",
    "if path.exists(os.path.join(INPUT_DATA_DIR, \"train\")):\n",
    "    shutil.rmtree(os.path.join(INPUT_DATA_DIR, \"train\"))\n",
    "if path.exists(os.path.join(INPUT_DATA_DIR, \"valid\")):\n",
    "    shutil.rmtree(os.path.join(INPUT_DATA_DIR, \"valid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "workflow.transform(train_dataset).to_parquet(\n",
    "    output_path=os.path.join(INPUT_DATA_DIR, \"train\"),\n",
    "    shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "    cats=[\"userId\", \"movieId\"],\n",
    "    labels=[\"rating\"],\n",
    "    dtypes=dict_dtypes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "workflow.transform(valid_dataset).to_parquet(\n",
    "    output_path=os.path.join(INPUT_DATA_DIR, \"valid\"),\n",
    "    shuffle=False,\n",
    "    cats=[\"userId\", \"movieId\"],\n",
    "    labels=[\"rating\"],\n",
    "    dtypes=dict_dtypes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.save(os.path.join(INPUT_DATA_DIR, \"workflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "TRAIN_PATHS = sorted(glob.glob(os.path.join(INPUT_DATA_DIR, \"train\", \"*.parquet\")))\n",
    "VALID_PATHS = sorted(glob.glob(os.path.join(INPUT_DATA_DIR, \"valid\", \"*.parquet\")))\n",
    "TRAIN_PATHS, VALID_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.read_parquet(TRAIN_PATHS[0])\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
