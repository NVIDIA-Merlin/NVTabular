{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Preprocessing the Rossmann Store Sales Dataset\n",
    "Here we implement some feature engineering outlined by FastAI in [their example solution](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb) to the [Kaggle Rossmann Store Sales competition](https://www.kaggle.com/c/rossmann-store-sales). We've simplified some sections and left out most of the documentation to keep things neat, so feel free to consult the original notebook for explanations of the feature engineering going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cudf\n",
    "import nvtabular as nvt\n",
    "import dask_cudf\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import Normalize, FillMissing, Categorify, LogOp, JoinExternal, Dropna, LambdaOp, JoinGroupby, Filter, HashBucket, FillMedian\n",
    "from nvtabular.column_similarity import ColumnSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import Client\n",
    "# from dask_cuda import LocalCUDACluster\n",
    "# cluster = LocalCUDACluster(device_memory_limit=\"16GB\")# Do this second to allow spilling from GPU memory to host memory in the workers\n",
    "# client = Client(cluster)\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = os.environ.get('INPUT_DATA_DIR', './rossmann')\n",
    "OUTPUT_DATA_DIR = os.environ.get('OUTPUT_DATA_DIR', './data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir -p $INPUT_DATA_DIR\n",
    "# ! wget -O $INPUT_DATA_DIR/rossmann.tgz http://files.fast.ai/part2/lesson14/rossmann.tgz\n",
    "# ! cd $INPUT_DATA_DIR && tar -xzf rossmann.tgz && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_table(table_name):\n",
    "    return cudf.read_csv(os.path.join(INPUT_DATA_DIR, f'{table_name}.csv'))\n",
    "\n",
    "train = read_table('train')\n",
    "store = read_table('store')\n",
    "store_states = read_table('store_states')\n",
    "state_names = read_table('state_names')\n",
    "googletrend = read_table('googletrend')\n",
    "weather = read_table('weather')\n",
    "test = read_table('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.StateHoliday = train.StateHoliday!='0'\n",
    "test.StateHoliday = test.StateHoliday!='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "googletrend['Date'] = googletrend.week.str.split(' - ', expand=True, n=1)[0]\n",
    "googletrend['State'] = googletrend.file.str.split('_', expand=True, n=2)[2]\n",
    "googletrend['State'] = googletrend.State.where(googletrend['State']!='NI', 'HB,NI')\n",
    "trend_de = googletrend.loc[googletrend.file == 'Rossmann_DE'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (weather, googletrend, train, test, trend_de):\n",
    "    #df.loc[:, 'Date'] = dask_cudf.to_datetime(df.Date)\n",
    "    df['Date'] = df['Date'].astype('datetime64[s]')\n",
    "    df['Month'] = df.Date.dt.month\n",
    "    df['Day'] = df.Date.dt.day\n",
    "    df['Year'] = df.Date.dt.year.astype(str)+ '-01-01'\n",
    "    df['Week']= (((df['Date'] - df['Year'].astype('datetime64[s]')).dt.days)/7).astype('int16') +1\n",
    "    df['Week']=  df.Week.where(df['Week']!=53, 52)\n",
    "    df['Year'] = df.Date.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(df, right, left_on, right_on=None, suffix=None):\n",
    "    df = df.merge(right, how='left', left_on=left_on, right_on=right_on or left_on, suffixes=('', suffix or '_y'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = merge(weather, state_names, 'file', right_on='StateName')\n",
    "store = merge(store, store_states, 'Store')\n",
    "train_df = merge(train, store, 'Store')\n",
    "test_df = merge(test, store, 'Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = merge(train_df, googletrend, ['State', 'Year', 'Week'])\n",
    "test_df = merge(test_df, googletrend, ['State', 'Year', 'Week'])\n",
    "\n",
    "train = test = googletrend = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = merge(train_df, trend_de, ['Year', 'Week'], right_on=['Year', 'Week'], suffix='_DE')\n",
    "test_df = merge(test_df, trend_de, ['Year', 'Week'], right_on=['Year', 'Week'], suffix='_DE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols(gdf):\n",
    "    for c in gdf.columns:\n",
    "        if c.endswith('_y'):\n",
    "            if c in gdf.columns: gdf.drop(c, inplace=True, axis=1)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = drop_cols(train_df)\n",
    "train_df = merge(train_df, weather, ['State', 'Date'], right_on=['State', 'Date'], suffix='_y')\n",
    "test_df = drop_cols(test_df)\n",
    "test_df = merge(test_df, weather, ['State', 'Date'], right_on=['State', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = drop_cols(train_df)\n",
    "test_df = drop_cols(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df['CompetitionOpenSinceYear'] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32)\n",
    "    df['CompetitionOpenSinceMonth'] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32)\n",
    "    df['Promo2SinceYear'] = df.Promo2SinceYear.fillna(1900).astype(np.int32)\n",
    "    df['Promo2SinceWeek'] = df.Promo2SinceWeek.fillna(1).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>Month</th>\n",
       "      <th>...</th>\n",
       "      <th>Mean_VisibilityKm</th>\n",
       "      <th>Min_VisibilitykM</th>\n",
       "      <th>Max_Wind_SpeedKm_h</th>\n",
       "      <th>Mean_Wind_SpeedKm_h</th>\n",
       "      <th>Max_Gust_SpeedKm_h</th>\n",
       "      <th>Precipitationmm</th>\n",
       "      <th>CloudCover</th>\n",
       "      <th>Events</th>\n",
       "      <th>WindDirDegrees</th>\n",
       "      <th>StateName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>900</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-06-22</td>\n",
       "      <td>6374</td>\n",
       "      <td>619</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>35</td>\n",
       "      <td>16</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Rain</td>\n",
       "      <td>243</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>901</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-06-22</td>\n",
       "      <td>6303</td>\n",
       "      <td>588</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.51</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Rain</td>\n",
       "      <td>244</td>\n",
       "      <td>SchleswigHolstein</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  DayOfWeek       Date  Sales  Customers  Open  Promo  StateHoliday  \\\n",
       "0    900          1 2015-06-22   6374        619     1      0         False   \n",
       "1    901          1 2015-06-22   6303        588     1      0         False   \n",
       "\n",
       "  SchoolHoliday  Month  ...  Mean_VisibilityKm  Min_VisibilitykM  \\\n",
       "0             0      6  ...               10.0              10.0   \n",
       "1             0      6  ...               11.0               5.0   \n",
       "\n",
       "   Max_Wind_SpeedKm_h  Mean_Wind_SpeedKm_h  Max_Gust_SpeedKm_h  \\\n",
       "0                  35                   16                55.0   \n",
       "1                  19                   10                <NA>   \n",
       "\n",
       "  Precipitationmm  CloudCover  Events WindDirDegrees          StateName  \n",
       "0            0.00         7.0    Rain            243             Bayern  \n",
       "1            0.51         6.0    Rain            244  SchleswigHolstein  \n",
       "\n",
       "[2 rows x 56 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df['year']= df['CompetitionOpenSinceYear']\n",
    "    df['month']= df['CompetitionOpenSinceMonth']\n",
    "    df['day']= '15'\n",
    "    df['CompetitionOpenSince'] = cudf.to_datetime(df[[\"year\", \"month\", \"day\"]])\n",
    "    df[\"CompetitionDaysOpen\"] = (df['Date'] - df['CompetitionOpenSince']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df.loc[df.CompetitionDaysOpen<0, \"CompetitionDaysOpen\"] = 0\n",
    "    df.loc[df.CompetitionOpenSinceYear<1990, \"CompetitionDaysOpen\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df[\"CompetitionMonthsOpen\"] = df[\"CompetitionDaysOpen\"]//30\n",
    "    df.loc[df.CompetitionMonthsOpen>24, \"CompetitionMonthsOpen\"] = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df['Promo2SinceYear_tmp']= df.Promo2SinceYear.astype(str)+ '-01-01'\n",
    "    dt = cudf.to_datetime(df.Promo2SinceYear_tmp, format='%Y').astype(np.int64) // 10**9\n",
    "    dt += 7*24*3600*df.Promo2SinceWeek\n",
    "    df[\"Promo2Since\"] = cudf.to_datetime(dt*10**9)\n",
    "    df[\"Promo2Days\"] = (df['Date']- df[\"Promo2Since\"]).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df.loc[df.Promo2Days<0, \"Promo2Days\"] = 0\n",
    "    df.loc[df.Promo2SinceYear<1990, \"Promo2Days\"] = 0\n",
    "    df[\"Promo2Weeks\"] = df[\"Promo2Days\"]//7\n",
    "    df.loc[df.Promo2Weeks<0, \"Promo2Weeks\"] = 0\n",
    "    df.loc[df.Promo2Weeks>25, \"Promo2Weeks\"] = 25\n",
    "    df.Promo2Weeks.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.append(test_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's drop these dummy columns.\n",
    "df.drop(['year', 'month', 'day', 'Promo2SinceYear_tmp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SchoolHoliday'] = df['SchoolHoliday'].astype('int32')\n",
    "# convert stateholiday values to 0-1\n",
    "df['StateHoliday'] = df['StateHoliday'] *1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is modififed version of the original code from https://github.com/fastai/course-v3/blob/master/nbs/dl1/rossman_data_clean.ipynb\n",
    "Still not working as intended with cudf. The `get_elapsed` function is defined for cumulative counting across a sorted dataframe. Given a particular field `fld` to monitor, this function will start tracking time since the last occurrence of that field. When the field is seen again, the counter is set to zero.\n",
    "\n",
    "Upon initialization, this will result in datetime na's until the field is encountered. This is reset every time a new store is seen. We'll see how to use this shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elapsed(fld, pre):\n",
    "    day1 = np.timedelta64(1, 'D')\n",
    "    print('day1:', day1)\n",
    "    last_date = np.datetime64()\n",
    "    last_store = 0\n",
    "    res = []\n",
    "    for s,v,d in zip(df.Store.values,df[fld].values, df.Date.values):\n",
    "        if s != last_store:\n",
    "            print('s', s)\n",
    "            last_date = np.datetime64()\n",
    "            last_store = s\n",
    "        if v: \n",
    "            print('v', v)\n",
    "            last_date = d\n",
    "            print((cudf.to_datetime(d)-cudf.to_datetime(last_date)).astype('timedelta64[D]'))\n",
    "        res.append(((cudf.to_datetime(d)-cudf.to_datetime(last_date)).astype('timedelta64[D]') / day1))\n",
    "        print(res)\n",
    "    df[pre+fld] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting error here most likely due to `nans`\n",
    "fld = 'StateHoliday'\n",
    "df = df.sort_values(['Store', 'Date'])\n",
    "get_elapsed(fld, 'After')\n",
    "df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n",
    "get_elapsed(fld, 'Before')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "fld = 'SchoolHoliday'\n",
    "df = df.sort_values(['Store', 'Date'])\n",
    "#df = df.iloc[:10, :].sort_values(['Store', 'Date'])\n",
    "get_elapsed(fld, 'After')\n",
    "df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n",
    "get_elapsed(fld, 'Before')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ops: masking, ffill, bfill, timedelta\n",
    "df = df.sort_values(by=['Store', 'Date'])\n",
    "# first build a mask indicating where stores start and end\n",
    "first_indices = df.Store.diff() != 0\n",
    "last_indices = df.Store.diff().iloc[1:].append(cudf.Series([1]))\n",
    "last_indices.index = first_indices.index\n",
    "idx_mask = ~(first_indices | last_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE DOES NOT WORK BCS CUDF DOES NOT SUPPORT FFILL AND BFILL METHODS\n",
    "\n",
    "# event_fields = ['SchoolHoliday', 'StateHoliday', 'Promo']\n",
    "# for field in event_fields:\n",
    "#     # use the mask from above to mask save dates from the start and end\n",
    "#     # of a given store's range, as well as all dates that have an event\n",
    "#     df['tmp'] = df.Date\n",
    "#     #df.loc[(df[field] == 0) & idx_mask, 'tmp'] = np.nan\n",
    "#     df[(df[field] == 0) & idx_mask]['tmp']= np.nan\n",
    "\n",
    "#     # then use ffill and bbfill to give the input to the time delta\n",
    "#     df['After'+field] = df.tmp.ffill()\n",
    "#     df['Before'+field] = df.tmp.bfill()\n",
    "\n",
    "#     # compute deltas between bfilled and ffilled dates and the current date\n",
    "#     df['After'+field] = (df['Date'] - df['After'+field]).astype('timedelta64[D]')\n",
    "#     df['Before'+field] = (df['Before'+field] - df['Date']).astype('timedelta64[D]')\n",
    "\n",
    "# # get rid of our dummy column\n",
    "# df = df.drop(columns=['tmp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(\"Date\")\n",
    "bwd = df[['Store']+event_fields].sort_index().groupby(\"Store\").rolling(7, min_periods=1).sum()\n",
    "fwd = df[['Store']+event_fields].sort_index(ascending=False).groupby(\"Store\").rolling(7, min_periods=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in (bwd, fwd):\n",
    "    d.drop('Store', 1, inplace=True)\n",
    "    d.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d, suffix in zip([bwd, fwd], ['_bw', '_fw']):\n",
    "    df = df.left.merge(d, ['Store', 'Date'], suffix=suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.left.merge(df, ['Store', 'Date'])\n",
    "test_df = test_df.left.merge(df, ['Store', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1017209, 75), (41088, 75))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df.Sales != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sort_values(by='Date', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41254"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a validation dataset of the same duration as test set\n",
    "cut = train_df['Date'][(train_df['Date'] == train_df['Date'][len(test_df)])].index.max()\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_valid = cut\n",
    "valid_df = train_df[-num_valid:]\n",
    "train_df = train_df[:-num_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $OUTPUT_DATA_DIR\n",
    "\n",
    "train_df.to_csv(os.path.join(OUTPUT_DATA_DIR, 'train.csv'), index=False)\n",
    "valid_df.to_csv(os.path.join(OUTPUT_DATA_DIR, 'valid.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(OUTPUT_DATA_DIR, 'test.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
