{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Preprocessing the Rossmann Store Sales Dataset\n",
    "Here we implement some feature engineering outlined by FastAI in [their example solution](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb) to the [Kaggle Rossmann Store Sales competition](https://www.kaggle.com/c/rossmann-store-sales). We've simplified some sections and left out most of the documentation to keep things neat, so feel free to consult the original notebook for explanations of the feature engineering going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cudf\n",
    "import nvtabular as nvt\n",
    "import dask_cudf\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import Normalize, FillMissing, Categorify, LogOp, JoinExternal, Dropna, LambdaOp, JoinGroupby, Filter, HashBucket, FillMedian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = os.environ.get('INPUT_DATA_DIR', './rossmann')\n",
    "OUTPUT_DATA_DIR = os.environ.get('OUTPUT_DATA_DIR', './data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir -p $INPUT_DATA_DIR\n",
    "# ! wget -O $INPUT_DATA_DIR/rossmann.tgz http://files.fast.ai/part2/lesson14/rossmann.tgz\n",
    "# ! cd $INPUT_DATA_DIR && tar -xzf rossmann.tgz && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_table(table_name):\n",
    "    return cudf.read_csv(os.path.join(INPUT_DATA_DIR, f'{table_name}.csv'))\n",
    "\n",
    "train = read_table('train')\n",
    "store = read_table('store')\n",
    "store_states = read_table('store_states')\n",
    "state_names = read_table('state_names')\n",
    "googletrend = read_table('googletrend')\n",
    "weather = read_table('weather')\n",
    "test = read_table('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.StateHoliday = train.StateHoliday!='0'\n",
    "test.StateHoliday = test.StateHoliday!='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "googletrend['Date'] = googletrend.week.str.split(' - ', expand=True, n=1)[0]\n",
    "googletrend['State'] = googletrend.file.str.split('_', expand=True, n=2)[2]\n",
    "googletrend['State'] = googletrend.State.where(googletrend['State']!='NI', 'HB,NI')\n",
    "trend_de = googletrend.loc[googletrend.file == 'Rossmann_DE'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (weather, googletrend, train, test, trend_de):\n",
    "    #df.loc[:, 'Date'] = dask_cudf.to_datetime(df.Date)\n",
    "    df['Date'] = df['Date'].astype('datetime64[s]')\n",
    "    df['Month'] = df.Date.dt.month\n",
    "    df['Day'] = df.Date.dt.day\n",
    "    df['Year'] = df.Date.dt.year.astype(str)+ '-01-01'\n",
    "    df['Week']= (((df['Date'] - df['Year'].astype('datetime64[s]')).dt.days)/7).astype('int16') +1\n",
    "    df['Week']=  df.Week.where(df['Week']!=53, 52)\n",
    "    df['Year'] = df.Date.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(df, right, left_on, right_on=None, suffix=None):\n",
    "    df = df.merge(right, how='left', left_on=left_on, right_on=right_on or left_on, suffixes=('', suffix or '_y'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols(gdf):\n",
    "    for c in gdf.columns:\n",
    "        if c.endswith('_y'):\n",
    "            if c in gdf.columns: gdf.drop(c, inplace=True, axis=1)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = merge(weather, state_names, 'file', right_on='StateName')\n",
    "store = merge(store, store_states, 'Store')\n",
    "train_df = merge(train, store, 'Store')\n",
    "test_df = merge(test, store, 'Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = merge(train_df, googletrend, ['State', 'Year', 'Week'])\n",
    "test_df = merge(test_df, googletrend, ['State', 'Year', 'Week'])\n",
    "train = test = googletrend = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = merge(train_df, trend_de, ['Year', 'Week'], right_on=['Year', 'Week'], suffix='_DE')\n",
    "test_df = merge(test_df, trend_de, ['Year', 'Week'], right_on=['Year', 'Week'], suffix='_DE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = drop_cols(train_df)\n",
    "train_df = merge(train_df, weather, ['State', 'Date'], right_on=['State', 'Date'], suffix='_y')\n",
    "test_df = drop_cols(test_df)\n",
    "test_df = merge(test_df, weather, ['State', 'Date'], right_on=['State', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = drop_cols(train_df)\n",
    "test_df = drop_cols(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df['CompetitionOpenSinceYear'] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32)\n",
    "    df['CompetitionOpenSinceMonth'] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32)\n",
    "    df['Promo2SinceYear'] = df.Promo2SinceYear.fillna(1900).astype(np.int32)\n",
    "    df['Promo2SinceWeek'] = df.Promo2SinceWeek.fillna(1).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df['year']= df['CompetitionOpenSinceYear']\n",
    "    df['month']= df['CompetitionOpenSinceMonth']\n",
    "    df['day']= '15'\n",
    "    df['CompetitionOpenSince'] = cudf.to_datetime(df[[\"year\", \"month\", \"day\"]])\n",
    "    df[\"CompetitionDaysOpen\"] = (df['Date'] - df['CompetitionOpenSince']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df.loc[df.CompetitionDaysOpen<0, \"CompetitionDaysOpen\"] = 0\n",
    "    df.loc[df.CompetitionOpenSinceYear<1990, \"CompetitionDaysOpen\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df[\"CompetitionMonthsOpen\"] = df[\"CompetitionDaysOpen\"]//30\n",
    "    df.loc[df.CompetitionMonthsOpen>24, \"CompetitionMonthsOpen\"] = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df['Promo2SinceYear_tmp']= df.Promo2SinceYear.astype(str)+ '-01-01'\n",
    "    dt = cudf.to_datetime(df.Promo2SinceYear_tmp, format='%Y').astype(np.int64) // 10**9\n",
    "    dt += 7*24*3600*df.Promo2SinceWeek\n",
    "    df[\"Promo2Since\"] = cudf.to_datetime(dt*10**9)\n",
    "    df[\"Promo2Days\"] = (df['Date']- df[\"Promo2Since\"]).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df.loc[df.Promo2Days<0, \"Promo2Days\"] = 0\n",
    "    df.loc[df.Promo2SinceYear<1990, \"Promo2Days\"] = 0\n",
    "    df[\"Promo2Weeks\"] = df[\"Promo2Days\"]//7\n",
    "    df.loc[df.Promo2Weeks<0, \"Promo2Weeks\"] = 0\n",
    "    df.loc[df.Promo2Weeks>25, \"Promo2Weeks\"] = 25\n",
    "    df.Promo2Weeks.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.append(test_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's drop these dummy columns.\n",
    "df = df.drop(['year', 'month', 'day', 'Promo2SinceYear_tmp'], inplace=True)\n",
    "train_df = train_df.drop(['year', 'month', 'day', 'Promo2SinceYear_tmp'], inplace=True)\n",
    "test_df = test_df.drop(['year', 'month', 'day', 'Promo2SinceYear_tmp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cast SchoolHoliday to int32.\n",
    "df['SchoolHoliday'] = df['SchoolHoliday'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ops: masking, ffill, bfill, timedelta\n",
    "df = df.sort_values(by=['Store', 'Date'])\n",
    "# We convert cudf dataframe to the pandas df be able to do `bfill` and `ffill` operations below.\n",
    "df = df.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is modififed version of the original code from https://github.com/fastai/course-v3/blob/master/nbs/dl1/rossman_data_clean.ipynb\n",
    "Still not working as intended with cudf. The `get_elapsed` function is defined for cumulative counting across a sorted dataframe. Given a particular field `fld` to monitor, this function will start tracking time since the last occurrence of that field. When the field is seen again, the counter is set to zero.\n",
    "\n",
    "Upon initialization, this will result in datetime na's until the field is encountered. This is reset every time a new store is seen. We'll see how to use this shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first build a mask indicating where stores start and end\n",
    "first_indices = df.Store.diff() != 0\n",
    "last_indices = df.Store.diff().iloc[1:].append(pd.Series([1]))\n",
    "last_indices.index = first_indices.index\n",
    "idx_mask = ~(first_indices | last_indices)\n",
    "\n",
    "event_fields = ['SchoolHoliday', 'StateHoliday', 'Promo']\n",
    "for field in event_fields:\n",
    "    # use the mask from above to mask save dates from the start and end\n",
    "    # of a given store's range, as well as all dates that have an event\n",
    "    df['tmp'] = df.Date\n",
    "    df.loc[(df[field] == 0) & idx_mask, 'tmp'] = np.nan\n",
    "    # then use ffill and bbfill to give the input to the time delta\n",
    "    df['After'+field] = df.tmp.ffill()\n",
    "    df['Before'+field] = df.tmp.bfill()\n",
    "\n",
    "    # compute deltas between bfilled and ffilled dates and the current date\n",
    "    df['After'+field] = (df['Date'] - df['After'+field]).astype('timedelta64[D]')\n",
    "    df['Before'+field] = (df['Before'+field] - df['Date']).astype('timedelta64[D]')\n",
    "\n",
    "# get rid of our dummy column\n",
    "df = df.drop(columns=['tmp'])\n",
    "\n",
    "#let's convert pandas back to cudf df\n",
    "df = cudf.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_fields = ['SchoolHoliday', 'StateHoliday', 'Promo']\n",
    "bwd = df[['Store']+event_fields].sort_index().groupby(\"Store\").rolling(7, min_periods=1).sum()\n",
    "fwd = df[['Store']+event_fields].sort_index(ascending=False).groupby(\"Store\").rolling(7, min_periods=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in (bwd, fwd):\n",
    "    d.drop('Store', 1, inplace=True)\n",
    "    d.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(bwd, left_on=['Date', 'Store'], right_on=['Date', 'Store'], how='left', suffixes=['', '_bw'])\n",
    "df = df.merge(fwd, left_on=['Date', 'Store'], right_on=['Date', 'Store'], how= 'left', suffixes=['', '_fw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = merge(train_df, df, ['Store', 'Date'], right_on=['Store', 'Date'])\n",
    "test_df = merge(test_df, df, ['Store', 'Date'], right_on=['Store', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = drop_cols(train_df)\n",
    "test_df = drop_cols(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df.Sales != 0]\n",
    "train_df = train_df.sort_values(by='Date', ascending=True)\n",
    "train_df=train_df.reset_index(drop=True)\n",
    "test_df=test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42171"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a validation dataset of the same duration as test set\n",
    "cut = train_df['Date'][(train_df['Date'] == train_df['Date'][len(test_df)])].index.max()\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_valid = cut\n",
    "valid_df = train_df[-num_valid:]\n",
    "train_df = train_df[:-num_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $OUTPUT_DATA_DIR\n",
    "\n",
    "train_df.to_csv(os.path.join(OUTPUT_DATA_DIR, 'train.csv'), index=False)\n",
    "valid_df.to_csv(os.path.join(OUTPUT_DATA_DIR, 'valid.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(OUTPUT_DATA_DIR, 'test.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
