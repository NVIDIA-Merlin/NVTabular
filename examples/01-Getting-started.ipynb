{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb28e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77464844",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Getting Started with NVTabular: Process Tabular Data On GPU\n",
    "\n",
    "This notebook is created using the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow/tags) container. \n",
    "\n",
    "## Overview\n",
    "\n",
    "[Merlin NVTabular](https://github.com/NVIDIA-Merlin/NVTabular) is a library for processing tabular data. It lets Data Scientists and ML Engineers easily process data leveraging custom operators specifically designed for machine learning workflows. The processing is carried out on the GPU with best practices baked into the library. Running on the GPU translates to faster iteration cycles and, thanks to leveraging `dask`, enables working on arbitrarily large datasets. NVTabular is a part of the [Merlin open source framework](https://developer.nvidia.com/nvidia-merlin) which allows for seamless transitioning to working with your preprocessed data using the numerous other libraries, including ones for model construction and serving.\n",
    "\n",
    "Core features of Merlin NVTabular:\n",
    "- Many different operators (`Categorify`, `FillMissing`, `TargetEncoding`, `Groupby`, etc) tailored for processing tabular data at scale\n",
    "- Flexible APIs targeted to both production and research\n",
    "- Deep integration with NVIDIA Merlin platform, including Merlin Models for constructing and training Deep Learning models and Merlin Systems for model serving\n",
    "\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "- Processing the Movielens dataset.\n",
    "- Understanding Merlin NVTabular high-level concepts (Dataset, Workflow)\n",
    "- A first look at operators and defining the preprocessing workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5598ae",
   "metadata": {},
   "source": [
    "## Downloading the dataset\n",
    "\n",
    "### MovieLens25M\n",
    "\n",
    "The [MovieLens25M](https://grouplens.org/datasets/movielens/25m/) is a popular dataset for recommender systems and is widely used in academic publications. The dataset contains 25M movie ratings for 62,000 movies given by 162,000 users. Many projects use only the user/item/rating information of MovieLens, but the original dataset provides metadata for the movies, as well. For example, which genres a movie has.\n",
    "\n",
    "To streamline obtaining data, we will use a function from Merlin Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60653f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 00:56:39.876941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-15 00:56:39.877383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-15 00:56:39.877518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from merlin.datasets.entertainment import get_movielens\n",
    "\n",
    "input_path = os.environ.get(\"INPUT_DATA_DIR\", os.path.expanduser(\"~/merlin-framework/movielens/\"))\n",
    "get_movielens(variant=\"ml-1m\", path=input_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95b716",
   "metadata": {},
   "source": [
    "The original dataset has been preprocessed to make it easier to work with. Instead of having to deal with `dat` files, we can read files directly into `cudf` using the parquet format.\n",
    "\n",
    "The data has already been split for us into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7293205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README                    ratings.dat    users.dat\r\n",
      "movies.dat                train.parquet  users_converted.parquet\r\n",
      "movies_converted.parquet  \u001b[0m\u001b[01;34mtransformed\u001b[0m/   valid.parquet\r\n"
     ]
    }
   ],
   "source": [
    "ls {input_path}/ml-1m #noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced624c2",
   "metadata": {},
   "source": [
    "For our initial workflow, let's use the `rating` and `genres` as our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0c3cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "\n",
    "train = cudf.read_parquet(f'{input_path}/ml-1m/train.parquet')\n",
    "valid = cudf.read_parquet(f'{input_path}/ml-1m/train.parquet')\n",
    "\n",
    "movies = cudf.read_parquet(f'{input_path}/ml-1m/movies_converted.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b83f0f2",
   "metadata": {},
   "source": [
    "From the provided `train` and `validation` sets we will extract `userId`, `movieId` and `rating`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf108aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>659261</th>\n",
       "      <td>3971</td>\n",
       "      <td>344</td>\n",
       "      <td>4</td>\n",
       "      <td>967070489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540969</th>\n",
       "      <td>3328</td>\n",
       "      <td>249</td>\n",
       "      <td>5</td>\n",
       "      <td>967916675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738258</th>\n",
       "      <td>4411</td>\n",
       "      <td>364</td>\n",
       "      <td>4</td>\n",
       "      <td>970369846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748921</th>\n",
       "      <td>4471</td>\n",
       "      <td>2333</td>\n",
       "      <td>3</td>\n",
       "      <td>973202645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326280</th>\n",
       "      <td>1931</td>\n",
       "      <td>735</td>\n",
       "      <td>3</td>\n",
       "      <td>974690516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating  timestamp\n",
       "659261    3971      344       4  967070489\n",
       "540969    3328      249       5  967916675\n",
       "738258    4411      364       4  970369846\n",
       "748921    4471     2333       3  973202645\n",
       "326280    1931      735       3  974690516"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee5c7c2",
   "metadata": {},
   "source": [
    "## Defining the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d75cb24",
   "metadata": {},
   "source": [
    "Before we can leverage `NVTabular`, we need to convert our data to a `Merlin Dataset`.\n",
    "\n",
    "We achieve this by passing the `cudf.DataFrame` to the `Dataset` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0f72d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<merlin.io.dataset.Dataset at 0x7fc760e23280>,\n",
       " <merlin.io.dataset.Dataset at 0x7fc760e231c0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nvtabular as nvt\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "train_ds = nvt.Dataset(train)\n",
    "valid_ds = nvt.Dataset(valid)\n",
    "\n",
    "train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ec1a1",
   "metadata": {},
   "source": [
    "Now that we have read in our data, let's define a workflow.\n",
    "\n",
    "A workflow consists of one or more preprocessing steps that will be applied to our data.\n",
    "\n",
    "We begin by converting `userId` and `movieId` columns to categories. In our dataset, they are already represented as integers, but many models require them to be continuous integers, which is not something we can guarantee about our input data if we don't preprocess it. Further to that, in order to train models on our data, we need to ensure we handle categories not seen in the train dataset.\n",
    "\n",
    "We accomplish both of these with the `Categorify` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce735546",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_userId = ['userId'] >> nvt.ops.Categorify()\n",
    "encoded_movieId = ['movieId'] >> nvt.ops.Categorify()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c1126d",
   "metadata": {},
   "source": [
    "Above, we are instructing `NVTabular` to select the `userId` and `movieId` columns and to apply the `Categorify` operator to them. We store the results as `output`.\n",
    "\n",
    "When we run the cell, the actual operation is not performed. Only a graph representation of the operation is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa9c3a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"142pt\" height=\"260pt\"\n",
       " viewBox=\"0.00 0.00 141.89 260.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-256 137.89,-256 137.89,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"66.94\" cy=\"-90\" rx=\"59.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"66.94\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">Categorify</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"66.94\" cy=\"-18\" rx=\"62.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"66.94\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">output cols</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M66.94,-71.7C66.94,-63.98 66.94,-54.71 66.94,-46.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"70.44,-46.1 66.94,-36.1 63.44,-46.1 70.44,-46.1\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"66.94\" cy=\"-162\" rx=\"66.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"66.94\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">SelectionOp</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M66.94,-143.7C66.94,-135.98 66.94,-126.71 66.94,-118.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"70.44,-118.1 66.94,-108.1 63.44,-118.1 70.44,-118.1\"/>\n",
       "</g>\n",
       "<!-- 1_selector -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1_selector</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"66.94\" cy=\"-234\" rx=\"51.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"66.94\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">[&#39;userId&#39;]</text>\n",
       "</g>\n",
       "<!-- 1_selector&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1_selector&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M66.94,-215.7C66.94,-207.98 66.94,-198.71 66.94,-190.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"70.44,-190.1 66.94,-180.1 63.44,-190.1 70.44,-190.1\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fc760e1e9d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_userId.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfa13a8",
   "metadata": {},
   "source": [
    "Let us add one more preprocessing step.\n",
    "\n",
    "In our dataset we want to include the `userId` and `movieId` columns as features, and the `rating` column which we will use as our target.\n",
    "\n",
    "Let's tag the columns appropriately so that other components of the Merlin Framework (for instance, Merlin Models or Merlin Systems) can make use of this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "252ccb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_userId = encoded_userId >> nvt.ops.TagAsUserID()\n",
    "encoded_movieId = encoded_movieId >> nvt.ops.TagAsItemID()\n",
    "target = ['rating'] >> nvt.ops.AddMetadata(tags=[Tags.TARGET, Tags.REGRESSION])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc778ca",
   "metadata": {},
   "source": [
    "We are now ready to construct a `Workflow` that will run the operations we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b7226fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = nvt.Workflow(encoded_userId + encoded_movieId + target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce8958",
   "metadata": {},
   "source": [
    "## Applying the workflow to the train and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f342e03",
   "metadata": {},
   "source": [
    "`NVTabular` follows the familiar `sklearn` API. We can fit the workflow to our train set and subsequently use it to transform our validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aef5e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>712</td>\n",
       "      <td>369</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1137</td>\n",
       "      <td>1216</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>266</td>\n",
       "      <td>156</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1785</td>\n",
       "      <td>516</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3161</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating\n",
       "0     712      369       4\n",
       "1    1137     1216       5\n",
       "2     266      156       4\n",
       "3    1785      516       3\n",
       "4    3161     2024       3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transformed = workflow.fit_transform(train_ds)\n",
    "train_transformed.compute().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46340ecf",
   "metadata": {},
   "source": [
    "We have `fit` our workflow to the train set. During this operation, the workflow computed and stored a mapping from `userId` and `movieId` values in the dataset to their encoded representation as continuous integers.\n",
    "\n",
    "Subsequently, we have transformed the train set and encoded the `userId` and `movieId` columns (both operations were performed when we called `fit_transform`).\n",
    "\n",
    "We can now use the fitted workflow to transform our validation set using the values precomputed on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "879d8345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>712</td>\n",
       "      <td>369</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1137</td>\n",
       "      <td>1216</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>266</td>\n",
       "      <td>156</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1785</td>\n",
       "      <td>516</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3161</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating\n",
       "0     712      369       4\n",
       "1    1137     1216       5\n",
       "2     266      156       4\n",
       "3    1785      516       3\n",
       "4    3161     2024       3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_transformed = workflow.transform(valid_ds)\n",
    "valid_transformed.compute().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa885500",
   "metadata": {},
   "source": [
    "## Using the transformed datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d89f5d9",
   "metadata": {},
   "source": [
    "We can export processed data to a `cudf` `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e2ef863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cudf.core.dataframe.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf = train_transformed.compute()\n",
    "type(gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899f068d",
   "metadata": {},
   "source": [
    "Or we can output it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6638df79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_file_list.txt\t_metadata  _metadata.json  part_0.parquet  schema.pbtxt\r\n"
     ]
    }
   ],
   "source": [
    "train_transformed.to_parquet('train_transformed')\n",
    "!ls train_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f2da31",
   "metadata": {},
   "source": [
    "Writing it out to disk not only stores the data, but also presrves the `schema` that we implicitly created during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae89f758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.start_index</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>userId</td>\n",
       "      <td>(Tags.USER_ID, Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.userId.parquet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6041.0</td>\n",
       "      <td>userId</td>\n",
       "      <td>6041.0</td>\n",
       "      <td>210.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>movieId</td>\n",
       "      <td>(Tags.ITEM_ID, Tags.ITEM, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.movieId.parquet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3680.0</td>\n",
       "      <td>movieId</td>\n",
       "      <td>3680.0</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rating</td>\n",
       "      <td>(Tags.TARGET, Tags.REGRESSION)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'userId', 'tags': {<Tags.USER_ID: 'user_id'>, <Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.userId.parquet', 'domain': {'min': 0, 'max': 6041, 'name': 'userId'}, 'embedding_sizes': {'cardinality': 6041, 'dimension': 210}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'movieId', 'tags': {<Tags.ITEM_ID: 'item_id'>, <Tags.ITEM: 'item'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.movieId.parquet', 'domain': {'min': 0, 'max': 3680, 'name': 'movieId'}, 'embedding_sizes': {'cardinality': 3680, 'dimension': 159}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'rating', 'tags': {<Tags.TARGET: 'target'>, <Tags.REGRESSION: 'regression'>}, 'properties': {}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transformed.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83847fe",
   "metadata": {},
   "source": [
    "The information contained in the schema can be leveraged by downstream tools, such as Merlin Models and Merlin Systems, to streamline model training and deployment.\n",
    "\n",
    "This brings us to the third option available to us.\n",
    "\n",
    "We can use our `Dataset`s to train a model directly.\n",
    "\n",
    "Let's finish off this notebook with training a DLRM (a Deep Learning Recommendation Model introduced in [Deep Learning Recommendation Model for Personalization and Recommendation Systems](https://arxiv.org/abs/1906.00091)) on our preprocessed data.\n",
    "\n",
    "Thanks to the information in the schema this will require very few lines of code.\n",
    "\n",
    "To learn more about the integration between NVTabular and Merlin Models, please see this [example](https://github.com/NVIDIA-Merlin/models/blob/main/examples/02-Merlin-Models-and-NVTabular-integration.ipynb) in the Merlin Models [repository](https://github.com/NVIDIA-Merlin/models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff4c40",
   "metadata": {},
   "source": [
    "## Training a DLRM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688b89c7",
   "metadata": {},
   "source": [
    "We define the DLRM model, whose prediction task is a binary classification. From the `schema`, the categorical features are identified (and embedded) and the target column is also automatically inferred, because of the schema tags. We talk more about the schema in the next [example notebook (02)](02-Merlin-Models-and-NVTabular-integration.ipynb),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7be72270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 00:56:41.785498: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-15 00:56:41.786590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-15 00:56:41.786840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-15 00:56:41.787026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-15 00:56:41.787406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-15 00:56:41.787606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-15 00:56:41.787802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-15 00:56:41.787968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 24576 MB memory:  -> device: 0, name: Quadro RTX 8000, pci bus id: 0000:08:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 7s 6ms/step - loss: 1.2931 - root_mean_squared_error: 1.1371 - regularization_loss: 0.0000e+00 - val_loss: 0.8278 - val_root_mean_squared_error: 0.9098 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.8330 - root_mean_squared_error: 0.9127 - regularization_loss: 0.0000e+00 - val_loss: 0.8034 - val_root_mean_squared_error: 0.8963 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.8104 - root_mean_squared_error: 0.9002 - regularization_loss: 0.0000e+00 - val_loss: 0.7787 - val_root_mean_squared_error: 0.8824 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.7917 - root_mean_squared_error: 0.8898 - regularization_loss: 0.0000e+00 - val_loss: 0.7645 - val_root_mean_squared_error: 0.8743 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.7787 - root_mean_squared_error: 0.8824 - regularization_loss: 0.0000e+00 - val_loss: 0.7575 - val_root_mean_squared_error: 0.8704 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 1/3\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.7429 - root_mean_squared_error: 0.8619 - regularization_loss: 0.0000e+00 - val_loss: 0.7319 - val_root_mean_squared_error: 0.8555 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.7338 - root_mean_squared_error: 0.8566 - regularization_loss: 0.0000e+00 - val_loss: 0.7255 - val_root_mean_squared_error: 0.8517 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.7290 - root_mean_squared_error: 0.8538 - regularization_loss: 0.0000e+00 - val_loss: 0.7208 - val_root_mean_squared_error: 0.8490 - val_regularization_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc97e8b7370>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "import merlin.models.tf as mm\n",
    "\n",
    "model = mm.DLRMModel(\n",
    "    train_transformed.schema,\n",
    "    embedding_dim=64,\n",
    "    bottom_block=mm.MLPBlock([128, 64]),\n",
    "    top_block=mm.MLPBlock([128, 64, 32]),\n",
    "    prediction_tasks=mm.RegressionTask('rating')\n",
    ")\n",
    "\n",
    "opt = tensorflow.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(optimizer=opt)\n",
    "model.fit(train_transformed, validation_data=valid_transformed, batch_size=1024, epochs=5)\n",
    "\n",
    "model.optimizer.learning_rate = 1e-4\n",
    "model.fit(train_transformed, validation_data=valid_transformed, batch_size=1024, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6ad327",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeba861b",
   "metadata": {},
   "source": [
    "NVTabular exposes operators tailored for processing tabular data at scale with machine learning best practices baked into the library. It tightly integrates with the rest of the Merlin Framework to streamline model construction, training and serving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9180e84",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ba3102",
   "metadata": {},
   "source": [
    "In subsequent notebooks, we will define more advanaced workflows and custom operators. We will also take a closer look at exporting NVTabular datasets and workflows at running in different environments (CPU, GPU and multi-GPU)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "merlin": {
   "containers": [
    "nvcr.io/nvidia/merlin/merlin-tensorflow:latest"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
