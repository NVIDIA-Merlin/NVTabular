{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Criteo: Training with HugeCTR\n",
    "\n",
    "## Overview\n",
    "\n",
    "HugeCTR is an open-source framework to accelerate the training of CTR estimation models on NVIDIA GPUs. It is written in CUDA C++ and highly exploits GPU-accelerated libraries such as cuBLAS, cuDNN, and NCCL.<br><br>\n",
    "HugeCTR offers multiple advantages to train deep learning recommender systems:\n",
    "\n",
    "1. **Speed**: HugeCTR is a highly efficient framework written C++. We experienced up to 10x speed up. HugeCTR on a NVIDIA DGX A100 system proved to be the fastest commercially available solution for training the architecture Deep Learning Recommender Model (DLRM) developed by Facebook.\n",
    "2. **Scale**: HugeCTR supports model parallel scaling. It distributes the large embedding tables over multiple GPUs or multiple nodes. \n",
    "3. **Easy-to-use**: Easy-to-use Python API similar to Keras. Examples for popular deep learning recommender systems architectures (Wide&Deep, DLRM, DCN, DeepFM) are available.\n",
    "\n",
    "HugeCTR is able to train recommender system models with larger-than-memory embedding tables by leveraging a parameter server. \n",
    "\n",
    "You can find more information about HugeCTR [here](https://github.com/NVIDIA/HugeCTR).\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this notebook, we learn how to to use HugeCTR for training recommender system models\n",
    "\n",
    "- Use **HugeCTR** to define a recommender system model\n",
    "- Train Facebook's [Deep Learning Recommendation Model](https://arxiv.org/pdf/1906.00091.pdf) with HugeCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with HugeCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As HugeCTR optimizes the training in CUDA++, we need to define the training pipeline and model architecture and execute it via the commandline. We will use the Python API, which is similar to Keras models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not familiar with HugeCTR's Python API and parameters, you can read more in its GitHub repository:\n",
    "- [HugeCTR User Guide](https://github.com/NVIDIA/HugeCTR/blob/master/docs/hugectr_user_guide.md)\n",
    "- [HugeCTR Python API](https://github.com/NVIDIA/HugeCTR/blob/master/docs/python_interface.md)\n",
    "- [HugeCTR example architectures](https://github.com/NVIDIA/HugeCTR/tree/master/samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write the code to a `./model.py` file and execute it. It will create snapshot, which we will use for inference in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dask  train  valid  workflow\n"
     ]
    }
   ],
   "source": [
    "!ls /raid/data/criteo/test_dask/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system(\"rm -rf ./criteo_hugectr/\")\n",
    "os.system(\"mkdir -p ./criteo_hugectr/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", '/tmp/model/data')\n",
    "data_path = os.path.join(INPUT_DATA_DIR, \"train\", \"_file_list.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `graph_to_json` to convert the model to a JSON configuration, required for the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile './model.py'\n",
    "file_to_write = f\"\"\"\n",
    "import hugectr\n",
    "from mpi4py import MPI  # noqa\n",
    "\n",
    "# HugeCTR\n",
    "solver = hugectr.CreateSolver(\n",
    "    vvgpu=[[0]],\n",
    "    max_eval_batches=100,\n",
    "    batchsize_eval=2720,\n",
    "    batchsize=2720,\n",
    "    i64_input_key=True,\n",
    "    use_mixed_precision=False,\n",
    "    repeat_dataset=True,\n",
    ")\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type=hugectr.Optimizer_t.SGD)\n",
    "reader = hugectr.DataReaderParams(\n",
    "    data_reader_type=hugectr.DataReaderType_t.Parquet,\n",
    "    source=[\"{data_path}\"],\n",
    "    eval_source=\"{data_path}\",\n",
    "    check_type=hugectr.Check_t.Non,\n",
    "    slot_size_array=[\n",
    "        10000000,\n",
    "        10000000,\n",
    "        3014529,\n",
    "        400781,\n",
    "        11,\n",
    "        2209,\n",
    "        11869,\n",
    "        148,\n",
    "        4,\n",
    "        977,\n",
    "        15,\n",
    "        38713,\n",
    "        10000000,\n",
    "        10000000,\n",
    "        10000000,\n",
    "        584616,\n",
    "        12883,\n",
    "        109,\n",
    "        37,\n",
    "        17177,\n",
    "        7425,\n",
    "        20266,\n",
    "        4,\n",
    "        7085,\n",
    "        1535,\n",
    "        64,\n",
    "    ],\n",
    ")\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.add(\n",
    "    hugectr.Input(\n",
    "        label_dim=1,\n",
    "        label_name=\"label\",\n",
    "        dense_dim=13,\n",
    "        dense_name=\"dense\",\n",
    "        data_reader_sparse_param_array=[hugectr.DataReaderSparseParam(\"data1\", 1, False, 26)],\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.SparseEmbedding(\n",
    "        embedding_type=hugectr.Embedding_t.LocalizedSlotSparseEmbeddingHash,\n",
    "        workspace_size_per_gpu_in_mb=6000,\n",
    "        embedding_vec_size=128,\n",
    "        combiner=\"sum\",\n",
    "        sparse_embedding_name=\"sparse_embedding1\",\n",
    "        bottom_name=\"data1\",\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"dense\"],\n",
    "        top_names=[\"fc1\"],\n",
    "        num_output=512,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc1\"], top_names=[\"relu1\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu1\"],\n",
    "        top_names=[\"fc2\"],\n",
    "        num_output=256,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc2\"], top_names=[\"relu2\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu2\"],\n",
    "        top_names=[\"fc3\"],\n",
    "        num_output=128,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc3\"], top_names=[\"relu3\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.Interaction,\n",
    "        bottom_names=[\"relu3\", \"sparse_embedding1\"],\n",
    "        top_names=[\"interaction1\"],\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"interaction1\"],\n",
    "        top_names=[\"fc4\"],\n",
    "        num_output=1024,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc4\"], top_names=[\"relu4\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu4\"],\n",
    "        top_names=[\"fc5\"],\n",
    "        num_output=1024,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc5\"], top_names=[\"relu5\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu5\"],\n",
    "        top_names=[\"fc6\"],\n",
    "        num_output=512,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc6\"], top_names=[\"relu6\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu6\"],\n",
    "        top_names=[\"fc7\"],\n",
    "        num_output=256,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(layer_type=hugectr.Layer_t.ReLU, bottom_names=[\"fc7\"], top_names=[\"relu7\"])\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.InnerProduct,\n",
    "        bottom_names=[\"relu7\"],\n",
    "        top_names=[\"fc8\"],\n",
    "        num_output=1,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "        bottom_names=[\"fc8\", \"label\"],\n",
    "        top_names=[\"loss\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "MAX_ITER = 10000\n",
    "EVAL_INTERVAL = 3200\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(max_iter=MAX_ITER, eval_interval=EVAL_INTERVAL, display=1000, snapshot=3200)\n",
    "model.graph_to_json(graph_config_file=\"./criteo_hugectr/1/criteo.json\")\n",
    "\"\"\"\n",
    "with open('./model.py', 'w', encoding='utf-8') as fi:\n",
    "    fi.write(file_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HugeCTR Version: 3.7\n",
      "====================================================Model Init=====================================================\n",
      "[HCTR][02:44:38.212][WARNING][RK0][main]: The model name is not specified when creating the solver.\n",
      "[HCTR][02:44:38.212][WARNING][RK0][main]: MPI was already initialized somewhere elese. Lifetime service disabled.\n",
      "[HCTR][02:44:38.212][INFO][RK0][main]: Global seed is 3391378239\n",
      "[HCTR][02:44:38.255][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "[HCTR][02:44:40.126][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][02:44:40.127][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][02:44:40.127][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][02:44:40.127][INFO][RK0][main]: Using All-reduce algorithm: NCCL\n",
      "[HCTR][02:44:40.127][INFO][RK0][main]: Device 0: Quadro RTX 8000\n",
      "[HCTR][02:44:40.128][INFO][RK0][main]: num of DataReader workers: 1\n",
      "[HCTR][02:44:40.129][INFO][RK0][main]: Vocabulary size: 54120457\n",
      "[HCTR][02:44:40.130][INFO][RK0][main]: max_vocabulary_size_per_gpu_=12288000\n",
      "[HCTR][02:44:40.130][DEBUG][RK0][tid #139916176520960]: file_name_ /tmp/pytest-of-root/pytest-9/test_criteo_hugectr0/tests/crit_test/train/part_0.parquet file_total_rows_ 138449698\n",
      "[HCTR][02:44:40.130][DEBUG][RK0][tid #139916168128256]: file_name_ /tmp/pytest-of-root/pytest-9/test_criteo_hugectr0/tests/crit_test/train/part_0.parquet file_total_rows_ 138449698\n",
      "[HCTR][02:44:40.138][INFO][RK0][main]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "[HCTR][02:44:55.150][INFO][RK0][main]: gpu0 start to init embedding\n",
      "[HCTR][02:44:55.230][INFO][RK0][main]: gpu0 init embedding done\n",
      "[HCTR][02:44:55.234][INFO][RK0][main]: Starting AUC NCCL warm-up\n",
      "[HCTR][02:44:55.235][INFO][RK0][main]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "[HCTR][02:44:55.235][INFO][RK0][main]: label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1                         \n",
      "(None, 1)                               (None, 13)                              \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "LocalizedSlotSparseEmbeddingHash        data1                         sparse_embedding1             (None, 26, 128)               \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dense                         fc1                           (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu1                         fc2                           (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu2                         fc3                           (None, 128)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc3                           relu3                         (None, 128)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Interaction                             relu3                         interaction1                  (None, 480)                   \n",
      "                                        sparse_embedding1                                                                         \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            interaction1                  fc4                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc4                           relu4                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu4                         fc5                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc5                           relu5                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu5                         fc6                           (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc6                           relu6                         (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu6                         fc7                           (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc7                           relu7                         (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu7                         fc8                           (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  fc8                           loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[HCTR][02:44:55.235][INFO][RK0][main]: Use non-epoch mode with number of iterations: 10000\n",
      "[HCTR][02:44:55.235][INFO][RK0][main]: Training batchsize: 2720, evaluation batchsize: 2720\n",
      "[HCTR][02:44:55.235][INFO][RK0][main]: Evaluation interval: 3200, snapshot interval: 3200\n",
      "[HCTR][02:44:55.235][INFO][RK0][main]: Dense network trainable: True\n",
      "[HCTR][02:44:55.235][INFO][RK0][main]: Sparse embedding sparse_embedding1 trainable: True\n",
      "[HCTR][02:44:55.235][INFO][RK0][main]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HCTR][02:44:55.235][INFO][RK0][main]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000\n",
      "[HCTR][02:44:55.235][INFO][RK0][main]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HCTR][02:44:55.235][INFO][RK0][main]: Training source file: /tmp/pytest-of-root/pytest-9/test_criteo_hugectr0/tests/crit_test/train/_file_list.txt\n",
      "[HCTR][02:44:55.235][INFO][RK0][main]: Evaluation source file: /tmp/pytest-of-root/pytest-9/test_criteo_hugectr0/tests/crit_test/train/_file_list.txt\n",
      "[HCTR][02:45:01.551][INFO][RK0][main]: Iter: 1000 Time(1000 iters): 6.31026s Loss: 0.170242 lr:0.001\n",
      "[HCTR][02:45:08.116][INFO][RK0][main]: Iter: 2000 Time(1000 iters): 6.5595s Loss: 0.142086 lr:0.001\n",
      "[HCTR][02:45:14.999][INFO][RK0][main]: Iter: 3000 Time(1000 iters): 6.87726s Loss: 0.144497 lr:0.001\n",
      "[HCTR][02:45:16.619][INFO][RK0][main]: Evaluation, AUC: 0.522062\n",
      "[HCTR][02:45:16.619][INFO][RK0][main]: Eval Time for 100 iters: 0.218802s\n",
      "[HCTR][02:45:17.186][INFO][RK0][main]: Rank0: Dump hash table from GPU0\n",
      "[HCTR][02:45:17.362][INFO][RK0][main]: Rank0: Write hash table <key,value> pairs to file\n",
      "[HCTR][02:45:18.490][INFO][RK0][main]: Done\n",
      "[HCTR][02:45:18.802][INFO][RK0][main]: Dumping sparse weights to files, successful\n",
      "[HCTR][02:45:18.802][INFO][RK0][main]: Dumping sparse optimzer states to files, successful\n",
      "[HCTR][02:45:18.812][INFO][RK0][main]: Dumping dense weights to file, successful\n",
      "[HCTR][02:45:18.812][INFO][RK0][main]: Dumping dense optimizer states to file, successful\n",
      "[HCTR][02:45:24.512][INFO][RK0][main]: Iter: 4000 Time(1000 iters): 9.50778s Loss: 0.142673 lr:0.001\n",
      "[HCTR][02:45:31.873][INFO][RK0][main]: Iter: 5000 Time(1000 iters): 7.35528s Loss: 0.13817 lr:0.001\n",
      "[HCTR][02:45:39.491][INFO][RK0][main]: Iter: 6000 Time(1000 iters): 7.61235s Loss: 0.145115 lr:0.001\n",
      "[HCTR][02:45:42.840][INFO][RK0][main]: Evaluation, AUC: 0.57392\n",
      "[HCTR][02:45:42.840][INFO][RK0][main]: Eval Time for 100 iters: 0.249069s\n",
      "[HCTR][02:45:43.756][INFO][RK0][main]: Rank0: Dump hash table from GPU0\n",
      "[HCTR][02:45:44.043][INFO][RK0][main]: Rank0: Write hash table <key,value> pairs to file\n",
      "[HCTR][02:45:45.935][INFO][RK0][main]: Done\n",
      "[HCTR][02:45:46.480][INFO][RK0][main]: Dumping sparse weights to files, successful\n",
      "[HCTR][02:45:46.480][INFO][RK0][main]: Dumping sparse optimzer states to files, successful\n",
      "[HCTR][02:45:46.486][INFO][RK0][main]: Dumping dense weights to file, successful\n",
      "[HCTR][02:45:46.486][INFO][RK0][main]: Dumping dense optimizer states to file, successful\n",
      "[HCTR][02:45:51.203][INFO][RK0][main]: Iter: 7000 Time(1000 iters): 11.7059s Loss: 0.138048 lr:0.001\n",
      "[HCTR][02:45:59.222][INFO][RK0][main]: Iter: 8000 Time(1000 iters): 8.01361s Loss: 0.149459 lr:0.001\n",
      "[HCTR][02:46:07.359][INFO][RK0][main]: Iter: 9000 Time(1000 iters): 8.1318s Loss: 0.152849 lr:0.001\n",
      "[HCTR][02:46:12.572][INFO][RK0][main]: Evaluation, AUC: 0.624589\n",
      "[HCTR][02:46:12.572][INFO][RK0][main]: Eval Time for 100 iters: 0.223472s\n",
      "[HCTR][02:46:13.798][INFO][RK0][main]: Rank0: Dump hash table from GPU0\n",
      "[HCTR][02:46:14.172][INFO][RK0][main]: Rank0: Write hash table <key,value> pairs to file\n",
      "[HCTR][02:46:16.936][INFO][RK0][main]: Done\n",
      "[HCTR][02:46:17.654][INFO][RK0][main]: Dumping sparse weights to files, successful\n",
      "[HCTR][02:46:17.655][INFO][RK0][main]: Dumping sparse optimzer states to files, successful\n",
      "[HCTR][02:46:17.661][INFO][RK0][main]: Dumping dense weights to file, successful\n",
      "[HCTR][02:46:17.661][INFO][RK0][main]: Dumping dense optimizer states to file, successful\n",
      "[HCTR][02:46:21.006][INFO][RK0][main]: Finish 10000 iterations with batchsize: 2720 in 85.77s.\n",
      "[HCTR][02:46:21.006][INFO][RK0][main]: Save the model graph to ./criteo_hugectr/1/criteo.json successfully\n",
      "run_time: 104.00127220153809\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "!python model.py\n",
    "end = time.time() - start\n",
    "print(f\"run_time: {end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained the model and created snapshots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}