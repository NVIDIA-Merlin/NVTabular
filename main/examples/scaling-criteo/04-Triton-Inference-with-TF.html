<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Scaling Criteo: Triton Inference with TensorFlow &mdash; NVTabular 2021 documentation</title><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/mystnb.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/togglebutton.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script >let toggleHintShow = 'Click to show';</script>
        <script >let toggleHintHide = 'Click to hide';</script>
        <script >let toggleOpenOnPrint = 'true';</script>
        <script src="../../_static/togglebutton.js"></script>
        <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Multi-GPU Example Notebooks" href="../multi-gpu-movielens/index.html" />
    <link rel="prev" title="Scaling Criteo: Triton Inference with HugeCTR" href="04-Triton-Inference-with-HugeCTR.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> NVTabular
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../Introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core_features.html">Core Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/index.html">Accelerated Training</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Example Notebooks</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#structure">Structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#available-example-notebooks">Available Example Notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#running-the-example-notebooks">Running the Example Notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#troubleshooting">Troubleshooting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting-started-movielens/index.html">Getting Started with MovieLens</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Scaling to Large Datasets with Criteo</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="01-Download-Convert.html">Download and Convert</a></li>
<li class="toctree-l3"><a class="reference internal" href="02-ETL-with-NVTabular.html">ETL with NVTabular</a></li>
<li class="toctree-l3"><a class="reference internal" href="03-Training-with-HugeCTR.html">Train with HugeCTR</a></li>
<li class="toctree-l3"><a class="reference internal" href="03-Training-with-TF.html">Train with TensorFlow</a></li>
<li class="toctree-l3"><a class="reference internal" href="04-Triton-Inference-with-HugeCTR.html">Serve a HugeCTR Model</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Serve a TensorFlow Model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../multi-gpu-movielens/index.html">Multi-GPU Example Notebooks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../api.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../resources/index.html">Additional Resources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">NVTabular</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">NVTabular Example Notebooks</a></li>
          <li class="breadcrumb-item"><a href="index.html">Scaling to Large Datasets with Criteo</a></li>
      <li class="breadcrumb-item active">Scaling Criteo: Triton Inference with TensorFlow</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Copyright 2021 NVIDIA Corporation. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="scaling-criteo-triton-inference-with-tensorflow">
<h1>Scaling Criteo: Triton Inference with TensorFlow<a class="headerlink" href="#scaling-criteo-triton-inference-with-tensorflow" title="Permalink to this headline"></a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>The last step is to deploy the ETL workflow and saved model to production. In the production setting, we want to transform the input data as during training (ETL). We need to apply the same mean/std for continuous features and use the same categorical mapping to convert the categories to continuous integer before we use the deep learning model for a prediction. Therefore, we deploy the NVTabular workflow with the TensorFlow model as an ensemble model to Triton Inference. The ensemble model garantuees that the same transformation are applied to the raw inputs.</p>
<a class="reference internal image-reference" href="../../_images/triton-tf.png"><img alt="../../_images/triton-tf.png" src="../../_images/triton-tf.png" style="width: 25%;" /></a>
<div class="section" id="learning-objectives">
<h3>Learning objectives<a class="headerlink" href="#learning-objectives" title="Permalink to this headline"></a></h3>
<p>In this notebook, we learn how to deploy our models to production</p>
<ul class="simple">
<li><p>Use <strong>NVTabular</strong> to generate config and model files for Triton Inference Server</p></li>
<li><p>Deploy an ensemble of NVTabular workflow and TensorFlow model</p></li>
<li><p>Send example request to Triton Inference Server</p></li>
</ul>
</div>
</div>
<div class="section" id="inference-with-triton-and-tensorflow">
<h2>Inference with Triton and TensorFlow<a class="headerlink" href="#inference-with-triton-and-tensorflow" title="Permalink to this headline"></a></h2>
<p>First, we need to generate the Triton Inference Server configurations and save the models in the correct format. In the previous notebooks <a class="reference internal" href="02-ETL-with-NVTabular.html"><span class="doc std std-doc">02-ETL-with-NVTabular</span></a> and <a class="reference internal" href="03-Training-with-TF.html"><span class="doc std std-doc">03-Training-with-TF</span></a> we saved the NVTabular workflow and TensorFlow model to disk. We will load them.</p>
<div class="section" id="saving-ensemble-model-for-triton-inference-server">
<h3>Saving Ensemble Model for Triton Inference Server<a class="headerlink" href="#saving-ensemble-model-for-triton-inference-server" title="Permalink to this headline"></a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">nvtabular</span> <span class="k">as</span> <span class="nn">nvt</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">BASE_DIR</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;BASE_DIR&quot;</span><span class="p">,</span> <span class="s2">&quot;/raid/data/criteo&quot;</span><span class="p">)</span>
<span class="n">input_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;INPUT_DATA_DIR&quot;</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">BASE_DIR</span><span class="p">,</span> <span class="s2">&quot;test_dask/output&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">workflow</span> <span class="o">=</span> <span class="n">nvt</span><span class="o">.</span><span class="n">Workflow</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">input_path</span><span class="p">,</span> <span class="s2">&quot;workflow&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.8/dist-packages/nvtabular/workflow/workflow.py:373: UserWarning: Loading workflow generated with nvtabular version 0.10.0+123.g44d3c3e8.dirty - but we are running nvtabular 1.2.2+4.gebf56ca0f. This might cause issues
  warnings.warn(
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">input_path</span><span class="p">,</span> <span class="s2">&quot;model.savedmodel&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-07-14 23:15:34.019787: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-07-14 23:15:36.054064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46898 MB memory:  -&gt; device: 0, name: Quadro RTX 8000, pci bus id: 0000:15:00.0, compute capability: 7.5
2022-07-14 23:15:36.054715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46890 MB memory:  -&gt; device: 1, name: Quadro RTX 8000, pci bus id: 0000:2d:00.0, compute capability: 7.5
</pre></div>
</div>
</div>
</div>
<p>TensorFlow expect the Integer as <code class="docutils literal notranslate"><span class="pre">int32</span></code> datatype. Therefore, we need to define the NVTabular output datatypes to <code class="docutils literal notranslate"><span class="pre">int32</span></code> for categorical features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">workflow</span><span class="o">.</span><span class="n">output_dtypes</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;C&quot;</span><span class="p">):</span>
        <span class="n">workflow</span><span class="o">.</span><span class="n">output_dtypes</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;int32&quot;</span>
</pre></div>
</div>
</div>
</div>
<p>NVTabular provides an easy function to deploy the ensemble model for Triton Inference Server.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nvtabular.inference.triton</span> <span class="kn">import</span> <span class="n">export_tensorflow_ensemble</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">export_tensorflow_ensemble</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">workflow</span><span class="p">,</span> <span class="s2">&quot;criteo&quot;</span><span class="p">,</span> <span class="s2">&quot;/tmp/model/models/&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:absl:Function `_wrapped_model` contains input name(s) C1, C10, C11, C12, C13, C14, C15, C16, C17, C18, C19, C2, C20, C21, C22, C23, C24, C25, C26, C3, C4, C5, C6, C7, C8, C9, I1, I10, I11, I12, I13, I2, I3, I4, I5, I6, I7, I8, I9 with unsupported characters which will be renamed to c1, c10, c11, c12, c13, c14, c15, c16, c17, c18, c19, c2, c20, c21, c22, c23, c24, c25, c26, c3, c4, c5, c6, c7, c8, c9, i1, i10, i11, i12, i13, i2, i3, i4, i5, i6, i7, i8, i9 in the SavedModel.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:tensorflow:Assets written to: /tmp/model/models/criteo_tf/1/model.savedmodel/assets
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:tensorflow:Assets written to: /tmp/model/models/criteo_tf/1/model.savedmodel/assets
WARNING:absl:&lt;keras.saving.saved_model.load.DenseFeatures object at 0x7f0638513520&gt; has the same name &#39;DenseFeatures&#39; as a built-in Keras object. Consider renaming &lt;class &#39;keras.saving.saved_model.load.DenseFeatures&#39;&gt; to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C1, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C10, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C11, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C12, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C13, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C14, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C15, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C16, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C17, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C18, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C19, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C2, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C20, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C21, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C22, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C23, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C24, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C25, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C26, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C3, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C4, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C5, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C6, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C7, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C8, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects int32 for column C9, but workflow  is producing type int64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects float32 for column I1, but workflow  is producing type float64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects float32 for column I10, but workflow  is producing type float64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects float32 for column I11, but workflow  is producing type float64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects float32 for column I12, but workflow  is producing type float64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects float32 for column I13, but workflow  is producing type float64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects float32 for column I2, but workflow  is producing type float64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects float32 for column I3, but workflow  is producing type float64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects float32 for column I4, but workflow  is producing type float64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects float32 for column I5, but workflow  is producing type float64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects float32 for column I6, but workflow  is producing type float64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects float32 for column I7, but workflow  is producing type float64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects float32 for column I8, but workflow  is producing type float64. Overriding dtype in NVTabular workflow.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/nvtabular/inference/triton/ensemble.py:85: UserWarning: TF model expects float32 for column I9, but workflow  is producing type float64. Overriding dtype in NVTabular workflow.
  warnings.warn(
</pre></div>
</div>
</div>
</div>
<p>We can take a look on the generated files.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>tree /tmp/model
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Blue">/tmp/model</span>
└── <span class=" -Color -Color-Bold -Color-Bold-Blue">models</span>
    ├── <span class=" -Color -Color-Bold -Color-Bold-Blue">criteo</span>
    │   ├── <span class=" -Color -Color-Bold -Color-Bold-Blue">1</span>
    │   └── config.pbtxt
    ├── <span class=" -Color -Color-Bold -Color-Bold-Blue">criteo_nvt</span>
    │   ├── <span class=" -Color -Color-Bold -Color-Bold-Blue">1</span>
    │   │   ├── model.py
    │   │   └── <span class=" -Color -Color-Bold -Color-Bold-Blue">workflow</span>
    │   │       ├── <span class=" -Color -Color-Bold -Color-Bold-Blue">categories</span>
    │   │       │   ├── unique.C1.parquet
    │   │       │   ├── unique.C10.parquet
    │   │       │   ├── unique.C11.parquet
    │   │       │   ├── unique.C12.parquet
    │   │       │   ├── unique.C13.parquet
    │   │       │   ├── unique.C14.parquet
    │   │       │   ├── unique.C15.parquet
    │   │       │   ├── unique.C16.parquet
    │   │       │   ├── unique.C17.parquet
    │   │       │   ├── unique.C18.parquet
    │   │       │   ├── unique.C19.parquet
    │   │       │   ├── unique.C2.parquet
    │   │       │   ├── unique.C20.parquet
    │   │       │   ├── unique.C21.parquet
    │   │       │   ├── unique.C22.parquet
    │   │       │   ├── unique.C23.parquet
    │   │       │   ├── unique.C24.parquet
    │   │       │   ├── unique.C25.parquet
    │   │       │   ├── unique.C26.parquet
    │   │       │   ├── unique.C3.parquet
    │   │       │   ├── unique.C4.parquet
    │   │       │   ├── unique.C5.parquet
    │   │       │   ├── unique.C6.parquet
    │   │       │   ├── unique.C7.parquet
    │   │       │   ├── unique.C8.parquet
    │   │       │   └── unique.C9.parquet
    │   │       ├── metadata.json
    │   │       └── workflow.pkl
    │   └── config.pbtxt
    └── <span class=" -Color -Color-Bold -Color-Bold-Blue">criteo_tf</span>
        ├── <span class=" -Color -Color-Bold -Color-Bold-Blue">1</span>
        │   └── <span class=" -Color -Color-Bold -Color-Bold-Blue">model.savedmodel</span>
        │       ├── <span class=" -Color -Color-Bold -Color-Bold-Blue">assets</span>
        │       ├── keras_metadata.pb
        │       ├── saved_model.pb
        │       └── <span class=" -Color -Color-Bold -Color-Bold-Blue">variables</span>
        │           ├── variables.data-00000-of-00001
        │           └── variables.index
        └── config.pbtxt

12 directories, 36 files
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="loading-ensemble-model-with-triton-inference-server">
<h3>Loading Ensemble Model with Triton Inference Server<a class="headerlink" href="#loading-ensemble-model-with-triton-inference-server" title="Permalink to this headline"></a></h3>
<p>We have only saved the models for Triton Inference Server. We started Triton Inference Server in explicit mode, meaning that we need to send a request that Triton will load the ensemble model.</p>
<p>First, we restart this notebook to free the GPU memory.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import IPython</span>

<span class="c1"># app = IPython.Application.instance()</span>
<span class="c1"># app.kernel.do_shutdown(True)</span>
</pre></div>
</div>
</div>
</div>
<p>We define the BASE_DIR again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="n">BASE_DIR</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;BASE_DIR&quot;</span><span class="p">,</span> <span class="s2">&quot;/raid/data/criteo&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We connect to the Triton Inference Server.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tritonclient.grpc</span> <span class="k">as</span> <span class="nn">grpc_client</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">triton_client</span> <span class="o">=</span> <span class="n">grpc_client</span><span class="o">.</span><span class="n">InferenceServerClient</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s2">&quot;localhost:8001&quot;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;client created.&quot;</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;channel creation failed: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>client created.
</pre></div>
</div>
</div>
</div>
<p>We deactivate warnings.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>

<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We check if the server is alive.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">triton_client</span><span class="o">.</span><span class="n">is_server_live</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>is_server_live, metadata ()

live: true
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>We check the available models in the repositories:</p>
<ul class="simple">
<li><p>criteo: Ensemble</p></li>
<li><p>criteo_nvt: NVTabular</p></li>
<li><p>criteo_tf: TensorFlow model</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">triton_client</span><span class="o">.</span><span class="n">get_model_repository_index</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>get_model_repository_index, metadata ()

models {
  name: &quot;criteo&quot;
}
models {
  name: &quot;criteo_nvt&quot;
}
models {
  name: &quot;criteo_tf&quot;
}
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>models {
  name: &quot;criteo&quot;
}
models {
  name: &quot;criteo_nvt&quot;
}
models {
  name: &quot;criteo_tf&quot;
}
</pre></div>
</div>
</div>
</div>
<p>We load the ensembled model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>

<span class="n">triton_client</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;criteo&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>load_model, metadata ()
override files omitted:
model_name: &quot;criteo&quot;

Loaded model &#39;criteo&#39;
CPU times: user 13.5 ms, sys: 8.86 ms, total: 22.4 ms
Wall time: 41.9 s
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="example-request-to-triton-inference-server">
<h3>Example Request to Triton Inference Server<a class="headerlink" href="#example-request-to-triton-inference-server" title="Permalink to this headline"></a></h3>
<p>Now, the models are loaded and we can create a sample request. We read an example <strong>raw batch</strong> for inference.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get dataframe library - cudf or pandas</span>
<span class="kn">from</span> <span class="nn">merlin.core.dispatch</span> <span class="kn">import</span> <span class="n">get_lib</span>

<span class="n">df_lib</span> <span class="o">=</span> <span class="n">get_lib</span><span class="p">()</span>

<span class="c1"># read in the workflow (to get input/output schema to call triton with)</span>
<span class="n">batch_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">BASE_DIR</span><span class="p">,</span> <span class="s2">&quot;converted/criteo&quot;</span><span class="p">)</span>
<span class="c1"># raise(ValueError(f&quot;{batch_path}&quot;))</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">df_lib</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">batch_path</span><span class="p">,</span> <span class="s2">&quot;*.parquet&quot;</span><span class="p">),</span> <span class="n">num_rows</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">x</span> <span class="o">!=</span> <span class="s2">&quot;label&quot;</span><span class="p">]]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>     I1   I2    I3    I4    I5  I6  I7  I8  I9  I10  ...        C17  \
0     5  110  &lt;NA&gt;    16  &lt;NA&gt;   1   0  14   7    1  ... -771205462   
1    32    3     5  &lt;NA&gt;     1   0   0  61   5    0  ... -771205462   
2  &lt;NA&gt;  233     1   146     1   0   0  99   7    0  ... -771205462   

          C18         C19         C20         C21        C22        C23  \
0 -1206449222 -1793932789 -1014091992   351689309  632402057 -675152885   
1 -1578429167 -1793932789   -20981661 -1556988767 -924717482  391309800   
2  1653545869 -1793932789 -1014091992   351689309  632402057 -675152885   

          C24         C25         C26  
0  2091868316   809724924  -317696227  
1  1966410890 -1726799382 -1218975401  
2   883538181   -10139646  -317696227  

[3 rows x 39 columns]
</pre></div>
</div>
</div>
</div>
<p>We prepare the batch for inference by using correct column names and data types. We use the same datatypes as defined in our dataframe.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch</span><span class="o">.</span><span class="n">dtypes</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>I1     int32
I2     int32
I3     int32
I4     int32
I5     int32
I6     int32
I7     int32
I8     int32
I9     int32
I10    int32
I11    int32
I12    int32
I13    int32
C1     int32
C2     int32
C3     int32
C4     int32
C5     int32
C6     int32
C7     int32
C8     int32
C9     int32
C10    int32
C11    int32
C12    int32
C13    int32
C14    int32
C15    int32
C16    int32
C17    int32
C18    int32
C19    int32
C20    int32
C21    int32
C22    int32
C23    int32
C24    int32
C25    int32
C26    int32
dtype: object
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tritonclient.grpc</span> <span class="k">as</span> <span class="nn">httpclient</span>
<span class="kn">from</span> <span class="nn">tritonclient.utils</span> <span class="kn">import</span> <span class="n">np_to_triton_dtype</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">col_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">col_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">col_names</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values_host</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">col_dtypes</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">d</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">d</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">httpclient</span><span class="o">.</span><span class="n">InferInput</span><span class="p">(</span><span class="n">col_names</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">d</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">np_to_triton_dtype</span><span class="p">(</span><span class="n">col_dtypes</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
    <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_data_from_numpy</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We send the request to the triton server and collect the last output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># placeholder variables for the output</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">httpclient</span><span class="o">.</span><span class="n">InferRequestedOutput</span><span class="p">(</span><span class="s2">&quot;output&quot;</span><span class="p">)]</span>

<span class="c1"># build a client to connect to our server.</span>
<span class="c1"># This InferenceServerClient object is what we&#39;ll be using to talk to Triton.</span>
<span class="c1"># make the request with tritonclient.http.InferInput object</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">triton_client</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="s2">&quot;criteo&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">request_id</span><span class="o">=</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;predicted softmax result:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">as_numpy</span><span class="p">(</span><span class="s2">&quot;output&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>infer, metadata ()
model_name: &quot;criteo&quot;
id: &quot;1&quot;
inputs {
  name: &quot;I1&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;I2&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;I3&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;I4&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;I5&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;I6&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;I7&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;I8&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;I9&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;I10&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;I11&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;I12&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;I13&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C1&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C2&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C3&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C4&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C5&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C6&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C7&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C8&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C9&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C10&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C11&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C12&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C13&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C14&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C15&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C16&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C17&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C18&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C19&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C20&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C21&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C22&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C23&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C24&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C25&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
inputs {
  name: &quot;C26&quot;
  datatype: &quot;INT32&quot;
  shape: 3
  shape: 1
}
outputs {
  name: &quot;output&quot;
}
raw_input_contents: &quot;\005\000\000\000 \000\000\000\000\000\000\000&quot;
raw_input_contents: &quot;n\000\000\000\003\000\000\000\351\000\000\000&quot;
raw_input_contents: &quot;\000\000\000\000\005\000\000\000\001\000\000\000&quot;
raw_input_contents: &quot;\020\000\000\000\000\000\000\000\222\000\000\000&quot;
raw_input_contents: &quot;\000\000\000\000\001\000\000\000\001\000\000\000&quot;
raw_input_contents: &quot;\001\000\000\000\000\000\000\000\000\000\000\000&quot;
raw_input_contents: &quot;\000\000\000\000\000\000\000\000\000\000\000\000&quot;
raw_input_contents: &quot;\016\000\000\000=\000\000\000c\000\000\000&quot;
raw_input_contents: &quot;\007\000\000\000\005\000\000\000\007\000\000\000&quot;
raw_input_contents: &quot;\001\000\000\000\000\000\000\000\000\000\000\000&quot;
raw_input_contents: &quot;\000\000\000\000\001\000\000\000\001\000\000\000&quot;
raw_input_contents: &quot;2\001\000\000U\014\000\000\035\014\000\000&quot;
raw_input_contents: &quot;\000\000\000\000\005\000\000\000\001\000\000\000&quot;
raw_input_contents: &quot;y\rwb\215\375\363\345y\rwb&quot;
raw_input_contents: &quot;X]\037\342\246\377\252\240\003B\230\255&quot;
raw_input_contents: &quot;/D\352\257\325\025\252o\r\306\276b&quot;
raw_input_contents: &quot;\317\177\\\224!4\212\332\356Il8&quot;
raw_input_contents: &quot;H\&#39;\2608#\237\326&lt;M\006U\347&quot;
raw_input_contents: &quot;\313m\315o\313m\315o\313m\315o&quot;
raw_input_contents: &quot;!\252\2005\201\355\026\253b\353\365\265&quot;
raw_input_contents: &quot;\003\211\200()lBC\213\314\362\321&quot;
raw_input_contents: &quot;\246\337\336FT\341\365\035\037\202N.&quot;
raw_input_contents: &quot;\301}\002.\251\300\351}\301}\002.&quot;
raw_input_contents: &quot;1B|\014d\334Rf1B|\014&quot;
raw_input_contents: &quot;\037\035\230\225\&#39;N\353\231\204aq\022&quot;
raw_input_contents: &quot;\267\377\305\000\267\377\305\000\267\377\305\000&quot;
raw_input_contents: &quot;7\345N\2767\345N\2767\345N\276&quot;
raw_input_contents: &quot;\314t\013\212\231\376\273\363\013\r\017\367&quot;
raw_input_contents: &quot;\372&gt;\334L\372&gt;\334L\372&gt;\334L&quot;
raw_input_contents: &quot;\252V\010\322\252V\010\322\252V\010\322&quot;
raw_input_contents: &quot;\272\013\027\270\021\025\353\241\215\033\217b&quot;
raw_input_contents: &quot;\013\302\022\225\013\302\022\225\013\302\022\225&quot;
raw_input_contents: &quot;(/\216\303c\330\277\376(/\216\303&quot;
raw_input_contents: &quot;]Z\366\024\241&lt;2\243]Z\366\024&quot;
raw_input_contents: &quot;\211\260\261%V\356\341\310\211\260\261%&quot;
raw_input_contents: &quot;\013\374\301\327\350\351R\027\013\374\301\327&quot;
raw_input_contents: &quot;\234`\257|\212\0145u\005\271\2514&quot;
raw_input_contents: &quot;\374kC0\352!\023\231\002He\377&quot;
raw_input_contents: &quot;\035W\020\355W\351W\267\035W\020\355&quot;

model_name: &quot;criteo&quot;
model_version: &quot;1&quot;
id: &quot;1&quot;
parameters {
  key: &quot;sequence_end&quot;
  value {
    bool_param: false
  }
}
parameters {
  key: &quot;sequence_id&quot;
  value {
    int64_param: 0
  }
}
parameters {
  key: &quot;sequence_start&quot;
  value {
    bool_param: false
  }
}
outputs {
  name: &quot;output&quot;
  datatype: &quot;FP32&quot;
  shape: 3
  shape: 1
}
raw_output_contents: &quot;Dd\217&lt;$r\233&lt;\241\231u&lt;&quot;

predicted softmax result:
 [[0.01750387]
 [0.01897532]
 [0.01499024]]
</pre></div>
</div>
</div>
</div>
<p>Let’s unload the model. We need to unload each model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">triton_client</span><span class="o">.</span><span class="n">unload_model</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;criteo&quot;</span><span class="p">)</span>
<span class="n">triton_client</span><span class="o">.</span><span class="n">unload_model</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;criteo_nvt&quot;</span><span class="p">)</span>
<span class="n">triton_client</span><span class="o">.</span><span class="n">unload_model</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;criteo_tf&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>unload_model, metadata ()
model_name: &quot;criteo&quot;
parameters {
  key: &quot;unload_dependents&quot;
  value {
    bool_param: false
  }
}

Unloaded model &#39;criteo&#39;
unload_model, metadata ()
model_name: &quot;criteo_nvt&quot;
parameters {
  key: &quot;unload_dependents&quot;
  value {
    bool_param: false
  }
}

Unloaded model &#39;criteo_nvt&#39;
unload_model, metadata ()
model_name: &quot;criteo_tf&quot;
parameters {
  key: &quot;unload_dependents&quot;
  value {
    bool_param: false
  }
}

Unloaded model &#39;criteo_tf&#39;
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="04-Triton-Inference-with-HugeCTR.html" class="btn btn-neutral float-left" title="Scaling Criteo: Triton Inference with HugeCTR" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../multi-gpu-movielens/index.html" class="btn btn-neutral float-right" title="Multi-GPU Example Notebooks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: main
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../../v1.3.1/index.html">v1.3.1</a></dd>
      <dd><a href="../../../v1.3.2/index.html">v1.3.2</a></dd>
      <dd><a href="../../../v1.3.3/index.html">v1.3.3</a></dd>
      <dd><a href="../../../v1.4.0/index.html">v1.4.0</a></dd>
      <dd><a href="../../../v1.5.0/index.html">v1.5.0</a></dd>
      <dd><a href="../../../v1.6.0/index.html">v1.6.0</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="04-Triton-Inference-with-TF.html">main</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>