<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>NVTabular demo on RecSys2020 Challenge &mdash; NVTabular 2021 documentation</title><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/mystnb.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/togglebutton.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="canonical" href="https://nvidia-merlin.github.io/NVTabular/main/examples/winning-solution-recsys2020-twitter/01-02-04-Download-Convert-ETL-with-NVTabular-Training-with-XGBoost.html" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script >let toggleHintShow = 'Click to show';</script>
        <script >let toggleHintHide = 'Click to hide';</script>
        <script >let toggleOpenOnPrint = 'true';</script>
        <script src="../../_static/togglebutton.js"></script>
        <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="API Documentation" href="../../api.html" />
    <link rel="prev" title="Multi-GPU Scaling in NVTabular with Dask" href="../multi-gpu-toy-example/multi-gpu_dask.html" /> 
</head>

<body class="wy-body-for-nav">
  <div class="banner">
    <p class="banner">
      Beginning in January 2023, versions for all NVIDIA Merlin projects
      will change from semantic versioning like <code>4.0</code>
      to calendar versioning like <code>23.01</code>.</p>
  </div>

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> NVTabular
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../Introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core_features.html">Core Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/index.html">Accelerated Training</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Example Notebooks</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#structure">Structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#available-example-notebooks">Available Example Notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#running-the-example-notebooks">Running the Example Notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#troubleshooting">Troubleshooting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting-started-movielens/index.html">Getting Started with MovieLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced-ops-outbrain/index.html">Advanced Ops with Outbrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../scaling-criteo/index.html">Scaling to Large Datasets with Criteo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tabular-data-rossmann/index.html">Applying Techniques to Rossmann Stores Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../multi-gpu-movielens/index.html">Multi-GPU Example Notebooks</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Winning Solution of the RecSys2020 Competition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">RecSys2020 Challenge</a></li>
<li class="toctree-l3"><a class="reference internal" href="#preparing-our-dataset">Preparing our dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#splitting-dataset-into-training-and-test">Splitting dataset into training and test</a></li>
<li class="toctree-l3"><a class="reference internal" href="#feature-engineering">Feature Engineering</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-our-model">Training our model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../api.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../resources/index.html">Additional Resources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">NVTabular</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">NVTabular Example Notebooks</a></li>
      <li class="breadcrumb-item active">NVTabular demo on RecSys2020 Challenge</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Copyright 2021 NVIDIA Corporation. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
</pre></div>
</div>
</div>
</div>
<img alt="http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png" src="http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png" />
<div class="section" id="nvtabular-demo-on-recsys2020-challenge">
<h1>NVTabular demo on RecSys2020 Challenge<a class="headerlink" href="#nvtabular-demo-on-recsys2020-challenge" title="Permalink to this headline"></a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>NVTabular is a feature engineering and preprocessing library for tabular data designed to quickly and easily manipulate terabyte scale datasets used to train deep learning based recommender systems.  It provides a high level abstraction to simplify code and accelerates computation on the GPU using the RAPIDS cuDF library.</p>
<div class="section" id="recsys2020-challenge">
<h3>RecSys2020 Challenge<a class="headerlink" href="#recsys2020-challenge" title="Permalink to this headline"></a></h3>
<p>The <a class="reference external" href="https://recsys.acm.org/">RecSys</a> conference is the leading data science conference for recommender systems and organizes an annual competiton in recommender systems. The <a class="reference external" href="https://recsys-twitter.com/">RecSys Challenge 2020</a>, hosted by Twitter, was about predicting interactions of ~200 mio. tweet-user pairs. NVIDIA’s team scored 1st place. The team explained the solution in the <a class="reference external" href="https://medium.com/rapids-ai/winning-solution-of-recsys2020-challenge-gpu-accelerated-feature-engineering-and-training-for-cd67c5a87b1f">blogpost</a> and published the code on <a class="reference external" href="https://github.com/rapidsai/deeplearning/tree/main/RecSys2020">github</a>.</p>
</div>
<div class="section" id="downloading-the-dataset">
<h3>Downloading the dataset<a class="headerlink" href="#downloading-the-dataset" title="Permalink to this headline"></a></h3>
<p>The dataset has to be downloaded from the original source, provided by Twitter. You need to create an account on the <a class="reference external" href="https://recsys-twitter.com/">RecSys Challenge 2020 website</a>. Twitter needs to (manually) approve your account, which can take a few days. After your account is approved, you can download the data <a class="reference external" href="https://recsys-twitter.com/data/show-downloads">here</a>. We will use only the <code class="docutils literal notranslate"><span class="pre">training.tsv</span></code> file, as we cannot make submissions anymore.</p>
</div>
<div class="section" id="learning-objectives">
<h3>Learning objectives<a class="headerlink" href="#learning-objectives" title="Permalink to this headline"></a></h3>
<p>This notebook covers the end-2-end pipeline, from loading the original .tsv file to training the models, with NVTabular, <a class="reference external" href="https://github.com/rapidsai/cudf">cuDF</a>, <a class="reference external" href="https://dask.org/">dask</a> and <a class="reference external" href="https://xgboost.readthedocs.io/">XGBoost</a>. We demonstrate multi-GPU support for NVTabular and new <code class="docutils literal notranslate"><span class="pre">nvt.ops</span></code>, implemented based on the success of our RecSys2020 solution.</p>
<ol class="arabic simple">
<li><p><strong>NVTabular</strong> to preprocess the original .tsv file.</p></li>
<li><p><strong>dask_cudf</strong> to split the preprocessed data into a training and validation set.</p></li>
<li><p><strong>NVTabular</strong> to create additional features with only <strong>~70 lines of code</strong>.</p></li>
<li><p><strong>dask_cudf</strong> / <strong>XGBoost on GPU</strong> to train our model.</p></li>
</ol>
</div>
</div>
<div class="section" id="id1">
<h2>RecSys2020 Challenge<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># External Dependencies
import os
import time
import glob
import gc

import cupy as cp  # CuPy is an implementation of NumPy-compatible multi-dimensional array on GPU
import cudf  # cuDF is an implementation of Pandas-like Dataframe on GPU
import dask  # dask is an open-source library to nateively scale Python on multiple workers/nodes
import dask_cudf  # dask_cudf uses dask to scale cuDF dataframes on multiple workers/nodes

import numpy as np

# NVTabular is the core library, we will use here for feature engineering/preprocessing on GPU
import nvtabular as nvt
import xgboost as xgb

# More dask / dask_cluster related libraries to scale NVTabular
from dask_cuda import LocalCUDACluster
from dask.distributed import Client

from nvtabular.ops import Categorify, DifferenceLag, FillMissing, JoinGroupby, Rename
</pre></div>
</div>
</div>
</div>
<p>Let’s have a short look on the library versions and setup, we will use.</p>
<p>We ran this example with 2x Quadro GV100 GPUs with each having 32GB of GPU memory. NVTabular used a single GPU whereas XGBoost trained with two GPUs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>time_total_start = time.time()
</pre></div>
</div>
</div>
</div>
<p>We define our input directory containing the data, and output directory to save the processed files.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>INPUT_DATA_DIR = os.environ.get(&quot;INPUT_DATA_DIR&quot;, &quot;/dataset/&quot;)
OUTPUT_DIR = os.environ.get(&quot;OUTPUT_DIR&quot;, &quot;./&quot;)
</pre></div>
</div>
</div>
</div>
<p>First, we initialize our local cuda cluster.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>cluster = LocalCUDACluster(protocol=&quot;tcp&quot;)
client = Client(cluster)
client
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table style="border: 2px solid white;">
<tr>
<td style="vertical-align: top; border: 0px solid white">
<h3 style="text-align: left;">Client</h3>
<ul style="text-align: left; list-style: none; margin: 0; padding: 0;">
  <li><b>Scheduler: </b>tcp://127.0.0.1:43407</li>
  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>
</ul>
</td>
<td style="vertical-align: top; border: 0px solid white">
<h3 style="text-align: left;">Cluster</h3>
<ul style="text-align: left; list-style:none; margin: 0; padding: 0;">
  <li><b>Workers: </b>2</li>
  <li><b>Cores: </b>2</li>
  <li><b>Memory: </b>49.28 GB</li>
</ul>
</td>
</tr>
</table></div></div>
</div>
</div>
<div class="section" id="preparing-our-dataset">
<h2>Preparing our dataset<a class="headerlink" href="#preparing-our-dataset" title="Permalink to this headline"></a></h2>
<p>The original data format had multiple inefficiencies, resulting in requiring more disk space and memory:</p>
<ol class="arabic simple">
<li><p>The file format is <code class="docutils literal notranslate"><span class="pre">.tsv</span></code>, an uncompressed, text-based format. The <code class="docutils literal notranslate"><span class="pre">parquet</span></code> file format stores tabular data in a compressed, column-oriented format. This saves a significant amount of disk space which results in fewer i/o operations and faster execution.</p></li>
<li><p>Some categorical features, such as tweet_id, user_id are hashed to String values (e.g. <code class="docutils literal notranslate"><span class="pre">cfcd208495d565ef66e7dff9f98764da</span></code>). These long Strings require significant amount of disk space/memory. We can encode the Categories as Integer values using <code class="docutils literal notranslate"><span class="pre">Categorify</span></code>. Representing the String <code class="docutils literal notranslate"><span class="pre">cfcd208495d565ef66e7dff9f98764da</span></code> as an Integer <code class="docutils literal notranslate"><span class="pre">0</span></code> can save up 90% in memory. Although other categorical features are no long hashes, they are still Strings (e.g. tweet_type) and we will represent them as Integers, as well.</p></li>
<li><p>In our experiments, the text_tokens were not a significant feature and we will drop the column before we split the data.</p></li>
</ol>
<p>First, we define the column names in the original .tsv file. The .tsv file has no header and we need to specify the names.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>features = [
    # Tweet Features
    &quot;text_tokens&quot;,
    &quot;hashtags&quot;,
    &quot;tweet_id&quot;,
    &quot;media&quot;,
    &quot;links&quot;,
    &quot;domains&quot;,
    &quot;tweet_type&quot;,
    &quot;language&quot;,
    &quot;timestamp&quot;,
    # Engaged With User Features
    &quot;a_user_id&quot;,
    &quot;a_follower_count&quot;,
    &quot;a_following_count&quot;,
    &quot;a_is_verified&quot;,
    &quot;a_account_creation&quot;,
    &quot;b_user_id&quot;,
    # Engaging User Features
    &quot;b_follower_count&quot;,
    &quot;b_following_count&quot;,
    &quot;b_is_verified&quot;,
    &quot;b_account_creation&quot;,
    &quot;b_follows_a&quot;,
    # Engagement Features
    &quot;reply&quot;,  # Target Reply
    &quot;retweet&quot;,  # Target Retweet
    &quot;retweet_comment&quot;,  # Target Retweet with comment
    &quot;like&quot;,  # Target Like
]
</pre></div>
</div>
</div>
</div>
<p>We define two helper function, we apply in our NVTabular workflow:</p>
<ol class="arabic simple">
<li><p>splitmedia2 splits the entries in media by <code class="docutils literal notranslate"><span class="pre">\t</span></code> and keeps only the first two values (if available),</p></li>
<li><p>count_token counts the number of token in a column (e.g. how many hashtags are in a tweet),</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def splitmedia(col):
    if col.shape[0] == 0:
        return col
    else:
        return (
            col.str.split(&quot;\t&quot;, expand=True)[0].fillna(&quot;&quot;)
            + &quot;_&quot;
            + col.str.split(&quot;\t&quot;, expand=True)[1].fillna(&quot;&quot;)
        )


def count_token(col, token):
    not_null = col.isnull() == 0
    return ((col.str.count(token) + 1) * not_null).fillna(0)
</pre></div>
</div>
</div>
</div>
<p>We will define our data processing pipeline with NVTabular.<br><br>
We count the number of tokens in the columns hashtags, domains, links. We use the <code class="docutils literal notranslate"><span class="pre">count_token</span></code> helper function in a <code class="docutils literal notranslate"><span class="pre">lambda</span></code> function. Finally, we rename the column names to avoid duplicated names.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>count_features = (
    nvt.ColumnGroup([&quot;hashtags&quot;, &quot;domains&quot;, &quot;links&quot;])
    &gt;&gt; (lambda col: count_token(col, &quot;\t&quot;))
    &gt;&gt; Rename(postfix=&quot;_count_t&quot;)
)
</pre></div>
</div>
</div>
</div>
<p>We apply splitmedia function to split the media.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>split_media = nvt.ColumnGroup([&quot;media&quot;]) &gt;&gt; (lambda col: splitmedia(col))
</pre></div>
</div>
</div>
</div>
<p>We encode categorical columns as a small, continuous integer to save memory. Some categorical columns contain long hashes of type String as values to preserve the privacy of the users (e.g. userId, language, etc.). Long hashes of type String requires significant amount of memory to store. We encode/map the Strings to continuous Integer to save significant memory.
<br><br>
Before we can apply <code class="docutils literal notranslate"><span class="pre">Categorify</span></code>, we need to fill na/missing values in the columns <code class="docutils literal notranslate"><span class="pre">hashtags</span></code>, <code class="docutils literal notranslate"><span class="pre">domains</span></code> and <code class="docutils literal notranslate"><span class="pre">links</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>multihot_filled = [&quot;hashtags&quot;, &quot;domains&quot;, &quot;links&quot;] &gt;&gt; FillMissing()
cat_columns = [&quot;language&quot;, &quot;tweet_type&quot;, &quot;tweet_id&quot;, &quot;a_user_id&quot;, &quot;b_user_id&quot;]
cat_features = split_media + multihot_filled + cat_columns &gt;&gt; Categorify(out_path=OUTPUT_DIR)
</pre></div>
</div>
</div>
</div>
<p>We want to fill na/missing values in the label columns as well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>label_name = [&quot;reply&quot;, &quot;retweet&quot;, &quot;retweet_comment&quot;, &quot;like&quot;]
label_name_feature = label_name &gt;&gt; nvt.ops.FillMissing()
</pre></div>
</div>
</div>
</div>
<p>We extract the weekday from the timestamp.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>weekday = (
    nvt.ColumnGroup([&quot;timestamp&quot;])
    &gt;&gt; (lambda col: cudf.to_datetime(col, unit=&quot;s&quot;).dt.weekday)
    &gt;&gt; nvt.ops.Rename(postfix=&quot;_wd&quot;)
)
</pre></div>
</div>
</div>
</div>
<p>We can visualize our pipeline.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>output = count_features + cat_features + label_name_feature + weekday
(output).graph
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/37cd9c366c929d0ee424188610d851784ed3edc2bceb37993110ca1c15b7a10f.svg" src="../../_images/37cd9c366c929d0ee424188610d851784ed3edc2bceb37993110ca1c15b7a10f.svg" /></div>
</div>
<p>Our calculation workflow looks correct. But we want to keep columns, which are not used in our pipeline (for <code class="docutils literal notranslate"><span class="pre">a_follower_count</span></code> or <code class="docutils literal notranslate"><span class="pre">b_follows_a</span></code>. Therefore, we include all columns in features, which are not part of our pipeline (except of <code class="docutils literal notranslate"><span class="pre">text_tokens</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>remaining_columns = [x for x in features if x not in (output.columns + [&quot;text_tokens&quot;])]
remaining_columns
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;timestamp&#39;,
 &#39;a_follower_count&#39;,
 &#39;a_following_count&#39;,
 &#39;a_is_verified&#39;,
 &#39;a_account_creation&#39;,
 &#39;b_follower_count&#39;,
 &#39;b_following_count&#39;,
 &#39;b_is_verified&#39;,
 &#39;b_account_creation&#39;,
 &#39;b_follows_a&#39;]
</pre></div>
</div>
</div>
</div>
<p>We initialize our NVTabular workflow.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>proc = nvt.Workflow(output + remaining_columns)
</pre></div>
</div>
</div>
</div>
<p>We initialize a nvt.Dataset. The engine is <code class="docutils literal notranslate"><span class="pre">csv</span></code> as the <code class="docutils literal notranslate"><span class="pre">.tsv</span></code> file has a similar structure. The <code class="docutils literal notranslate"><span class="pre">.tsv</span></code> file uses the special character <code class="docutils literal notranslate"><span class="pre">\x01</span></code> to separate columns. There is no header in the file and we define column names with the parameter <code class="docutils literal notranslate"><span class="pre">names</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>trains_itrs = nvt.Dataset(
    INPUT_DATA_DIR + &quot;training.tsv&quot;,
    header=None,
    names=features,
    engine=&quot;csv&quot;,
    sep=&quot;\x01&quot;,
    part_size=&quot;1GB&quot;,
)
</pre></div>
</div>
</div>
</div>
<p>First, we collect the training dataset statistics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%time

time_preproc_start = time.time()
proc.fit(trains_itrs)
time_preproc = time.time() - time_preproc_start
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 1min 11s, sys: 1min 4s, total: 2min 16s
Wall time: 3min 12s
</pre></div>
</div>
</div>
</div>
<p>Next, we apply the transformation to the dataset and persist it to disk.<br><br>
We define the output datatypes for continuous columns to save memory. We can define the output datatypes as a dict and parse it to the <code class="docutils literal notranslate"><span class="pre">to_parquet</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>dict_dtypes = {}
for col in label_name + [
    &quot;media&quot;,
    &quot;language&quot;,
    &quot;tweet_type&quot;,
    &quot;tweet_id&quot;,
    &quot;a_user_id&quot;,
    &quot;b_user_id&quot;,
    &quot;hashtags&quot;,
    &quot;domains&quot;,
    &quot;links&quot;,
    &quot;timestamp&quot;,
    &quot;a_follower_count&quot;,
    &quot;a_following_count&quot;,
    &quot;a_account_creation&quot;,
    &quot;b_follower_count&quot;,
    &quot;b_following_count&quot;,
    &quot;b_account_creation&quot;,
]:
    dict_dtypes[col] = np.uint32
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%time

time_preproc_start = time.time()
proc.transform(trains_itrs).to_parquet(
    output_path=OUTPUT_DIR + &quot;preprocess/&quot;, dtypes=dict_dtypes, out_files_per_proc=10
)
time_preproc += time.time() - time_preproc_start
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 1min 49s, sys: 1min 42s, total: 3min 31s
Wall time: 4min 40s
</pre></div>
</div>
</div>
</div>
<p>We can take a look in the output folder.</p>
</div>
<div class="section" id="splitting-dataset-into-training-and-test">
<h2>Splitting dataset into training and test<a class="headerlink" href="#splitting-dataset-into-training-and-test" title="Permalink to this headline"></a></h2>
<p>We split the training data by time into a train and validation set. The first 5 days are train and the last 2 days are for validation. We use the weekday for it. The first day of the dataset is a Thursday (weekday id = 3) and the last day is Wednesday (weekday id = 2). Therefore, we split the weekday ids 1 and 2 into the validation set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%time

time_split_start = time.time()

df = dask_cudf.read_parquet(os.path.join(OUTPUT_DIR, &quot;preprocess/*.parquet&quot;))
if &quot;text_tokens&quot; in list(df.columns):
    df = df.drop(&quot;text_tokens&quot;, axis=1)
VALID_DOW = [1, 2]

valid = df[df[&quot;timestamp_wd&quot;].isin(VALID_DOW)].reset_index(drop=True)
train = df[~df[&quot;timestamp_wd&quot;].isin(VALID_DOW)].reset_index(drop=True)
train = train.sort_values([&quot;b_user_id&quot;, &quot;timestamp&quot;]).reset_index(drop=True)
valid = valid.sort_values([&quot;b_user_id&quot;, &quot;timestamp&quot;]).reset_index(drop=True)
train.to_parquet(OUTPUT_DIR + &quot;nv_train/&quot;)
valid.to_parquet(OUTPUT_DIR + &quot;nv_valid/&quot;)
time_split = time.time() - time_split_start

del train
del valid
gc.collect()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 3.04 s, sys: 332 ms, total: 3.37 s
Wall time: 31.5 s
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>9113
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="feature-engineering">
<h2>Feature Engineering<a class="headerlink" href="#feature-engineering" title="Permalink to this headline"></a></h2>
<p>Now, we can apply the actual feature engineering. We define our data pipelines.</p>
<p>We count encode the columns <em>media</em>, <em>tweet_type</em>, <em>language</em>, <em>a_user_id</em>, <em>b_user_id</em>. CountEncoding is explained <a class="reference external" href="https://github.com/rapidsai/deeplearning/blob/main/RecSys2020Tutorial/03_4_CountEncoding.ipynb">here</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>count_encode = (
    [&quot;media&quot;, &quot;tweet_type&quot;, &quot;language&quot;, &quot;a_user_id&quot;, &quot;b_user_id&quot;]
    &gt;&gt; Rename(postfix=&quot;_c&quot;)
    &gt;&gt; JoinGroupby(cont_cols=[&quot;reply&quot;], stats=[&quot;count&quot;], out_path=&quot;./&quot;)
)
</pre></div>
</div>
</div>
</div>
<p>We transform timestamp to datetime type.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>datetime = nvt.ColumnGroup([&quot;timestamp&quot;]) &gt;&gt; (
    lambda col: cudf.to_datetime(col.astype(&quot;int32&quot;), unit=&quot;s&quot;)
)
</pre></div>
</div>
</div>
</div>
<p>We extract hour from datetime.<br>
We extract minute from datetime.<br>
We extract seconds from datetime</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>hour = datetime &gt;&gt; (lambda col: col.dt.hour) &gt;&gt; Rename(postfix=&quot;_hour&quot;)
minute = datetime &gt;&gt; (lambda col: col.dt.minute) &gt;&gt; Rename(postfix=&quot;_minute&quot;)
seconds = datetime &gt;&gt; (lambda col: col.dt.second) &gt;&gt; Rename(postfix=&quot;_second&quot;)
</pre></div>
</div>
</div>
</div>
<p>We difference encode <em>b_follower_count, b_following_count, language</em> grouped by <em>b_user_id</em>. DifferenceEncoding is explained <a class="reference external" href="https://github.com/rapidsai/tdeeplearning/blob/main/RecSys2020Tutorial/05_2_TimeSeries_Differences.ipynb">here</a>. First, we need to transform the datatype to float32 to prevent overflow/underflow. After DifferenceEncoding, we want to fill NaN values with 0.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>diff_lag = (
    nvt.ColumnGroup([&quot;b_follower_count&quot;, &quot;b_following_count&quot;, &quot;language&quot;])
    &gt;&gt; (lambda col: col.astype(&quot;float32&quot;))
    &gt;&gt; DifferenceLag(partition_cols=[&quot;b_user_id&quot;], shift=[1, -1])
    &gt;&gt; FillMissing(fill_val=0)
)
</pre></div>
</div>
</div>
</div>
<p>We need to transform the LABEL_COLUMNS into boolean (0/1) targets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>LABEL_COLUMNS = [&quot;reply&quot;, &quot;retweet&quot;, &quot;retweet_comment&quot;, &quot;like&quot;]
labels = nvt.ColumnGroup(LABEL_COLUMNS) &gt;&gt; (lambda col: (col &gt; 0).astype(&quot;int8&quot;))
</pre></div>
</div>
</div>
</div>
<p>We apply TargetEncoding with kfold of 5 and smoothing of 20. TargetEncoding is explained in <a class="reference external" href="https://medium.com/rapids-ai/target-encoding-with-rapids-cuml-do-more-with-your-categorical-data-8c762c79e784">here</a> and <a class="reference external" href="https://github.com/rapidsai/deeplearning/blob/main/RecSys2020Tutorial/03_3_TargetEncoding.ipynb">here</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>target_encode = [
    &quot;media&quot;,
    &quot;tweet_type&quot;,
    &quot;language&quot;,
    &quot;a_user_id&quot;,
    &quot;b_user_id&quot;,
    [&quot;domains&quot;, &quot;language&quot;, &quot;b_follows_a&quot;, &quot;tweet_type&quot;, &quot;media&quot;, &quot;a_is_verified&quot;],
] &gt;&gt; nvt.ops.TargetEncoding(
    labels,
    kfold=5,
    p_smooth=20,
    out_dtype=&quot;float32&quot;,
)
</pre></div>
</div>
</div>
</div>
<p>We visualize our NVTabular workflow.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>output = count_encode + hour + minute + seconds + diff_lag + labels + target_encode
(output).graph
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/c0d0af4a634a0b78d0e548c7de340b73d09434a328604b45aeb505faec3c4ab6.svg" src="../../_images/c0d0af4a634a0b78d0e548c7de340b73d09434a328604b45aeb505faec3c4ab6.svg" /></div>
</div>
<p>We want to keep all columns of the input dataset. Therefore, we extract all column names from the first input parquet file.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>df_tmp = cudf.read_parquet(OUTPUT_DIR + &quot;/nv_train/part.0.parquet&quot;)
all_input_columns = df_tmp.columns
del df_tmp
gc.collect()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>37
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>remaining_columns = [x for x in all_input_columns if x not in (output.columns + [&quot;text_tokens&quot;])]
remaining_columns
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;hashtags_count_t&#39;,
 &#39;domains_count_t&#39;,
 &#39;links_count_t&#39;,
 &#39;media&#39;,
 &#39;hashtags&#39;,
 &#39;domains&#39;,
 &#39;links&#39;,
 &#39;language&#39;,
 &#39;tweet_type&#39;,
 &#39;tweet_id&#39;,
 &#39;a_user_id&#39;,
 &#39;b_user_id&#39;,
 &#39;timestamp_wd&#39;,
 &#39;timestamp&#39;,
 &#39;a_follower_count&#39;,
 &#39;a_following_count&#39;,
 &#39;a_is_verified&#39;,
 &#39;a_account_creation&#39;,
 &#39;b_follower_count&#39;,
 &#39;b_following_count&#39;,
 &#39;b_is_verified&#39;,
 &#39;b_account_creation&#39;,
 &#39;b_follows_a&#39;]
</pre></div>
</div>
</div>
</div>
<p>We initialize our NVTabular workflow and add the “remaining” columns to it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>proc = nvt.Workflow(output + remaining_columns)
</pre></div>
</div>
</div>
</div>
<p>We initialize the train and valid as NVTabular datasets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>train_dataset = nvt.Dataset(
    glob.glob(OUTPUT_DIR + &quot;nv_train/*.parquet&quot;), engine=&quot;parquet&quot;, part_size=&quot;2GB&quot;
)
valid_dataset = nvt.Dataset(
    glob.glob(OUTPUT_DIR + &quot;nv_valid/*.parquet&quot;), engine=&quot;parquet&quot;, part_size=&quot;2GB&quot;
)
</pre></div>
</div>
</div>
</div>
<p>We collect statistics from our train dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%time

time_fe_start = time.time()
proc.fit(train_dataset)
time_fe = time.time() - time_fe_start
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 27.6 s, sys: 10.9 s, total: 38.4 s
Wall time: 42.3 s
</pre></div>
</div>
</div>
</div>
<p>The columns <em>a_is_verified</em>, <em>b_is_verified</em> and <em>b_follows_a</em> have the datatype boolean. XGBoost does not support boolean datatypes and we need convert them to int8. We can define the output datatypes as a dict and parse it to the <code class="docutils literal notranslate"><span class="pre">.to_parquet</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>dict_dtypes = {}
for col in [&quot;a_is_verified&quot;, &quot;b_is_verified&quot;, &quot;b_follows_a&quot;]:
    dict_dtypes[col] = np.int8
</pre></div>
</div>
</div>
</div>
<p>We apply the transformation to the train and valid datasets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%time

time_fe_start = time.time()
proc.transform(train_dataset).to_parquet(
    output_path=OUTPUT_DIR + &quot;nv_train_fe/&quot;, dtypes=dict_dtypes
)
proc.transform(valid_dataset).to_parquet(
    output_path=OUTPUT_DIR + &quot;nv_valid_fe/&quot;, dtypes=dict_dtypes
)
time_fe += time.time() - time_fe_start
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 30.9 s, sys: 44.3 s, total: 1min 15s
Wall time: 1min 20s
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="training-our-model">
<h2>Training our model<a class="headerlink" href="#training-our-model" title="Permalink to this headline"></a></h2>
<p>After the preprocessing and feature engineering is done, we can train a model to predict our targets. We load our datasets with <code class="docutils literal notranslate"><span class="pre">dask_cudf</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>train = dask_cudf.read_parquet(os.path.join(OUTPUT_DIR, &quot;nv_train_fe/*.parquet&quot;))
valid = dask_cudf.read_parquet(os.path.join(OUTPUT_DIR, &quot;nv_valid_fe/*.parquet&quot;))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>train[[&quot;a_is_verified&quot;, &quot;b_is_verified&quot;, &quot;b_follows_a&quot;]].dtypes
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>a_is_verified    int8
b_is_verified    int8
b_follows_a      int8
dtype: object
</pre></div>
</div>
</div>
</div>
<p>Some columns are only used for feature engineering. Therefore, we define the columns we want to ignore for training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>dont_use = [
    &quot;__null_dask_index__&quot;,
    &quot;text_tokens&quot;,
    &quot;timestamp&quot;,
    &quot;a_account_creation&quot;,
    &quot;b_account_creation&quot;,
    &quot;hashtags&quot;,
    &quot;tweet_id&quot;,
    &quot;links&quot;,
    &quot;domains&quot;,
    &quot;a_user_id&quot;,
    &quot;b_user_id&quot;,
    &quot;timestamp_wd&quot;,
    &quot;timestamp_to_datetime&quot;,
    &quot;a_following_count_a_ff_rate&quot;,
    &quot;b_following_count_b_ff_rate&quot;,
]
dont_use = [x for x in train.columns if x in dont_use]
label_names = [&quot;reply&quot;, &quot;retweet&quot;, &quot;retweet_comment&quot;, &quot;like&quot;]
</pre></div>
</div>
</div>
</div>
<p>We drop the columns, which are not required for training.</p>
<p>Our experiments show that we require only 10% of the training dataset. Our feature engineering, such as TargetEncoding, uses the training datasets and leverage the information of the full dataset. In the competition, we trained our models with higher ratio (20% and 50%), but we could not observe an improvement in performance.<br><br>
We sample the training dataset to 10% of the size and drop all columns, which we do not want to use.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>SAMPLE_RATIO = 0.1
SEED = 1

if SAMPLE_RATIO &lt; 1.0:
    train[&quot;sample&quot;] = train[&quot;tweet_id&quot;].map_partitions(lambda cudf_df: cudf_df.hash_encode(stop=10))
    print(len(train))

    train = train[train[&quot;sample&quot;] &lt; 10 * SAMPLE_RATIO]
    (train,) = dask.persist(train)
    print(len(train))


Y_train = train[label_names]
(Y_train,) = dask.persist(Y_train)

train = train.drop([&quot;sample&quot;] + label_names + dont_use, axis=1)
(train,) = dask.persist(train)

print(&quot;Using %i features&quot; % (train.shape[1]))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>87914868
8764882
Using 51 features
</pre></div>
</div>
</div>
</div>
<p>Similar to the training dataset, our experiments show that 35% of our validation dataset is enough to get a good estimate of the performance metric. 35% of the validation dataset has a similar size as the test set of the RecSys2020 competition.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>SAMPLE_RATIO = 0.35
SEED = 1
if SAMPLE_RATIO &lt; 1.0:
    print(len(valid))
    valid[&quot;sample&quot;] = valid[&quot;tweet_id&quot;].map_partitions(lambda cudf_df: cudf_df.hash_encode(stop=10))

    valid = valid[valid[&quot;sample&quot;] &lt; 10 * SAMPLE_RATIO]
    (valid,) = dask.persist(valid)
    print(len(valid))

Y_valid = valid[label_names]
(Y_valid,) = dask.persist(Y_valid)

valid = valid.drop([&quot;sample&quot;] + label_names + dont_use, axis=1)
(valid,) = dask.persist(valid)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>33471563
13367169
</pre></div>
</div>
</div>
</div>
<p>We initialize our XGBoost parameter.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(&quot;XGB Version&quot;, xgb.__version__)

xgb_parms = {
    &quot;max_depth&quot;: 8,
    &quot;learning_rate&quot;: 0.1,
    &quot;subsample&quot;: 0.8,
    &quot;colsample_bytree&quot;: 0.3,
    &quot;eval_metric&quot;: &quot;logloss&quot;,
    &quot;objective&quot;: &quot;binary:logistic&quot;,
    &quot;tree_method&quot;: &quot;gpu_hist&quot;,
    &quot;predictor&quot;: &quot;gpu_predictor&quot;,
}
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>XGB Version 1.3.3
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>train, valid = dask.persist(train, valid)
</pre></div>
</div>
</div>
</div>
<p>We train our XGBoost models. The challenge requires to predict 4 targets, does a user</p>
<ol class="arabic simple">
<li><p>like a tweet</p></li>
<li><p>reply a tweet</p></li>
<li><p>comment a tweet</p></li>
<li><p>comment and reply a tweet</p></li>
</ol>
<p>We train 4x XGBoost models for 300 rounds on a GPU.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%time
time_train_start = time.time()

NROUND = 300
VERBOSE_EVAL = 50
preds = []
for i in range(4):

    name = label_names[i]
    print(&quot;#&quot; * 25)
    print(&quot;###&quot;, name)
    print(&quot;#&quot; * 25)

    start = time.time()
    print(&quot;Creating DMatrix...&quot;)
    dtrain = xgb.dask.DaskDMatrix(client, data=train, label=Y_train.iloc[:, i])
    print(&quot;Took %.1f seconds&quot; % (time.time() - start))

    start = time.time()
    print(&quot;Training...&quot;)
    model = xgb.dask.train(
        client, xgb_parms, dtrain=dtrain, num_boost_round=NROUND, verbose_eval=VERBOSE_EVAL
    )
    print(&quot;Took %.1f seconds&quot; % (time.time() - start))

    start = time.time()
    print(&quot;Predicting...&quot;)
    preds.append(xgb.dask.predict(client, model, valid))
    print(&quot;Took %.1f seconds&quot; % (time.time() - start))

    del model, dtrain

time_train = time.time() - time_train_start
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>#########################
### reply
#########################
Creating DMatrix...
Took 1.0 seconds
Training...
Took 16.8 seconds
Predicting...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/conda/envs/rapids/lib/python3.8/site-packages/distributed/worker.py:3451: UserWarning: Large object of size 7.93 MB detected in task graph: 
  [&lt;function _predict_async.&lt;locals&gt;.mapped_predict  ... titions&gt;, True]
Consider scattering large objects ahead of time
with client.scatter to reduce scheduler burden and 
keep data on workers

    future = client.submit(func, big_data)    # bad

    big_future = client.scatter(big_data)     # good
    future = client.submit(func, big_future)  # good
  warnings.warn(
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Took 1.8 seconds
#########################
### retweet
#########################
Creating DMatrix...
Took 0.5 seconds
Training...
Took 16.6 seconds
Predicting...
Took 1.7 seconds
#########################
### retweet_comment
#########################
Creating DMatrix...
Took 0.6 seconds
Training...
Took 15.4 seconds
Predicting...
Took 1.3 seconds
#########################
### like
#########################
Creating DMatrix...
Took 0.4 seconds
Training...
Took 16.9 seconds
Predicting...
Took 1.7 seconds
CPU times: user 8.7 s, sys: 1.06 s, total: 9.75 s
Wall time: 1min 14s
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>yvalid = Y_valid[label_names].values.compute()
oof = cp.array([i.values.compute() for i in preds]).T
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>yvalid.shape
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(13367169, 4)
</pre></div>
</div>
</div>
</div>
<p>The hosts of the RecSys2020 competition provide code for calculating the performance metric <code class="docutils literal notranslate"><span class="pre">PRAUC</span></code> and <code class="docutils literal notranslate"><span class="pre">RCE</span></code>. We optimized the code to speed up the calculation, as well. Using cuDF / cupy, we calculate the performance metric on the GPU.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from sklearn.metrics import auc


def precision_recall_curve(y_true, y_pred):
    y_true = y_true.astype(&quot;float32&quot;)
    ids = cp.argsort(-y_pred)
    y_true = y_true[ids]
    y_pred = y_pred[ids]
    y_pred = cp.flip(y_pred, axis=0)

    acc_one = cp.cumsum(y_true)
    sum_one = cp.sum(y_true)

    precision = cp.flip(acc_one / cp.cumsum(cp.ones(len(y_true))), axis=0)
    precision[:-1] = precision[1:]
    precision[-1] = 1.0

    recall = cp.flip(acc_one / sum_one, axis=0)
    recall[:-1] = recall[1:]
    recall[-1] = 0
    n = (recall == 1).sum()

    return precision[n - 1 :], recall[n - 1 :], y_pred[n:]


def compute_prauc(pred, gt):
    prec, recall, thresh = precision_recall_curve(gt, pred)
    recall, prec = cp.asnumpy(recall), cp.asnumpy(prec)

    prauc = auc(recall, prec)
    return prauc


def log_loss(y_true, y_pred, eps=1e-7, normalize=True, sample_weight=None):
    y_true = y_true.astype(&quot;int32&quot;)
    y_pred = cp.clip(y_pred, eps, 1 - eps)
    if y_pred.ndim == 1:
        y_pred = cp.expand_dims(y_pred, axis=1)
    if y_pred.shape[1] == 1:
        y_pred = cp.hstack([1 - y_pred, y_pred])

    y_pred /= cp.sum(y_pred, axis=1, keepdims=True)
    loss = -cp.log(y_pred)[cp.arange(y_pred.shape[0]), y_true]
    return _weighted_sum(loss, sample_weight, normalize).item()


def _weighted_sum(sample_score, sample_weight, normalize):
    if normalize:
        return cp.average(sample_score, weights=sample_weight)
    elif sample_weight is not None:
        return cp.dot(sample_score, sample_weight)
    else:
        return sample_score.sum()


def compute_rce_fast(pred, gt):
    cross_entropy = log_loss(gt, pred)
    yt = cp.mean(gt).item()
    # cross_entropy and yt are single numbers (no arrays) and using CPU is fast
    strawman_cross_entropy = -(yt * np.log(yt) + (1 - yt) * np.log(1 - yt))
    return (1.0 - cross_entropy / strawman_cross_entropy) * 100.0
</pre></div>
</div>
</div>
</div>
<p>Finally, we calculate the performance metric PRAUC and RCE for each target.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>txt = &quot;&quot;
for i in range(4):
    prauc = compute_prauc(oof[:, i], yvalid[:, i])
    rce = compute_rce_fast(oof[:, i], yvalid[:, i]).item()
    txt_ = f&quot;{label_names[i]:20} PRAUC:{prauc:.5f} RCE:{rce:.5f}&quot;
    print(txt_)
    txt += txt_ + &quot;\n&quot;
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>reply                PRAUC:0.13491 RCE:15.57019
retweet              PRAUC:0.50176 RCE:25.95678
retweet_comment      PRAUC:0.04228 RCE:9.24986
like                 PRAUC:0.76676 RCE:23.75791
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>time_total = time.time() - time_total_start
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(&quot;Total time: {:.2f}s&quot;.format(time_total))
print()
print(&quot;1. Preprocessing:       {:.2f}s&quot;.format(time_preproc))
print(&quot;2. Splitting:           {:.2f}s&quot;.format(time_split))
print(&quot;3. Feature engineering: {:.2f}s&quot;.format(time_fe))
print(&quot;4. Training:            {:.2f}s&quot;.format(time_train))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total time: 848.91s

1. Preprocessing:       472.54s
2. Splitting:           31.39s
3. Feature engineering: 122.55s
4. Training:            74.65s
</pre></div>
</div>
</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../multi-gpu-toy-example/multi-gpu_dask.html" class="btn btn-neutral float-left" title="Multi-GPU Scaling in NVTabular with Dask" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../api.html" class="btn btn-neutral float-right" title="API Documentation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: v1.4.0
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../../v1.3.2/index.html">v1.3.2</a></dd>
      <dd><a href="../../../v1.3.3/index.html">v1.3.3</a></dd>
      <dd><a href="01-02-04-Download-Convert-ETL-with-NVTabular-Training-with-XGBoost.html">v1.4.0</a></dd>
      <dd><a href="../../../v1.5.0/index.html">v1.5.0</a></dd>
      <dd><a href="../../../v1.6.0/index.html">v1.6.0</a></dd>
      <dd><a href="../../../v1.7.0/index.html">v1.7.0</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="../../../main/index.html">main</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NVJ1Y1YJHK"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-NVJ1Y1YJHK', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>