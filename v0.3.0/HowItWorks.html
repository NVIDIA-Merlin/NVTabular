<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>How it Works &mdash; NVTabular 2020 documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Examples" href="examples/index.html" />
    <link rel="prev" title="NVTabular | API documentation" href="Introduction.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> NVTabular
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">Introduction</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">How it Works</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#operations">Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#a-higher-level-of-abstraction">A higher level of abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#framework-interoperability">Framework Interoperability</a></li>
<li class="toctree-l2"><a class="reference internal" href="#multi-gpu-support">Multi-GPU Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="#multi-node-support">Multi-Node Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="#multihot-encoding-and-pre-existing-embeddings">MultiHot Encoding and Pre-existing Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cpu-support">CPU Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="#getting-your-data-ready-for-nvtabular">Getting your data ready for NVTabular</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#troubleshooting">Troubleshooting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#checking-the-schema-of-parquet-files">Checking the schema of parquet files</a></li>
<li class="toctree-l4"><a class="reference internal" href="#setting-the-row-group-size-of-parquet-files">Setting the row group size of parquet files</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="examples/index.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/index.html">API Documentation</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVTabular</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>How it Works</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="how-it-works">
<h1>How it Works<a class="headerlink" href="#how-it-works" title="Permalink to this headline"></a></h1>
<p><img alt="NVTabular Workflow" src="_images/nvt_workflow.png" /></p>
<p>With the transition to v0.2 the NVTabular engine uses the <a class="reference external" href="http://www.rapids.ai">RAPIDS</a> <a class="reference external" href="https://github.com/rapidsai/dask-cuda">Dask-cuDF library</a> which provides the bulk of the functionality, accelerating dataframe operations on the GPU, and scaling across multiple GPUs.  NVTabular provides functionality commonly found in deep learning recommendation workflows, allowing you to focus on what you want to do with your data, not how you need to do it.  We also provide a template for our core compute mechanism, Operations, or ‘ops’ allowing you to build your own custom ops from cuDF and other libraries.</p>
<p>Follow our <a class="reference external" href="https://nvidia.github.io/NVTabular/main/Introduction.html#getting-started">getting started guide</a> to get NVTabular installed on your container or system.  Once installed you can setup a workflow in the following way:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nvtabular</span> <span class="k">as</span> <span class="nn">nvt</span>
<span class="n">workflow</span> <span class="o">=</span> <span class="n">nvt</span><span class="o">.</span><span class="n">Workflow</span><span class="p">(</span>
    <span class="n">cat_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;user_id&quot;</span><span class="p">,</span> <span class="s2">&quot;item_id&quot;</span><span class="p">,</span> <span class="s2">&quot;city&quot;</span><span class="p">],</span>
    <span class="n">cont_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;age&quot;</span><span class="p">,</span> <span class="s2">&quot;time_of_day&quot;</span><span class="p">,</span> <span class="s2">&quot;item_num_views&quot;</span><span class="p">],</span>
    <span class="n">label_name</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>With the workflow in place we can now explore the library in detail.</p>
<div class="section" id="operations">
<h2>Operations<a class="headerlink" href="#operations" title="Permalink to this headline"></a></h2>
<p>Operations are a reflection of the way in which compute happens on the GPU across large datasets.  At a high level we’re concerned with two types of compute: the type that touches the entire dataset (or some large chunk of it) and the type that operates on a single row.  Operations split the compute such that the first phase, which we call statistics gathering, is the only place where operations that cross the row boundary can take place.  An example of this would be in the Normalize op which relies on two statistics, the mean and standard deviation.  In order to normalize a row, we must first have calculated these two values, and we use a Dask-cudf graph to compute this part of the op.</p>
<p>The second phase of operations is the apply phase, which uses the statistics created earlier to modify the dataset, transforming the data.  Notably we allow for the application of transforms not only during the modification of the dataset, but also during dataloading, with plans to support the same transforms during inference.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># by default, the op will be applied to _all_</span>
<span class="c1"># columns of the associated variable type</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_cont_preprocess</span><span class="p">(</span><span class="n">nvt</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">Normalize</span><span class="p">())</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">nvt</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="s2">&quot;/path/to/data.parquet&quot;</span><span class="p">)</span>

<span class="c1"># record stats, transform the dataset, and export</span>
<span class="c1"># the transformed data to a parquet file</span>
<span class="n">proc</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">nvt</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">Shuffle</span><span class="o">.</span><span class="n">PER_WORKER</span><span class="p">,</span> <span class="n">output_path</span><span class="o">=</span><span class="s2">&quot;/path/to/export/dir&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Dask-cuDF does the scheduling to help optimize the task graph providing an optimal solution to whatever configuration of GPUs you have, from a single GPU to a cluster of many.</p>
</div>
<div class="section" id="a-higher-level-of-abstraction">
<h2>A higher level of abstraction<a class="headerlink" href="#a-higher-level-of-abstraction" title="Permalink to this headline"></a></h2>
<p>NVTabular code is targeted at the operator level, not the dataframe level, providing a method for specifying the operation you want to perform, and the columns or type of data that you want to perform it on.</p>
<p>We make an explicit distinction between feature engineering ops, which create new variables, and preprocessing ops which transform data more directly to make it ready for the model to which it’s feeding.  While the type of computation involved in these two stages is often similar, we want to allow for the creation of new features that will then be preprocessed in the same way as other input variables.</p>
<p>Two main data types are currently supported: categorical variables and continuous variables. Feature engineering operators explicitly take as input one or more continuous or categorical columns and produce one or more columns of a specific type.  By default the input columns used to create the new feature are also included in the output, however this can be overridden with the [replace] keyword in the operator. We extend this to multihot categoricals with the transition to v0.3 in addition to providing specific functionality for high cardinality categoricals which must be treated differently due to memory constraints.</p>
<p>Preprocessing operators take in a set of columns of the same type and perform the operation across each column, transforming the output during the final operation into a long tensor in the case of categorical variables or a float tensor in the case of continuous variables.  Preprocessing operations replace the column values with their new representation by default, but again we allow the user to override this.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># same example as before, but now only apply normalization</span>
<span class="c1"># to `age` and `item_num_views` columns, which will create</span>
<span class="c1"># new columns `age_normalize` and `item_num_views_normalize`</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_cont_preprocess</span><span class="p">(</span><span class="n">nvt</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;age&quot;</span><span class="p">,</span> <span class="s2">&quot;item_num_views&quot;</span><span class="p">],</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">nvt</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="s2">&quot;/path/to/data.parquet&quot;</span><span class="p">)</span>
<span class="n">proc</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">nvt</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">Shuffle</span><span class="o">.</span><span class="n">PER_WORKER</span><span class="p">,</span> <span class="n">output_path</span><span class="o">=</span><span class="s2">&quot;/path/to/export/dir&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Operators may also be chained to allow for more complex feature engineering or preprocessing.  Chaining of operators is done by creating a list of the operators.  By default only the final operator in a chain that includes preprocessing will be included in the output with all other intermediate steps implicitly dropped.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Replace negative and missing values with 0 and then take log(1+x)</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_cont_feature</span><span class="p">([</span><span class="n">FillMissing</span><span class="p">(),</span> <span class="n">Clip</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">LogOp</span><span class="p">()])</span>

<span class="c1"># then normalize</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_cont_preprocess</span><span class="p">(</span><span class="n">Normalize</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="section" id="framework-interoperability">
<h2>Framework Interoperability<a class="headerlink" href="#framework-interoperability" title="Permalink to this headline"></a></h2>
<p>In addition to providing mechanisms for transforming the data to prepare it for deep learning models we also provide framework-specific dataloaders to help optimize getting that data to the GPU.  Under a traditional dataloading scheme, data is read in item by item and collated into a batch.  PyTorch allows for multiple processes to create many batches at the same time, however this still leads to many individual rows of tabular data accessed independently which impacts I/O, especially when this data is on the disk and not in CPU memory.  TensorFlow loads and shuffles TFRecords by adopting a windowed buffering scheme that loads data sequentially to a buffer, from which it randomly samples batches and replenishes with the next sequential elements from disk. Larger buffer sizes ensure more randomness, but can quickly bottleneck performance as TensorFlow tries to keep the buffer saturated. Smaller buffer sizes mean that datasets which aren’t uniformly distributed on disk lead to biased sampling and potentially degraded convergence.</p>
<p>In NVTabular we provide an option to shuffle during dataset creation, creating a uniformly shuffled dataset allowing the dataloader to read in contiguous chunks of data that are already randomized across the entire dataset.  NVTabular provides the option to control the number of chunks that are combined into a batch, allowing the end user flexibility when trading off between performance and true randomization.  This mechanism is critical when dealing with datasets that exceed CPU memory and per epoch shuffling is desired during training.  Full shuffle of such a dataset can exceed training time for the epoch by several orders of magnitude.</p>
<p>Stay tuned for benchmarks on our dataloader performance as compared to those native frameworks.</p>
</div>
<div class="section" id="multi-gpu-support">
<h2>Multi-GPU Support<a class="headerlink" href="#multi-gpu-support" title="Permalink to this headline"></a></h2>
<p>NVTabular supports multi-GPU scaling with <a class="reference external" href="https://github.com/rapidsai/dask-cuda">Dask-CUDA</a> and <a class="reference external" href="https://distributed.dask.org/en/latest/">dask.distributed</a>.  To enable distributed parallelism, the NVTabular <code class="docutils literal notranslate"><span class="pre">Workflow</span></code> must be initialized with a <code class="docutils literal notranslate"><span class="pre">dask.distributed.Client</span></code> object:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nvtabular</span> <span class="k">as</span> <span class="nn">nvt</span>
<span class="kn">from</span> <span class="nn">dask.distributed</span> <span class="kn">import</span> <span class="n">Client</span>

<span class="c1"># Deploy a new cluster</span>
<span class="c1"># (or specify the port of an existing scheduler)</span>
<span class="n">cluster</span> <span class="o">=</span> <span class="s2">&quot;tcp://MachineA:8786&quot;</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="n">cluster</span><span class="p">)</span>
<span class="n">workflow</span> <span class="o">=</span> <span class="n">nvt</span><span class="o">.</span><span class="n">Workflow</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">client</span><span class="o">=</span><span class="n">client</span><span class="p">)</span>
<span class="o">...</span>
</pre></div>
</div>
<p>There are currenly many ways to deploy a “cluster” for Dask.  <a class="reference external" href="https://blog.dask.org/2020/07/23/current-state-of-distributed-dask-clusters">This article</a> gives a nice summary of all practical options.  For a single machine with multiple GPUs, the <code class="docutils literal notranslate"><span class="pre">dask_cuda.LocalCUDACluster</span></code> API is typically the most convenient option.</p>
<p>Since NVTabular already uses <a class="reference external" href="https://docs.rapids.ai/api/cudf/stable/dask-cudf.html">Dask-CuDF</a> for internal data processing, there are no other requirements for multi-GPU scaling.  With that said, the parallel performance can depend strongly on (1) the size of <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> partitions, (2) the shuffling procedure used for data output, and (3) the specific arguments used for both global-statistics and transformation operations. See the <a class="reference external" href="https://github.com/NVIDIA/NVTabular/blob/main/examples/multi-gpu_dask.ipynb">Multi-GPU</a> section of this documentation for a simple step-by-step example.</p>
<p>Users are also encouraged to experiment with the <a class="reference external" href="https://github.com/NVIDIA/NVTabular/blob/main/examples/dask-nvtabular-criteo-benchmark.py">multi-GPU Criteo/DLRM benchmark example</a>. For detailed notes on the parameter space for the benchmark, see the <a class="reference external" href="https://github.com/NVIDIA/NVTabular/blob/main/examples/MultiGPUBench.md">Multi-GPU Criteo Benchmark</a> section of this documentation.</p>
</div>
<div class="section" id="multi-node-support">
<h2>Multi-Node Support<a class="headerlink" href="#multi-node-support" title="Permalink to this headline"></a></h2>
<p>NVTabular supports multi node scaling with <a class="reference external" href="https://github.com/rapidsai/dask-cuda">Dask-CUDA</a> and <a class="reference external" href="https://distributed.dask.org/en/latest/">dask.distributed</a>.  To enable distributed parallelism, we need to start a cluster and then connect to it to run the application.</p>
<ol class="simple">
<li><p>Start the scheduler <code class="docutils literal notranslate"><span class="pre">dask-scheduler</span></code></p></li>
<li><p>Start the workers <code class="docutils literal notranslate"><span class="pre">dask-cuda-worker</span> <span class="pre">schedulerIP:schedulerPort</span></code></p></li>
<li><p>Run the NVTabular application where the NVTabular <code class="docutils literal notranslate"><span class="pre">Workflow</span></code> has been initialized as described in the Multi-GPU Support section.</p></li>
</ol>
<p>For a detailed description of all the existing methods to start a cluster, please read <a class="reference external" href="https://blog.dask.org/2020/07/23/current-state-of-distributed-dask-clusters">this article</a>.</p>
</div>
<div class="section" id="multihot-encoding-and-pre-existing-embeddings">
<h2>MultiHot Encoding and Pre-existing Embeddings<a class="headerlink" href="#multihot-encoding-and-pre-existing-embeddings" title="Permalink to this headline"></a></h2>
<p>NVTabular now supports processing datasets with multihot categorical columns, and also supports passing along
vector continuous features like pretrained embeddings. This support includes basic preprocessing
and feature engineering ability, as well as full support in the dataloaders for training models
using these features with both TensorFlow and PyTorch.</p>
<p>Multihots let you represent a set of categories as a single feature. For example, in a movie recommendation system each movie might
have a list of genres associated with the movie like comedy, drama, horror or science fiction. Since movies can
belong to more than one genre we can’t use single-hot encoding like we are doing for scalar
columns. Instead we train models with multihot embeddings for these features, with the deep
learning model looking up an embedding for each categorical in the list and then summing all the
categories for each row.</p>
<p>Both multihot categoricals and vector continuous features are represented using list columns in
our datasets. cuDF has recently added support for list columns, and we’re leveraging that support in NVTabular
0.3 to power this feature.</p>
<p>We’ve added support to our Categorify and HashBucket operators to map list columns down to small
contiguous integers suitable for use in an embedding lookup table. That is if you pass a dataset
containing two rows like <code class="docutils literal notranslate"><span class="pre">[['comedy',</span> <span class="pre">'horror'],</span> <span class="pre">['comedy',</span> <span class="pre">'sciencefiction']]</span></code> we can transform
the strings for each into categorical ids like <code class="docutils literal notranslate"><span class="pre">[[0,</span> <span class="pre">1],</span> <span class="pre">[0,</span> <span class="pre">2]]</span></code> that can be used in our embeddings
layers using these two operators.</p>
<p>Our PyTorch and TensorFlow dataloaders have been extended to handle both categorical and
continuous list columns.  In TensorFlow, the KerasSequenceLoader class will transform each list
column into two tensors representing the values and offsets into those values for each batch.
These tensors can be converted into RaggedTensors for multihot columns, and for vector continuous
columns the offsets tensor can be safely ignored. We’ve provided a
<code class="docutils literal notranslate"><span class="pre">nvtabular.framework_utils.tensorflow.layers.DenseFeatures</span></code> Keras layer that will
automatically handle these conversions for both continuous and categorical columns. For PyTorch,
we’ve added support for multihot columns to our
<code class="docutils literal notranslate"><span class="pre">nvtabular.framework_utils.torch.models.Model</span></code> class, which internally is using the PyTorch
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html">EmbeddingBag</a> layer to
handle the multihot columns.</p>
</div>
<div class="section" id="cpu-support">
<h2>CPU Support<a class="headerlink" href="#cpu-support" title="Permalink to this headline"></a></h2>
<p>Operators will also be developed using pandas to provide support for users who don’t have access to GPU resources and who wish to use the higher level API that NVTabular provides.  We will try to provide support and feature parity for CPU but GPU acceleration is the focus of this library.  Check the API documentation for coverage.</p>
</div>
<div class="section" id="getting-your-data-ready-for-nvtabular">
<h2>Getting your data ready for NVTabular<a class="headerlink" href="#getting-your-data-ready-for-nvtabular" title="Permalink to this headline"></a></h2>
<p>NVTabular is designed with a specific type of dataset in mind. Ideally, the dataset will have the following characteristics:</p>
<ol class="simple">
<li><p>Comprises 1+ parquet files</p></li>
<li><p>All parquet files must have the same schema (including column types and nullable (“not null”) option)</p></li>
<li><p>Each parquet file consists of row-groups around 128MB in size</p></li>
<li><p>Each parquet file is large enough to map onto an entire dask_cudf.DataFrame partition. This typically means &gt;=1GB.</p></li>
<li><p>All parquet files should be located within a “root” directory, and that directory should contain a global “_metadata” file.
<em>Note</em>: This “_metadata” file allows the dask_cudf client to produce a DataFrame collection much faster, because all metadata can be accessed from a single file. When this file is not present, the client needs to aggregate footer metadata from all files in the dataset.</p></li>
</ol>
<p>CSV files are support but not recommended, because they are not efficiently stored and loaded into memory compared to parquet files (columnar format).</p>
<div class="section" id="troubleshooting">
<h3>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this headline"></a></h3>
<div class="section" id="checking-the-schema-of-parquet-files">
<h4>Checking the schema of parquet files<a class="headerlink" href="#checking-the-schema-of-parquet-files" title="Permalink to this headline"></a></h4>
<p>NVTabular expects that all input parquet files have the same schema (including column types and nullable (not null) option).</p>
<p>If you get the error <code class="docutils literal notranslate"><span class="pre">RuntimeError:</span> <span class="pre">Schemas</span> <span class="pre">are</span> <span class="pre">inconsistent,</span> <span class="pre">try</span> <span class="pre">using</span> <span class="pre">to_parquet(...,</span> <span class="pre">schema=&quot;infer&quot;),</span> <span class="pre">or</span> <span class="pre">pass</span> <span class="pre">an</span> <span class="pre">explicit</span> <span class="pre">pyarrow</span> <span class="pre">schema.</span> <span class="pre">Such</span> <span class="pre">as</span> <span class="pre">to_parquet(...,</span> <span class="pre">schema={&quot;column1&quot;:</span> <span class="pre">pa.string()})</span></code> when you load the dataset as below, some parquet file might have a different schema:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ds</span> <span class="o">=</span> <span class="n">nvt</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">PATH</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="s2">&quot;parquet&quot;</span><span class="p">,</span> <span class="n">part_size</span><span class="o">=</span><span class="s2">&quot;1000MB&quot;</span><span class="p">)</span>
<span class="n">ds</span><span class="o">.</span><span class="n">to_ddf</span><span class="p">()</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<p>The easiest way to fix this is to load your dataset with dask_cudf and save it again to parquet format ( <code class="docutils literal notranslate"><span class="pre">dask_cudf.read_parquet(&quot;INPUT_FOLDER&quot;).to_parquet(&quot;OUTPUT_FOLDER&quot;)</span></code>), so that files are standardized and the <code class="docutils literal notranslate"><span class="pre">_metadata</span></code> file is generated.</p>
<p>If you want to identify which parquet files and which columns have a different schema, you may run one of these scripts, using either <a class="reference external" href="https://github.com/dask/dask/issues/6504#issuecomment-675465645">PyArrow</a> or <a class="reference external" href="https://github.com/rapidsai/cudf/pull/6796#issue-522934284">cudf=0.17</a>, which checks the consistency and generates only the <code class="docutils literal notranslate"><span class="pre">_metadata</span></code> file, rather than converting all parquet files. If the schema is not consistent across all files, the script will raise an exception describing inconsistent files with schema for troubleshooting. More info in this issue <a class="reference external" href="https://github.com/NVIDIA/NVTabular/issues/429">here</a>.</p>
</div>
<div class="section" id="setting-the-row-group-size-of-parquet-files">
<h4>Setting the row group size of parquet files<a class="headerlink" href="#setting-the-row-group-size-of-parquet-files" title="Permalink to this headline"></a></h4>
<p>You can set the row group size (number of rows) of your parquet files by using most Data Frame frameworks. In the following examples with Pandas and cuDF, the <code class="docutils literal notranslate"><span class="pre">row_group_size</span></code> is the number of rows that will be stored in each row group (internal structure within the parquet file):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#Pandas</span>
<span class="n">pandas_df</span><span class="o">.</span><span class="n">to_parquet</span><span class="p">(</span><span class="s2">&quot;/file/path&quot;</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="s2">&quot;pyarrow&quot;</span><span class="p">,</span> <span class="n">row_group_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="c1">#cuDF</span>
<span class="n">cudf_df</span><span class="o">.</span><span class="n">to_parquet</span><span class="p">(</span><span class="s2">&quot;/file/path&quot;</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="s2">&quot;pyarrow&quot;</span><span class="p">,</span> <span class="n">row_group_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
</pre></div>
</div>
<p>The row group <strong>memory</strong> size of the parquet files should be lower than the <strong>part_size</strong> you set for the NVTabular dataset (like in <code class="docutils literal notranslate"><span class="pre">nvt.Dataset(TRAIN_DIR,</span> <span class="pre">engine=&quot;parquet&quot;,</span> <span class="pre">part_size=&quot;1000MB&quot;</span></code>).
To know how much memory a row group will hold, you can slice your dataframe to a specific number of rows and use the following function to get the memory usage in bytes. Then, you can set the row_group_size (number of rows) accordingly when you save the parquet file. A row group memory size of around 128MB is recommended in general.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_memory_usage</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;this function is a workaround of a problem with getting memory usage of lists</span>
<span class="sd">    in cudf0.16.  This can be deleted and just use `df.memory_usage(deep= True, index=True).sum()`</span>
<span class="sd">    once we are using cudf 0.17 (fixed in https://github.com/rapidsai/cudf/pull/6549)&quot;&quot;&quot;</span>
    <span class="n">size</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">cudf</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dtypes</span><span class="o">.</span><span class="n">is_list_dtype</span><span class="p">(</span><span class="n">col</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">col</span><span class="o">.</span><span class="n">base_children</span><span class="p">:</span>
                <span class="n">size</span> <span class="o">+=</span> <span class="n">child</span><span class="o">.</span><span class="n">__sizeof__</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">size</span> <span class="o">+=</span> <span class="n">col</span><span class="o">.</span><span class="n">_memory_usage</span><span class="p">(</span><span class="n">deep</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">size</span> <span class="o">+=</span> <span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">memory_usage</span><span class="p">(</span><span class="n">deep</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">size</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="Introduction.html" class="btn btn-neutral float-left" title="NVTabular | API documentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="examples/index.html" class="btn btn-neutral float-right" title="Examples" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: v0.3.0
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../v0.1.0/HowItWorks.html">v0.1.0</a></dd>
      <dd><a href="../v0.1.1/HowItWorks.html">v0.1.1</a></dd>
      <dd><a href="../v0.10.0/index.html">v0.10.0</a></dd>
      <dd><a href="../v0.11.0/index.html">v0.11.0</a></dd>
      <dd><a href="../v0.2.0/HowItWorks.html">v0.2.0</a></dd>
      <dd><a href="HowItWorks.html">v0.3.0</a></dd>
      <dd><a href="../v0.4.0/index.html">v0.4.0</a></dd>
      <dd><a href="../v0.5.0/index.html">v0.5.0</a></dd>
      <dd><a href="../v0.5.1/index.html">v0.5.1</a></dd>
      <dd><a href="../v0.5.2/index.html">v0.5.2</a></dd>
      <dd><a href="../v0.5.3/index.html">v0.5.3</a></dd>
      <dd><a href="../v0.6.0/index.html">v0.6.0</a></dd>
      <dd><a href="../v0.6.1/index.html">v0.6.1</a></dd>
      <dd><a href="../v0.7.0/index.html">v0.7.0</a></dd>
      <dd><a href="../v0.7.1/index.html">v0.7.1</a></dd>
      <dd><a href="../v0.8.0/index.html">v0.8.0</a></dd>
      <dd><a href="../v0.9.0/index.html">v0.9.0</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="../main/index.html">main</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>