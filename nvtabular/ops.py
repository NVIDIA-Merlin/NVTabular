#
# Copyright (c) 2020, NVIDIA CORPORATION.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
import cudf
import cupy
import numpy as np
from cudf._lib.nvtx import annotate
from dask.core import flatten
from dask.delayed import Delayed

from nvtabular import categorify as nvt_cat
from nvtabular.io import _detect_format
from nvtabular.worker import fetch_table_data, get_worker_cache

CONT = "continuous"
CAT = "categorical"
ALL = "all"


class Operator:
    """
    Base class for all operator classes.
    """

    def __init__(self, columns=None):
        self.columns = columns

    @property
    def _id(self):
        return str(self.__class__.__name__)

    def describe(self):
        raise NotImplementedError("All operators must have a desription.")

    def get_columns(self, cols_ctx, cols_grp, target_cols):
        # providing any operator with direct list of columns overwrites cols dict
        # burden on user to ensure columns exist in dataset (as discussed)
        if self.columns:
            return self.columns
        tar_cols = []
        for tar in target_cols:
            if tar in cols_ctx[cols_grp].keys():
                tar_cols = tar_cols + cols_ctx[cols_grp][tar]
        return tar_cols


class TransformOperator(Operator):
    """
    Base class for transformer operator classes.
    """

    default_in = None
    default_out = None

    def __init__(self, columns=None, preprocessing=True, replace=True):
        super().__init__(columns=columns)
        self.preprocessing = preprocessing
        self.replace = replace

    def get_default_in(self):
        if self.default_in is None:
            raise NotImplementedError(
                "default_in columns have not been specified for this operator"
            )
        return self.default_in

    def get_default_out(self):
        if self.default_out is None:
            raise NotImplementedError(
                "default_out columns have not been specified for this operator"
            )
        return self.default_out

    def update_columns_ctx(self, columns_ctx, input_cols, new_cols, origin_targets, pro=False):
        """
        columns_ctx: columns context, belonging to the container workflow object
        input_cols: input columns; columns actioned on origin columns context key
        new_cols: new columns; new columns generated by operator to be added to columns context
        ----
        This function generalizes the action of updating the columns context dictionary
        of the container workflow object, after an operator has created new columns via a
        new transformation of a subset or entire dataset.
        """

        new_key = self._id

        columns_ctx[input_cols][new_key] = []
        if self.replace and self.preprocessing:
            # not making new columns instead using old ones
            # must reference original target with new operator for chaining
            columns_ctx[input_cols][new_key] = origin_targets
            return
        columns_ctx[input_cols][new_key] = list(new_cols)
        if not self.preprocessing and self._id not in columns_ctx["final"]["ctx"][input_cols]:
            columns_ctx["final"]["ctx"][input_cols].append(self._id)

    def apply_op(
        self,
        gdf: cudf.DataFrame,
        columns_ctx: dict,
        input_cols,
        target_cols=["base"],
        stats_context=None,
    ):
        target_columns = self.get_columns(columns_ctx, input_cols, target_cols)
        new_gdf = self.op_logic(gdf, target_columns, stats_context=stats_context)
        self.update_columns_ctx(columns_ctx, input_cols, new_gdf.columns, target_columns)
        return self.assemble_new_df(gdf, new_gdf, target_columns)

    def assemble_new_df(self, origin_gdf, new_gdf, target_columns):
        if self.replace and self.preprocessing and target_columns:
            if new_gdf.shape[0] < origin_gdf.shape[0]:
                return new_gdf
            else:
                origin_gdf[target_columns] = new_gdf
                return origin_gdf
        return cudf.concat([origin_gdf, new_gdf], axis=1)

    def op_logic(self, gdf, target_columns, stats_context=None):
        raise NotImplementedError(
            """Must implement transform in the op_logic method,
                                     The return value must be a dataframe with all required
                                     transforms."""
        )


class DFOperator(TransformOperator):
    """
    Base class for data frame operator classes.
    """

    @property
    def req_stats(self):
        raise NotImplementedError(
            "Should consist of a list of identifiers, that should map to available statistics"
        )


class StatOperator(Operator):
    """
    Base class for statistical operator classes.
    """

    def __init__(self, columns=None):
        super(StatOperator, self).__init__(columns)

    def stat_logic(self, ddf, columns_ctx, input_cols, target_cols):
        raise NotImplementedError(
            """The dask operations needed to return a dictionary of uncomputed statistics."""
        )

    def finalize(self, dask_stats):
        raise NotImplementedError(
            """Follow-up operations to convert dask statistics in to member variables"""
        )

    def registered_stats(self):
        raise NotImplementedError(
            """Should return a list of statistics this operator will collect.
                The list is comprised of simple string values."""
        )

    def stats_collected(self):
        raise NotImplementedError(
            """Should return a list of tuples of name and statistics operator."""
        )

    def clear(self):
        raise NotImplementedError("""zero and reinitialize all relevant statistical properties""")


class MinMax(StatOperator):
    """
    MinMax operation calculates min and max statistics of features.

    Parameters
    -----------
    columns :
    batch_mins : list of float, default None
    batch_maxs : list of float, default None
    mins : list of float, default None
    maxs : list of float, default None
    """

    def __init__(self, columns=None, batch_mins=None, batch_maxs=None, mins=None, maxs=None):
        super().__init__(columns=columns)
        self.batch_mins = batch_mins if batch_mins is not None else {}
        self.batch_maxs = batch_maxs if batch_maxs is not None else {}
        self.mins = mins if mins is not None else {}
        self.maxs = maxs if maxs is not None else {}

    @annotate("MinMax_op", color="green", domain="nvt_python")
    def stat_logic(self, ddf, columns_ctx, input_cols, target_cols):
        cols = self.get_columns(columns_ctx, input_cols, target_cols)
        dask_stats = {}
        dask_stats["mins"] = ddf[cols].min()
        dask_stats["maxs"] = ddf[cols].max()
        return dask_stats

    @annotate("MinMax_finalize", color="green", domain="nvt_python")
    def finalize(self, stats):
        for col in stats["mins"].index.values_host:
            self.mins[col] = stats["mins"][col]
            self.maxs[col] = stats["maxs"][col]

    def registered_stats(self):
        return ["mins", "maxs", "batch_mins", "batch_maxs"]

    def stats_collected(self):
        result = [
            ("mins", self.mins),
            ("maxs", self.maxs),
            ("batch_mins", self.batch_mins),
            ("batch_maxs", self.batch_maxs),
        ]
        return result

    def clear(self):
        self.batch_mins = {}
        self.batch_maxs = {}
        self.mins = {}
        self.maxs = {}
        return


class Moments(StatOperator):
    """
    Moments operation calculates some of the statistics of features including
    mean, variance, standarded deviation, and count.

    Parameters
    -----------
    columns :
    counts : list of float, default None
    means : list of float, default None
    varis : list of float, default None
    stds : list of float, default None
    """

    def __init__(self, columns=None, counts=None, means=None, varis=None, stds=None):
        super().__init__(columns=columns)
        self.counts = counts if counts is not None else {}
        self.means = means if means is not None else {}
        self.varis = varis if varis is not None else {}
        self.stds = stds if stds is not None else {}

    @annotate("Moments_op", color="green", domain="nvt_python")
    def stat_logic(self, ddf, columns_ctx, input_cols, target_cols):
        cols = self.get_columns(columns_ctx, input_cols, target_cols)
        dask_stats = {}
        dask_stats["count"] = ddf[cols].count()
        dask_stats["mean"] = ddf[cols].mean()
        dask_stats["std"] = ddf[cols].std()
        return dask_stats

    @annotate("Moments_finalize", color="green", domain="nvt_python")
    def finalize(self, dask_stats):
        for col in dask_stats["count"].index.values_host:
            self.counts[col] = float(dask_stats["count"][col])
            self.means[col] = float(dask_stats["mean"][col])
            self.stds[col] = float(dask_stats["std"][col])
            self.varis[col] = float(self.stds[col] * self.stds[col])

    def registered_stats(self):
        return ["means", "stds", "vars", "counts"]

    def stats_collected(self):
        result = [
            ("means", self.means),
            ("stds", self.stds),
            ("vars", self.varis),
            ("counts", self.counts),
        ]
        return result

    def clear(self):
        self.counts = {}
        self.means = {}
        self.varis = {}
        self.stds = {}
        return


class Median(StatOperator):
    """
    This operation calculates median of features.

    Parameters
    -----------
    columns :
    fill : float, default None
    batch_medians : list, default None
    medians : list, default None
    """

    def __init__(self, columns=None, fill=None, batch_medians=None, medians=None):
        super().__init__(columns=columns)
        self.fill = fill
        self.batch_medians = batch_medians if batch_medians is not None else {}
        self.medians = medians if medians is not None else {}

    @annotate("Median_op", color="green", domain="nvt_python")
    def stat_logic(self, ddf, columns_ctx, input_cols, target_cols):
        cols = self.get_columns(columns_ctx, input_cols, target_cols)
        # TODO: Use `method="tidigest"` when crick supports device
        dask_stats = ddf[cols].quantile(q=0.5, method="dask")
        return dask_stats

    @annotate("Median_finalize", color="green", domain="nvt_python")
    def finalize(self, dask_stats):
        for col in dask_stats.index.values_host:
            self.medians[col] = float(dask_stats[col])

    def registered_stats(self):
        return ["medians"]

    def stats_collected(self):
        result = [("medians", self.medians)]
        return result

    def clear(self):
        self.batch_medians = {}
        self.medians = {}
        return


class Clip(TransformOperator):
    """
    This operation clips values continous values so that they are with a min/max bound.
    For instance by setting the min value to 0, you can replace all negative values with 0.
    This is helpful in cases where you want to log normalize values.
    """

    default_in = CONT
    default_out = CONT

    def __init__(
        self, min_value=None, max_value=None, columns=None, preprocessing=True, replace=True
    ):
        if min_value is None and max_value is None:
            raise ValueError("Must specify a min or max value to clip to")
        super().__init__(columns=columns, preprocessing=preprocessing, replace=replace)
        self.min_value = min_value
        self.max_value = max_value

    @annotate("Clip_op", color="darkgreen", domain="nvt_python")
    def op_logic(self, gdf: cudf.DataFrame, target_columns: list, stats_context=None):
        cont_names = target_columns
        if not cont_names:
            return gdf

        z_gdf = gdf[cont_names]
        z_gdf.columns = [f"{col}_{self._id}" for col in z_gdf.columns]
        if self.min_value is not None:
            z_gdf[z_gdf < self.min_value] = self.min_value
        if self.max_value is not None:
            z_gdf[z_gdf > self.max_value] = self.max_value

        return z_gdf


class Dropna(TransformOperator):
    """
    This operation detects missing values, and returns
    a cudf DataFrame with Null entries dropped from it.

    Although you can directly call methods of this class to
    transform your categorical and/or continuous features, it's typically used within a
    Workflow class.
    """

    default_in = ALL
    default_out = ALL

    @annotate("Dropna_op", color="darkgreen", domain="nvt_python")
    def apply_op(
        self,
        gdf: cudf.DataFrame,
        columns_ctx: dict,
        input_cols,
        target_cols=["base"],
        stats_context=None,
    ):
        target_columns = self.get_columns(columns_ctx, input_cols, target_cols)
        new_gdf = gdf.dropna(subset=target_columns or None)
        new_gdf.reset_index(drop=True, inplace=True)
        self.update_columns_ctx(columns_ctx, input_cols, new_gdf.columns, target_columns)
        return new_gdf


class LogOp(TransformOperator):

    """
    Standardizing the features around 0 with a standard deviation
    of 1 is a common technique to compare measurements that have
    different units. This operation can be added to the workflow
    to standardize the features.

    Although you can directly call methods of this class to
    transform your continuous features, it's typically used within a
    Workflow class.
    """

    default_in = CONT
    default_out = CONT

    @annotate("LogOp_op", color="darkgreen", domain="nvt_python")
    def op_logic(self, gdf: cudf.DataFrame, target_columns: list, stats_context=None):
        cont_names = target_columns
        if not cont_names:
            return gdf
        new_gdf = np.log(gdf[cont_names].astype(np.float32) + 1)
        new_cols = [f"{col}_{self._id}" for col in new_gdf.columns]
        new_gdf.columns = new_cols
        return new_gdf


class HashBucket(TransformOperator):
    default_in = CAT
    default_out = CAT

    def __init__(self, num_buckets, columns=None, **kwargs):
        if isinstance(num_buckets, dict):
            columns = [i for i in num_buckets.keys()]
            self.num_buckets = num_buckets
        elif isinstance(num_buckets, (tuple, list)):
            assert columns is not None
            assert len(columns) == len(num_buckets)
            self.num_buckets = {col: nb for col, nb in zip(columns, num_buckets)}
        elif isinstance(num_buckets, int):
            self.num_buckets = num_buckets
        else:
            raise TypeError(
                "`num_buckets` must be dict, iterable, or int, got type {}".format(
                    type(num_buckets)
                )
            )
        super(HashBucket, self).__init__(columns=columns, **kwargs)

    @annotate("HashBucket_op", color="darkgreen", domain="nvt_python")
    def op_logic(self, gdf: cudf.DataFrame, target_columns: list, stats_context=None):
        cat_names = target_columns
        if isinstance(self.num_buckets, int):
            num_buckets = {name: self.num_buckets for name in cat_names}
        else:
            num_buckets = self.num_buckets

        new_gdf = cudf.DataFrame()
        for col, nb in num_buckets.items():
            new_col = f"{col}_{self._id}"
            new_gdf[new_col] = gdf[col].hash_values() % nb
        return new_gdf


class Normalize(DFOperator):
    """
    Standardizing the features around 0 with a standard deviation
    of 1 is a common technique to compare measurements that have
    different units. This operation can be added to the workflow
    to standardize the features.

    It performs Normalization using the mean std method.

    Although you can directly call methods of this class to
    transform your continuous features, it's typically used within a
    Workflow class.
    """

    default_in = CONT
    default_out = CONT

    @property
    def req_stats(self):
        return [Moments(columns=self.columns)]

    @annotate("Normalize_op", color="darkgreen", domain="nvt_python")
    def op_logic(self, gdf: cudf.DataFrame, target_columns: list, stats_context=None):
        cont_names = target_columns
        if not cont_names or not stats_context["stds"]:
            return
        gdf = self.apply_mean_std(gdf, stats_context, cont_names)
        return gdf

    def apply_mean_std(self, gdf, stats_context, cont_names):
        new_gdf = cudf.DataFrame()
        for name in cont_names:
            if stats_context["stds"][name] > 0:
                new_col = f"{name}_{self._id}"
                new_gdf[new_col] = (gdf[name] - stats_context["means"][name]) / (
                    stats_context["stds"][name]
                )
                new_gdf[new_col] = new_gdf[new_col].astype("float32")
        return new_gdf


class NormalizeMinMax(DFOperator):
    """
    Standardizing the features around 0 with a standard deviation
    of 1 is a common technique to compare measurements that have
    different units. This operation can be added to the workflow
    to standardize the features.

    It performs Normalization using the min max method.

    Although you can directly call methods of this class to
    transform your continuous features, it's typically used within a
    Workflow class.
    """

    default_in = CONT
    default_out = CONT

    @property
    def req_stats(self):
        return [MinMax()]

    @annotate("NormalizeMinMax_op", color="darkgreen", domain="nvt_python")
    def op_logic(self, gdf: cudf.DataFrame, target_columns: list, stats_context=None):
        cont_names = target_columns
        if not cont_names or not stats_context["mins"]:
            return
        gdf = self.apply_min_max(gdf, stats_context, cont_names)
        return gdf

    def apply_min_max(self, gdf, stats_context, cont_names):
        new_gdf = cudf.DataFrame()
        for name in cont_names:
            dif = stats_context["maxs"][name] - stats_context["mins"][name]
            new_col = f"{name}_{self._id}"
            if dif > 0:
                new_gdf[new_col] = (gdf[name] - stats_context["mins"][name]) / dif
            elif dif == 0:
                new_gdf[new_col] = gdf[name] / (2 * gdf[name])
            new_gdf[new_col] = new_gdf[new_col].astype("float32")
        return new_gdf


class FillMissing(DFOperator):
    """
    This operation replaces missing values with a constant pre-defined value

    Although you can directly call methods of this class to
    transform your continuous features, it's typically used within a
    Workflow class.

    Parameters
    -----------
    fill_val : float, default 0
        The constant value to replace missing values with
    columns :
    preprocessing : bool, default True
    replace : bool, default True
    """

    default_in = CONT
    default_out = CONT

    def __init__(self, fill_val=0, columns=None, preprocessing=True, replace=True):
        super().__init__(columns=columns, preprocessing=preprocessing, replace=replace)
        self.fill_val = fill_val

    @property
    def req_stats(self):
        return []

    @annotate("FillMissing_op", color="darkgreen", domain="nvt_python")
    def op_logic(self, gdf: cudf.DataFrame, target_columns: list, stats_context=None):
        cont_names = target_columns
        if not cont_names:
            return gdf
        z_gdf = gdf[cont_names].fillna(self.fill_val)
        z_gdf.columns = [f"{col}_{self._id}" for col in z_gdf.columns]
        return z_gdf


class FillMedian(DFOperator):
    """
    This operation replaces missing values with the median value for the column.
    Although you can directly call methods of this class to
    transform your continuous features, it's typically used within a
    Workflow class.

    Parameters
    -----------
    columns :
    preprocessing : bool, default True
    replace : bool, default True
    """

    default_in = CONT
    default_out = CONT

    @property
    def req_stats(self):
        return [Median()]

    @annotate("FillMedian_op", color="darkgreen", domain="nvt_python")
    def op_logic(self, gdf: cudf.DataFrame, target_columns: list, stats_context=None):
        if not target_columns:
            return gdf

        new_gdf = cudf.DataFrame()
        for col in target_columns:
            stat_val = stats_context["medians"][col]
            new_gdf[col] = gdf[col].fillna(stat_val)
        new_gdf.columns = [f"{col}_{self._id}" for col in new_gdf.columns]
        return new_gdf


class GroupbyStatistics(StatOperator):
    """
    Uses groupby aggregation to determine the unique groups of a categorical
    feature and calculates the desired statistics of requested continuous
    features (along with the count of rows in each group).  The statistics
    for each category will be written to a distinct parquet file, and a
    dictionary of paths will be returned as the final "statistics".

    Parameters
    -----------
    cont_names : list of str
        The continuous column names to calculate statistics for
        (for each unique group in each column in `columns`)
    stats : list of str, default []
        List of statistics to calculate for each unique group. Note
        that "count" corresponds to the group itself, while all
        other statistics correspond to a specific continuous column.
        Supported statistics include ["count", "sum", "mean", "std", "var", "min", "max"].
    columns : list of str or list(str), default None
        Categorical columns (or "column groups") to collect statistics for.
        If None, the operation will target all known categorical columns.
    concat_groups : bool, default False
        Applies only if there are list elements in the ``columns`` input. If True,
        the values within these column groups will be concatenated, and the
        new (temporary) columns will be used to perform the groupby.  The purpose of
        this option is to enable multiple columns to be label-encoded jointly.
        (see Categorify). Note that this option is only allowed for the "count"
        statistics (with cont_names == None).
    tree_width : dict or int, optional
        Tree width of the hash-based groupby reduction for each categorical
        column. High-cardinality columns may require a large `tree_width`,
        while low-cardinality columns can likely use `tree_width=1`.
        If passing a dict, each key and value should correspond to the column
        name and width, respectively. The default value is 8 for all columns.
    out_path : str, optional
        Root directory where groupby statistics will be written out in
        parquet format.
    freq_threshold : int, default 0
        Categories with a `count` statistic less than this number will
        be omitted from the `GroupbyStatistics` output.
    on_host : bool, default True
        Whether to convert cudf data to pandas between tasks in the hash-based
        groupby reduction. The extra host <-> device data movement can reduce
        performance.  However, using `on_host=True` typically improves stability
        (by avoiding device-level memory pressure).
    name_sep : str, default "_"
        String separator to use between concatenated column names
        for multi-column groups.
    """

    def __init__(
        self,
        cont_names=None,
        stats=None,
        columns=None,
        tree_width=None,
        out_path=None,
        on_host=True,
        freq_threshold=None,
        stat_name=None,
        concat_groups=False,
        name_sep="_",
    ):
        # Set column_groups if the user has passed in a list of columns
        self.column_groups = None
        if isinstance(columns, str):
            columns = [columns]
        if isinstance(columns, list):
            self.column_groups = columns
            columns = list(set(flatten(columns, container=list)))

        super(GroupbyStatistics, self).__init__(columns)
        self.cont_names = cont_names or []
        self.stats = stats or []
        self.categories = {}
        self.tree_width = tree_width or 8
        self.on_host = on_host
        self.freq_threshold = freq_threshold or 0
        self.out_path = out_path or "./"
        self.stat_name = stat_name or "categories"
        self.op_name = "GroupbyStatistics-" + self.stat_name
        self.concat_groups = concat_groups
        self.name_sep = name_sep

    @property
    def _id(self):
        return str(self.op_name)

    def stat_logic(self, ddf, columns_ctx, input_cols, target_cols):
        col_groups = self.column_groups or self.get_columns(columns_ctx, input_cols, target_cols)
        supported_ops = ["count", "sum", "mean", "std", "var", "min", "max"]
        for op in self.stats:
            if op not in supported_ops:
                raise ValueError(op + " operation is not supported.")

        agg_cols = self.cont_names
        agg_list = self.stats
        dsk, key = nvt_cat._category_stats(
            ddf,
            col_groups,
            agg_cols,
            agg_list,
            self.out_path,
            self.freq_threshold,
            self.tree_width,
            self.on_host,
            stat_name=self.stat_name,
            concat_groups=self.concat_groups,
            name_sep=self.name_sep,
        )
        return Delayed(key, dsk)

    def finalize(self, dask_stats):
        for col in dask_stats:
            self.categories[col] = dask_stats[col]

    def registered_stats(self):
        return [self.stat_name]

    def stats_collected(self):
        result = [(self.stat_name, self.categories)]
        return result

    def clear(self):
        self.categories = {}
        return


class JoinGroupby(DFOperator):
    """
    One of the ways to create new features is to calculate
    the basic statistics of the data that is grouped by categorical
    features. This operator groups the data by the given categorical
    feature(s) and calculates the desired statistics of requested continuous
    features (along with the count of rows in each group). The aggregated
    statistics are merged with the data (by joining on the desired
    categorical columns).

    Although you can directly call methods of this class to
    transform your categorical features, it's typically used within a
    Workflow class.

    Parameters
    -----------
    cont_names : list of str
        The continuous column names to calculate statistics for
        (for each unique group in each column in `columns`)
    stats : list of str, default []
        List of statistics to calculate for each unique group. Note
        that "count" corresponds to the group itself, while all
        other statistics correspond to a specific continuous column.
        Supported statistics include ["count", "sum", "mean", "std", "var"].
    columns : list of str or list(str), default None
        Categorical columns (or multi-column "groups") to target for this op.
        If None, the operation will target all known categorical columns.
    preprocessing : bool, default True
        Sets if this is a pre-processing operation or not
    replace : bool, default False
        This parameter is ignored
    tree_width : dict or int, optional
        Passed to `GroupbyStatistics` dependency.
    out_path : str, optional
        Passed to `GroupbyStatistics` dependency.
    on_host : bool, default True
        Passed to `GroupbyStatistics` dependency.
    name_sep : str, default "_"
        String separator to use between concatenated column names
        for multi-column groups.
    """

    default_in = CAT
    default_out = CAT

    def __init__(
        self,
        cont_names=None,
        stats=["count"],
        columns=None,
        preprocessing=True,
        replace=False,
        tree_width=None,
        cat_cache="host",
        out_path=None,
        on_host=True,
        name_sep="_",
    ):
        self.column_groups = None
        self.storage_name = {}
        self.name_sep = name_sep
        if isinstance(columns, str):
            columns = [columns]
        if isinstance(columns, list):
            self.column_groups = columns
            columns = list(set(flatten(columns, container=list)))
            for group in self.column_groups:
                if isinstance(group, list) and len(group) > 1:
                    name = nvt_cat._make_name(*group, sep=self.name_sep)
                    for col in group:
                        self.storage_name[col] = name

        super().__init__(columns=columns, preprocessing=preprocessing, replace=False)
        self.cont_names = cont_names
        self.stats = stats
        self.tree_width = tree_width
        self.out_path = out_path
        self.on_host = on_host
        self.cat_cache = cat_cache
        self.stat_name = "gb_categories"

    @property
    def req_stats(self):
        return [
            GroupbyStatistics(
                columns=self.column_groups or self.columns,
                concat_groups=False,
                cont_names=self.cont_names,
                stats=self.stats,
                tree_width=self.tree_width,
                out_path=self.out_path,
                on_host=self.on_host,
                stat_name=self.stat_name,
                name_sep=self.name_sep,
            )
        ]

    def op_logic(self, gdf: cudf.DataFrame, target_columns: list, stats_context=None):

        new_gdf = cudf.DataFrame()
        tmp = "__tmp__"  # Temporary column for sorting
        gdf[tmp] = cupy.arange(len(gdf), dtype="int32")
        if self.column_groups:
            cat_names, multi_col_group = nvt_cat._get_multicolumn_names(
                self.column_groups, gdf.columns, self.name_sep
            )
        else:
            multi_col_group = {}
            cat_names = [name for name in target_columns if name in gdf.columns]

        for name in cat_names:
            storage_name = self.storage_name.get(name, name)
            name = multi_col_group.get(name, name)
            path = stats_context[self.stat_name][storage_name]
            selection_l = name.copy() if isinstance(name, list) else [name]
            selection_r = name if isinstance(name, list) else [storage_name]

            stat_gdf = nvt_cat._read_groupby_stat_df(path, storage_name, self.cat_cache)
            tran_gdf = gdf[selection_l + [tmp]].merge(
                stat_gdf, left_on=selection_l, right_on=selection_r, how="left"
            )
            tran_gdf = tran_gdf.sort_values(tmp)
            tran_gdf.drop(columns=selection_l + [tmp], inplace=True)
            new_cols = [c for c in tran_gdf.columns if c not in new_gdf.columns]
            new_gdf[new_cols] = tran_gdf[new_cols].reset_index(drop=True)
        gdf.drop(columns=[tmp], inplace=True)
        return new_gdf


class JoinExternal(TransformOperator):
    """
    Join each dataset partition to an external table. For performance
    reasons, only "left" and "inner" join transformations are supported.

    Parameters
    -----------
    df_ext : DataFrame, pyarrow.Table, or file path
        The external table to join to each partition of the dataset.
    on : str or list(str)
        Column name(s) to merge on
    how : {"left", "inner"}; default "left"
        Type of join operation to perform.
    on_ext : str or list(str); Optional
        Column name(s) on external table to join on. By default,
        we assume ``on_ext`` is the same as ``on``.
    columns_ext : list(str); Optional
        Subset of columns to select from external table before join.
    drop_duplicates_ext : bool; Default False
        Drop duplicates from external table before join.
    kind_ext : {"arrow", "cudf", "pandas", "parquet", "csv"}
        Format of ``df_ext``.  If nothing is specified, the format
        will be inferred.
    cache : {"device", "host", "disk"}
        Where to cache ``df_ext`` between transformations. Only used
        if the data is originally stored on disk.
    """

    default_in = ALL
    default_out = ALL

    def __init__(
        self,
        df_ext,
        on,
        how="left",
        on_ext=None,
        columns_ext=None,
        drop_duplicates_ext=None,
        kind_ext=None,
        cache="host",
        preprocessing=True,
        **kwargs,
    ):
        super().__init__(preprocessing=preprocessing, replace=False)
        self.on = on
        self.df_ext = df_ext
        self.on_ext = on_ext or self.on
        self.how = how
        self.kind_ext = kind_ext or _detect_format(self.df_ext)
        self.columns_ext = columns_ext
        self.drop_duplicates_ext = drop_duplicates_ext
        self.cache = cache
        self.kwargs = kwargs
        if self.how not in ("left", "inner"):
            raise ValueError("Only left join is currently supported.")
        if self.kind_ext not in ("arrow", "cudf", "pandas", "parquet", "csv"):
            raise ValueError("kind_ext option not recognized.")

    @property
    def _ext(self):
        # Define _ext, depending on `kind_ext`
        if self.kind_ext == "cudf":
            _ext = self.df_ext
        elif self.kind_ext == "pandas":
            _ext = cudf.DataFrame.from_pandas(self.df_ext)
        elif self.kind_ext == "arrow":
            _ext = cudf.DataFrame.from_arrow(self.df_ext)
        else:
            if self.kind_ext == "parquet":
                reader = cudf.read_parquet
            elif self.kind_ext == "csv":
                reader = cudf.read_csv
            else:
                raise ValueError("Disk format not yet supported")

            with get_worker_cache(self.df_ext) as cached_table:
                _ext = fetch_table_data(
                    cached_table,
                    self.df_ext,
                    cache=self.cache,
                    columns=self.columns_ext,
                    reader=reader,
                    **self.kwargs,
                )

        # Take subset of columns if a list is specified
        if self.columns_ext:
            _ext = _ext[self.columns_ext]

        # Drop duplicates if requested
        if self.drop_duplicates_ext:
            _ext.drop_duplicates(ignore_index=True, inplace=True)

        return _ext

    def apply_op(
        self,
        gdf: cudf.DataFrame,
        columns_ctx: dict,
        input_cols,
        target_cols=["base"],
        stats_context=None,
    ):
        target_columns = self.get_columns(columns_ctx, input_cols, target_cols)
        tmp = "__tmp__"  # Temporary column for sorting
        gdf[tmp] = cupy.arange(len(gdf), dtype="int32")
        new_gdf = gdf.merge(self._ext, left_on=self.on, right_on=self.on_ext, how=self.how)
        new_gdf = new_gdf.sort_values(tmp)
        new_gdf.drop(columns=[tmp], inplace=True)
        gdf.drop(columns=[tmp], inplace=True)
        new_gdf.reset_index(drop=True, inplace=True)
        self.update_columns_ctx(columns_ctx, input_cols, new_gdf.columns, target_columns)
        return new_gdf


class Categorify(DFOperator):
    """
    Most of the data set will contain categorical features,
    and these variables are typically stored as text values.
    Machine Learning algorithms don't support these text values.
    Categorify operation can be added to the workflow to
    transform categorical features into unique integer values.

    Although you can directly call methods of this class to
    transform your categorical features, it's typically used within a
    Workflow class.

    Parameters
    -----------
    freq_threshold : int, default 0
        Categories with a count/frequency below this threshold will be
        ommited from the encoding and corresponding data will be mapped
        to the "null" category.
    columns : list of str or list(str), default None
        Categorical columns (or multi-column "groups") to target for this op.
        If None, the operation will target all known categorical columns.
        If columns contains 1+ list(str) elements, the columns within each
        list/group will be encoded according to the `encode_type` setting.
    encode_type : {"joint", "combo"}, default "joint"
        If "joint", the columns within any multi-column group will be
        jointly encoded. If "combo", the combination of values will be
        encoded as a new column. Note that replacement is not allowed for
        "combo", because the same column name can be included in
        multiple groups.
    preprocessing : bool, default True
        Sets if this is a pre-processing operation or not
    replace : bool, default True
        Replaces the transformed column with the original input.
        Note that this does not apply to multi-column groups with
        `encoded_type="combo"`.
    tree_width : dict or int, optional
        Passed to `GroupbyStatistics` dependency.
    out_path : str, optional
        Passed to `GroupbyStatistics` dependency.
    on_host : bool, default True
        Passed to `GroupbyStatistics` dependency.
    na_sentinel : default 0
        Label to use for null-category mapping
    cat_cache : {"device", "host", "disk"} or dict
        Location to cache the list of unique categories for
        each categorical column. If passing a dict, each key and value
        should correspond to the column name and location, respectively.
        Default is "host" for all columns.
    dtype :
        If specified, categorical labels will be cast to this dtype
        after encoding is performed.
    name_sep : str, default "_"
        String separator to use between concatenated column names
        for multi-column groups.
    """

    default_in = CAT
    default_out = CAT

    def __init__(
        self,
        freq_threshold=0,
        columns=None,
        preprocessing=True,
        replace=True,
        out_path=None,
        tree_width=None,
        na_sentinel=None,
        cat_cache="host",
        dtype=None,
        on_host=True,
        encode_type="joint",
        name_sep="_",
    ):

        # We need to handle three types of encoding here:
        #
        #   (1) Conventional encoding. There are no multi-column groups. So,
        #       each categorical column is separately transformed into a new
        #       "encoded" column (1-to-1).  The unique values are calculated
        #       separately for each column.
        #
        #   (2) Multi-column "Joint" encoding (there are multi-column groups
        #       in `columns` and `encode_type="joint"`).  Still a
        #       1-to-1 transofrmation of categorical columns.  However,
        #       we concatenate column groups to determine uniques (rather
        #       than getting uniques of each categorical column separately).
        #
        #   (3) Multi-column "Group" encoding (there are multi-column groups
        #       in `columns` and `encode_type="combo"`). No longer
        #       a 1-to-1 transformation of categorical columns. Each column
        #       group will be transformed to a single "encoded" column.  This
        #       means the unique "values" correspond to unique combinations.
        #       Since the same column may be included in multiple groups,
        #       replacement is not allowed for this transform.

        # Set column_groups if the user has passed in a list of columns.
        # The purpose is to capture multi-column groups. If the user doesn't
        # specify `columns`, there are no multi-column groups to worry about.
        self.column_groups = None
        self.name_sep = name_sep

        # For case (2), we need to keep track of the multi-column group name
        # that will be used for the joint encoding of each column in that group.
        # For case (3), we also use this "storage name" to signify the name of
        # the file with the required "combination" groupby statistics.
        self.storage_name = {}

        if isinstance(columns, str):
            columns = [columns]
        if isinstance(columns, list):
            # User passed in a list of column groups. We need to figure out
            # if this list contains any multi-column groups, and if there
            # are any (obvious) problems with these groups
            self.column_groups = columns
            columns = list(set(flatten(columns, container=list)))
            columns_all = list(flatten(columns, container=list))
            if sorted(columns_all) != sorted(columns) and encode_type == "joint":
                # If we are doing "joint" encoding, there must be unique mapping
                # between input column names and column groups.  Otherwise, more
                # than one unique-value table could be used to encode the same
                # column.
                raise ValueError("Same column name included in multiple groups.")
            for group in self.column_groups:
                if isinstance(group, list) and len(group) > 1:
                    # For multi-column groups, we concatenate column names
                    # to get the "group" name.
                    name = nvt_cat._make_name(*group, sep=self.name_sep)
                    for col in group:
                        self.storage_name[col] = name

        # Only support two kinds of multi-column encoding
        if encode_type not in ("joint", "combo"):
            raise ValueError(f"encode_type={encode_type} not supported.")

        # Other self-explanatory intialization
        super().__init__(columns=columns, preprocessing=preprocessing, replace=replace)
        self.freq_threshold = freq_threshold
        self.out_path = out_path or "./"
        self.tree_width = tree_width
        self.na_sentinel = na_sentinel or 0
        self.dtype = dtype
        self.on_host = on_host
        self.cat_cache = cat_cache
        self.stat_name = "categories"
        self.encode_type = encode_type

    @property
    def req_stats(self):
        return [
            GroupbyStatistics(
                columns=self.column_groups or self.columns,
                concat_groups=self.encode_type == "joint",
                cont_names=[],
                stats=[],
                freq_threshold=self.freq_threshold,
                tree_width=self.tree_width,
                out_path=self.out_path,
                on_host=self.on_host,
                stat_name=self.stat_name,
                name_sep=self.name_sep,
            )
        ]

    @annotate("Categorify_op", color="darkgreen", domain="nvt_python")
    def apply_op(
        self,
        gdf: cudf.DataFrame,
        columns_ctx: dict,
        input_cols,
        target_cols=["base"],
        stats_context={},
    ):
        new_gdf = gdf.copy(deep=False)
        target_columns = self.get_columns(columns_ctx, input_cols, target_cols)
        if not target_columns:
            return new_gdf

        if self.column_groups and not self.encode_type == "joint":
            # Case (3) - We want to track multi- and single-column groups separately
            #            when we are NOT performing a joint encoding. This is because
            #            there is not a 1-to-1 mapping for columns in multi-col groups.
            #            We use `multi_col_group` to preserve the list format of
            #            multi-column groups only, and use `cat_names` to store the
            #            string representation of both single- and multi-column groups.
            #
            cat_names, multi_col_group = nvt_cat._get_multicolumn_names(
                self.column_groups, gdf.columns, self.name_sep
            )
        else:
            # Case (1) & (2) - Simple 1-to-1 mapping
            multi_col_group = {}
            cat_names = [name for name in target_columns if name in gdf.columns]

        # Encode each column-group separately
        for name in cat_names:
            new_col = f"{name}_{self._id}"

            # Use the column-group `list` directly (not the string name)
            use_name = multi_col_group.get(name, name)
            # Storage name may be different than group for case (2)
            # Only use the "aliased" `storage_name` if we are dealing with
            # a multi-column group, or if we are doing joint encoding
            if use_name != name or self.encode_type == "joint":
                storage_name = self.storage_name.get(name, name)
            else:
                storage_name = name
            path = stats_context[self.stat_name][storage_name]
            new_gdf[new_col] = nvt_cat._encode(
                use_name,
                storage_name,
                path,
                gdf,
                self.cat_cache,
                na_sentinel=self.na_sentinel,
                freq_threshold=self.freq_threshold,
            )
            if self.dtype:
                new_gdf[new_col] = new_gdf[new_col].astype(self.dtype, copy=False)

        # Deal with replacement
        if self.replace:
            for name in cat_names:
                new_col = f"{name}_{self._id}"
                new_gdf[name] = new_gdf[new_col]
                new_gdf.drop(columns=[new_col], inplace=True)

        self.update_columns_ctx(columns_ctx, input_cols, new_gdf.columns, target_columns)
        return new_gdf


def _get_embedding_order(cat_names):
    """ Returns a consistent sorder order for categorical variables

    Parameters
    -----------
    cat_names : list of str
        names of the categorical columns
    """
    return sorted(cat_names)


def get_embedding_sizes(workflow):
    cols = _get_embedding_order(workflow.columns_ctx["categorical"]["base"])
    return _get_embeddings_dask(workflow.stats["categories"], cols)


def _get_embeddings_dask(paths, cat_names):
    embeddings = {}
    for col in cat_names:
        path = paths[col]
        num_rows, _, _ = cudf.io.read_parquet_metadata(path)
        embeddings[col] = _emb_sz_rule(num_rows)
    return embeddings


def _emb_sz_rule(n_cat: int) -> int:
    return n_cat, int(min(16, round(1.6 * n_cat ** 0.56)))


class LambdaOp(TransformOperator):
    """
    Enables to call Methods to cudf.Series

    Parameters
    -----------
    op_name : str
        name of the operator column. It is used as a post_fix for the
        modified column names (if replace=False)
    f : lambda function
        defines the function executed on dataframe level, expectation is lambda col, gdf: ...
        col is the cudf.Series defined by the context
        gdf is the full cudf.DataFrame
    columns :
    preprocessing : bool, default True
        Sets if this is a pre-processing operation or not
    replace : bool, default True
        Replaces the transformed column with the original input
        if set Yes
    """

    default_in = ALL
    default_out = ALL

    def __init__(self, op_name, f, columns=None, preprocessing=True, replace=True):
        super().__init__(columns=columns, preprocessing=preprocessing, replace=replace)
        if op_name is None:
            raise ValueError("op_name cannot be None. It is required for naming the column.")
        if f is None:
            raise ValueError("f cannot be None. LambdaOp op applies f to dataframe")
        self.f = f
        self.op_name = op_name

    @property
    def _id(self):
        return str(self.op_name)

    @annotate("DFLambda_op", color="darkgreen", domain="nvt_python")
    def op_logic(self, gdf: cudf.DataFrame, target_columns: list, stats_context=None):
        new_gdf = cudf.DataFrame()
        for col in target_columns:
            new_gdf[col] = self.f(gdf[col], gdf)
        new_gdf.columns = [f"{col}_{self._id}" for col in new_gdf.columns]
        return new_gdf


class Filter(TransformOperator):
    """
    Filters rows from the dataset. This works by taking a callable that takes a dataframe,
    and returns a dataframe with unwanted rows filtered out.

    Parameters
    -----------
    f : callable
        Defines a function that filter rows from a dataframe. Exp: ```lambda gdf: gdf[gdf.a >= 0]```
        would filter out the rows with a negative value in the ```a``` column.
    preprocessing : bool, default True
        Sets if this is a pre-processing operation or not
    """

    default_in = ALL
    default_out = ALL

    def __init__(self, f, preprocessing=True, replace=True):
        super().__init__(preprocessing=preprocessing, replace=replace)
        if f is None:
            raise ValueError("f cannot be None. Filter op applies f to dataframe")
        self.f = f

    @annotate("Filter_op", color="darkgreen", domain="nvt_python")
    def apply_op(
        self,
        gdf: cudf.DataFrame,
        columns_ctx: dict,
        input_cols,
        target_cols=["base"],
        stats_context=None,
    ):
        new_gdf = self.f(gdf)
        new_gdf.reset_index(drop=True, inplace=True)
        return new_gdf
